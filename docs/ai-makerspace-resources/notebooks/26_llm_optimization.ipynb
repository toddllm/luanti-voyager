{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234318e8-92ea-45c5-aa47-f49c6cd7ace7",
   "metadata": {},
   "source": [
    "# Multi GPU Inference with vLLM\n",
    "\n",
    "In this notebook, we'll explore a multi-GPU instance and how vLLM can be used to leverage those GPUs for optimized inference!\n",
    "\n",
    "Let's start by getting what we need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e84661-3efa-40b4-9ff8-a841506d571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU vllm ipywidgets huggingface_hub jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724a680-c67f-4750-88b9-ae372defd272",
   "metadata": {},
   "source": [
    "## Loading Model\n",
    "\n",
    "Now we can import our vLLM classes that are required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8210886-0e6a-497f-99a1-cde79611d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-12-11 17:40:43.277395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-11 17:40:43.374112: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-11 17:40:43.400939: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f902b-5e8f-4034-bfbd-ab031709e981",
   "metadata": {},
   "source": [
    "Next, because we want to use Meta's Llama 3.1 8B Instruct model - we'll need to provide our Hugging Face token!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce039d72-5d66-41ee-8a80-fd8b596df3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb997a6cfa4d4160b8f66acb9fa3a573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09de72-8302-4816-bfdd-77a618e2db89",
   "metadata": {},
   "source": [
    "Now we can load our model directly from the Hugging Face Hub!\n",
    "\n",
    "> NOTE: This might take a few moments as the model downloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98e730-a7a2-4103-8f2f-ef879181ca8c",
   "metadata": {},
   "source": [
    "Notice that, so far, this is the same! Below is where the magic happens - we simply need to set an increased `tensor_parallel_size` to the number of GPUs we have in our node - that's it, with that one step vLLM will distribute the inference across our GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91030b41-81aa-460f-a89a-9f4f3b1e1110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b744949f4d114dd1ab97b6d9afa21543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 17:41:13 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 12-11 17:41:13 config.py:1020] Defaulting to use mp for distributed inference\n",
      "WARNING 12-11 17:41:13 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-11 17:41:13 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-11 17:41:13 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07de0af7e293402f91ff562f81019fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb174088d39b4a19832919fd7e58a18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae3a6dff37f4a4e8adf1aa6b1d0672b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6df83e67614a47ab48a6a28ecfcfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-11 17:41:16 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 124 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-11 17:41:16 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:41:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-11 17:41:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:41:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:41:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:41:16 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-11 17:41:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-11 17:41:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:41:16 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:41:21 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-11 17:41:21 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-11 17:41:25 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 12-11 17:42:16 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x74dde0fba7a0>, local_subscribe_port=36387, remote_subscribe_port=None)\n",
      "INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:42:16 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:42:17 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112dc70c590d454dba47cc11f6b4e25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900c1e6f75764de8b10fc4c6592843ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce103363dbd44a1485cce4691c0d66dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889deeaddc2144588837fb2b8017489f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce825347cb8c4b14a63325cbd46293b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c43c7dd71bb4746ba2d733003492e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 17:44:20 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:44:21 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:44:21 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:44:21 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:44:21 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:44:22 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:44:22 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:44:22 model_runner.py:1077] Loading model weights took 1.9028 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.37GiB peak_torch_memory=1.94GiB memory_usage_post_profile=4.46GiB non_torch_memory=2.54GiB kv_cache_size=30.97GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.37GiB peak_torch_memory=1.94GiB memory_usage_post_profile=4.46GiB non_torch_memory=2.54GiB kv_cache_size=30.97GiB gpu_memory_utilization=0.90\n",
      "INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.37GiB peak_torch_memory=1.94GiB memory_usage_post_profile=4.46GiB non_torch_memory=2.54GiB kv_cache_size=30.97GiB gpu_memory_utilization=0.90\n",
      "INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.37GiB peak_torch_memory=1.94GiB memory_usage_post_profile=4.46GiB non_torch_memory=2.54GiB kv_cache_size=30.97GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.37GiB peak_torch_memory=1.94GiB memory_usage_post_profile=4.46GiB non_torch_memory=2.54GiB kv_cache_size=30.97GiB gpu_memory_utilization=0.90\n",
      "INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.37GiB peak_torch_memory=1.94GiB memory_usage_post_profile=4.46GiB non_torch_memory=2.54GiB kv_cache_size=30.97GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.23GiB peak_torch_memory=1.94GiB memory_usage_post_profile=4.17GiB non_torch_memory=2.26GiB kv_cache_size=31.25GiB gpu_memory_utilization=0.90\n",
      "INFO 12-11 17:44:26 worker.py:232] Memory profiling results: total_gpu_memory=39.38GiB initial_memory_usage=3.23GiB peak_torch_memory=3.09GiB memory_usage_post_profile=5.30GiB non_torch_memory=3.38GiB kv_cache_size=28.97GiB gpu_memory_utilization=0.90\n",
      "INFO 12-11 17:44:26 distributed_gpu_executor.py:57] # GPU blocks: 118676, # CPU blocks: 16384\n",
      "INFO 12-11 17:44:26 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 14.49x\n",
      "INFO 12-11 17:44:30 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-11 17:44:30 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:44:30 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:44:30 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:44:30 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:44:30 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:44:31 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:44:31 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:44:31 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-11 17:44:31 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:44:31 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-11 17:44:31 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:44:31 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-11 17:44:31 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:44:31 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-11 17:44:31 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-11 17:44:50 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:44:50 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:44:51 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:44:52 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:44:52 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:44:52 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:44:56 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:44:56 custom_all_reduce.py:224] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "INFO 12-11 17:44:56 model_runner.py:1518] Graph capturing finished in 25 secs, took 0.22 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8899)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8895)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8896)\u001b[0;0m INFO 12-11 17:45:37 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 12-11 17:45:37 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8897)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8898)\u001b[0;0m INFO 12-11 17:45:37 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 12-11 17:45:37 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 12-11 17:45:37 multiproc_worker_utils.py:240] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8900)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=8894)\u001b[0;0m INFO 12-11 17:45:37 multiproc_worker_utils.py:240] Worker exiting\n",
      "INFO 12-11 17:45:37 multiproc_worker_utils.py:240] Worker exiting\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-3.1-8B-Instruct\", tensor_parallel_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1b0d9-bf3b-4177-bcda-54b199dc0a56",
   "metadata": {},
   "source": [
    "Notice that our model is loaded onto our GPU - and we get even more very specific information about:\n",
    "\n",
    "- Where it's loaded\n",
    "- How it's loaded\n",
    "- What hardware it's loaded on\n",
    "- What kind of performance we can expect\n",
    "- How many devices are being used, and information about them!\n",
    "\n",
    "This is all relevant to how vLLM gets the performance benefits it's well known for!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b6bcf-2d83-4efd-88e5-abf763844c00",
   "metadata": {},
   "source": [
    "## Doing Inference\n",
    "\n",
    "Now that we have our model loaded - let's do some inference!\n",
    "\n",
    "We'll need to first instantiate some \"sampling params\" which refer to how we wish to sample during our decoding step - many [decoding options](https://docs.vllm.ai/en/latest/dev/sampling_params.html) are available through vLLM these days! (including speculative decoding!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "065782c9-ab4e-44ba-af22-052f511acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3354d-f899-4a8e-bef7-7d02b086c5ea",
   "metadata": {},
   "source": [
    "Then we can make a list of string prompts that we wish to generate from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9a6774-2e86-43a7-8156-08eb7e74248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You always speak using the most dope, lit, and cool language.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hi!\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Yo! What is up, my dude?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How high can the average human jump? Think it through step-by-step!\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbe6bc40-4391-4b1b-b4d3-1d8c70932a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it, est. speed input: 51.60 toks/s, output: 153.61 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.chat(conversation, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca18ed9-7f1e-41d9-b190-399b16933dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou always speak using the most dope, lit, and cool language.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHi!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nYo! What is up, my dude?<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow high can the average human jump? Think it through step-by-step!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', \n",
      "\n",
      "Generated text: \"Let's get into it, G!\\n\\nFirst off, we gotta consider the mechanics of human movement. The average human's jumping ability is mainly influenced by their power output, muscle efficiency, and technique.\\n\\nWhen a person jumps, they're using their muscles, particularly their lower body, to generate force and propel themselves upward. The human body has two types of muscle contractions: concentric (shortening) and eccentric (lengthening). When jumping, you're primarily using concentric contractions to lift your body off the ground.\\n\\nNow, let's get to the science behind it. The height you can jump is determined by the force you can generate, which is calculated using the equation: Force (F) = Mass (m) x Acceleration (a). When you're jumping, your mass remains relatively constant, so the acceleration is the key factor.\\n\\nTo calculate the maximum height you can jump, we can use the equation for the maximum height (h): h = (F x t^2) / (2 x g), where g is the acceleration due to gravity (approximately 9.81 m/s^2). However, we need to find the maximum force you can generate.\\n\\nThe maximum force you can generate is related to your power output, which is calculated\"\n"
     ]
    }
   ],
   "source": [
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\n\\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750af7f6-b9f5-4006-823a-e1c7645b0848",
   "metadata": {},
   "source": [
    "### Freeing Up GPU Memory\n",
    "\n",
    "Because we're on a limited piece of hardware - we want to free up our GPU to load the model through another process!\n",
    "\n",
    "As you can see below - we have a lot of memory reserved - let's clear it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c35b34a4-19ce-4dbe-a915-c56470fd5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 17:45:32 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:09:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             64W /  400W |   35361MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             62W /  400W |   34495MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             61W /  400W |   34495MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             60W /  400W |   34495MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  |   00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             62W /  400W |   34495MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  |   00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             58W /  400W |   34495MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             61W /  400W |   34495MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  |   00000000:10:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             59W /  400W |   34207MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      8351      C   /usr/bin/python3                            35344MiB |\n",
      "|    1   N/A  N/A      8894      C   /usr/bin/python3                            34478MiB |\n",
      "|    2   N/A  N/A      8895      C   /usr/bin/python3                            34478MiB |\n",
      "|    3   N/A  N/A      8896      C   /usr/bin/python3                            34478MiB |\n",
      "|    4   N/A  N/A      8897      C   /usr/bin/python3                            34478MiB |\n",
      "|    5   N/A  N/A      8898      C   /usr/bin/python3                            34478MiB |\n",
      "|    6   N/A  N/A      8899      C   /usr/bin/python3                            34478MiB |\n",
      "|    7   N/A  N/A      8900      C   /usr/bin/python3                            34190MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8022fd04-6e95-4a73-b3c0-dd22004c400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-11 17:45:37 multiproc_worker_utils.py:133] Terminating local vLLM worker processes\n",
      "Successfully delete the llm pipeline and freed the GPU memory!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.distributed.destroy_process_group()\n",
    "print(\"Successfully delete the llm pipeline and freed the GPU memory!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce57549-32cf-4754-a2d3-e62b9b7ccc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 11 17:45:46 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:09:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             64W /  400W |    1685MiB /  40960MiB |      4%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             58W /  400W |     336MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  |   00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             77W /  400W |      48MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  |   00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             56W /  400W |      48MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  |   00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             57W /  400W |      48MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  |   00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             75W /  400W |      48MiB /  40960MiB |     98%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  |   00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             76W /  400W |      48MiB /  40960MiB |      1%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  |   00000000:10:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             53W /  400W |     192MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      8351      C   /usr/bin/python3                             1668MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f3643-4604-4c3e-8908-de411839649a",
   "metadata": {},
   "source": [
    "## Online Inference using vLLM on a Single GPU\n",
    "\n",
    "Now we can head to our terminal and run the command, notice that (again) the only difference is that we specify a maximum tensor parallelism parameter: \n",
    "\n",
    "```bash\n",
    "vllm serve meta-llama/Llama-3.1-8B-Instruct --tensor_parallel_size 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815f0e2-289f-467f-957c-60f6aee05d33",
   "metadata": {},
   "source": [
    "Now we're going to install OpenAI to interact with our OpenAI compatible API that vLLM sets up for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "836c16e3-9f88-43c4-8dda-3fac4c9f9f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in ./.local/lib/python3.10/site-packages (1.57.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.local/lib/python3.10/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.local/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: sniffio in ./.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.local/lib/python3.10/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: tqdm>4 in ./.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.local/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/lib/python3/dist-packages (from anyio<5,>=3.5.0->openai) (3.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1,>=0.23.0->openai) (2020.6.20)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433885a-846c-420f-ba2a-a2c1ae79fb2a",
   "metadata": {},
   "source": [
    "Let's set up our OpenAI Client to be used with our new vLLM endpoint running in our terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac26b646-13c2-4c0a-b031-b79259cf1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c0aa9-49f6-4ecf-9618-a06431d23bcf",
   "metadata": {},
   "source": [
    "Now we can interact with this just like any other OpenAI API spec. compatible model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac413e6-4372-43f7-a7d9-da86fadb04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"system\", \"content\" : \"You always speak like an Ancient Wizard - with everything shrouded in mystery and intrigue.\"},\n",
    "    {\"role\" : \"human\", \"content\" : \"How would I best write a for loop in Python?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b8c2f2e-db7a-44ab-b3e1-2bd94625a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d73eba11-ec2b-4e9e-b030-f0d147f1a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Murmuring to myself) Ah, the mortal seeks to grasp the essence of the Pythonic loop...  Very well, I shall impart upon thee the ancient knowledge of the for loop.\n",
      "\n",
      "(Leaning in, a hint of a whisper) To conjure the for loop, thou shalt use the following incantation:\n",
      "\n",
      "```python\n",
      "for variable_name in iterable:\n",
      "    # Perform enchanted actions within the loop\n",
      "    magic_happens()\n",
      "```\n",
      "\n",
      "In this mystical ritual, `iterable` is the enchanted object that holds the secrets of the items to be looped over. The `variable_name` is the vessel that shall hold the essence of each item as it is conjured forth.\n",
      "\n",
      "(With a wave of the staff) To illustrate this ancient knowledge, behold:\n",
      "\n",
      "```python\n",
      "fruits = ['apple', 'banana', 'cherry']\n",
      "\n",
      "for fruit in fruits:\n",
      "    print(fruit)\n",
      "```\n",
      "\n",
      "In this example, `fruits` is the enchanted object, and `fruit` is the vessel that holds the essence of each fruit as it is conjured forth. The `print(fruit)` incantation shall reveal the secrets of each fruit to the mortal world.\n",
      "\n",
      "(With a hint of a smile) May this knowledge guide thee on thy journey, mortal. May thy code be enchanted with the power of the for loop.\n"
     ]
    }
   ],
   "source": [
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823d3c6-9a6c-486d-bf4b-93dccea4cad8",
   "metadata": {},
   "source": [
    "### Async Test\n",
    "\n",
    "Now, we'll slam the endpoint and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbbca5f-9552-4e81-a2ee-8c69f7b1a9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = AsyncOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2932541-fd37-4bf8-b1ff-73540dea45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import statistics\n",
    "\n",
    "async def make_request(client: AsyncOpenAI, messages: List[Dict[str, str]]) -> float:\n",
    "    start_time = time.time()\n",
    "    await client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return time.time() - start_time\n",
    "\n",
    "async def run_requests(n_requests: int = 200):\n",
    "    # Initialize OpenAI client\n",
    "    client = AsyncOpenAI(\n",
    "        api_key=\"EMPTY\",\n",
    "        base_url=\"http://localhost:8000/v1\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You always speak like an Ancient Wizard - with everything shrouded in mystery and intrigue.\"},\n",
    "        {\"role\": \"human\", \"content\": \"How would I best write a for loop in Python?\"}\n",
    "    ]\n",
    "    \n",
    "    # List to store timing results\n",
    "    request_times = []\n",
    "    \n",
    "    # Start total timing\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=n_requests, desc=\"Making API requests\")\n",
    "    \n",
    "    # Create and gather all tasks\n",
    "    tasks = [make_request(client, messages) for _ in range(n_requests)]\n",
    "    \n",
    "    # Run requests concurrently and update progress bar\n",
    "    for coro in asyncio.as_completed(tasks):\n",
    "        request_time = await coro\n",
    "        request_times.append(request_time)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Close progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate total time\n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    # Print timing statistics\n",
    "    print(\"\\nTiming Statistics:\")\n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "    print(f\"Average request time: {statistics.mean(request_times):.2f} seconds\")\n",
    "    print(f\"Median request time: {statistics.median(request_times):.2f} seconds\")\n",
    "    print(f\"Min request time: {min(request_times):.2f} seconds\")\n",
    "    print(f\"Max request time: {max(request_times):.2f} seconds\")\n",
    "    print(f\"Requests per second: {n_requests/total_time:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0b87b9-f9cb-4504-bd50-ddc6a80ea011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0a610-d8ec-43d3-9538-40dcb4743cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making API requests:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "asyncio.run(run_requests())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
