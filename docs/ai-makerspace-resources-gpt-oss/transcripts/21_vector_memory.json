{
  "text": " Welcome, everybody. We've got a legendary show for you today, and I'm really excited to hand off today's session to AI Makerspace's newest YouTube star, Laura, the legend, Thunderbird. Welcome up, Laura. You've been part of the community for a long time. Tell us a little bit about yourself and what we can expect today. Yeah. Well, thank you so much for having me, Greg. I mean, you and I have met each other for a few years now. You guys were teaching at Fourth Brain back in the day, and I met you through one of your bootcamps. You spun up AI Makerspace, and I kept following you guys. And we did a few collaborations back when I started working as a dev role at Bitewax. And I recently joined as a peer supporter for the AI engineering bootcamp and absolutely loved it. And I think this is the first time I'll get to share this with the community, but I recently joined as a developer advocate and community leader. And I'm really excited to be here today. And I'm really excited to be here today. And I think this is the first time I'll get to share this with the community, And I think this is the first time I'll get to share this with the community, for the AI Makerspace team, where my focus is not just doing events, but I'm also doing things like creating social media posts, doing the newsletter, engaging with folks on Discord, and highlighting the awesome things that people build, ship, and share every day with the community. Yeah. And you have made a huge impact already. And I'm really excited to start this series, this Laura the Legend series, where you're going to be looking at different tools from different tool stacks with different tools. And I'm really excited to be here today. And I'm really grateful for the many different developer advocates that you've met along the way and that you've yet to meet out in the industry, tools that we just don't really have time in 52 weeks of YouTube events to cover. The first tool today. What are we looking at and who are we going to meet? Yeah, so today we're going to be working with Tawanna Chelic from LAMA index. Tawanna works as a senior developer advocate engineer. And she and I have worked a few times in the past, both in our developer advocacy roles at different places. And it's difficult for her to work my way device The amount appropriately I don't reckon we've ever worked in this program has had a benign challenge to play. She's never had a safe job as a rol erp many decades ago. But she's a\u0412e heavyweight and she could points in our lives and i've always enjoyed working with her so i'm very very happy that she's the first person that's invited onto the laura the legend show let's go well welcome to anna and hello to anna can you tell us a little bit about what we're going to talk about today agent memory is that kind of like when chad gbt remembers your birthday is that is that what that's all about uh can you kind of the high level yeah so uh we're going to be talking about agent memory specifically um a few of the latest implementations we have in llama index and basically you think about yeah kind of chat gpt remembering your birthday but maybe we want agents to remember different kinds of stuff so we're going to be talking about that awesome well laura off to you it's your show thank you everybody for joining the very first laura the legend show number one thank you greg all right everyone so today's session is going to be a little bit different than what you're used to seeing with dr greg and the wiz typically you know you're used to the sessions having greg talking for quite a bit and then wiz showing some code snippets today i want the session to be a little bit more interactive and to one is going to be engaging in discussion with all of you and with me uh during the session so i've prepared some talking points and a few questions we're going to be addressing your questions at the end of the session and there's a couple of questions that we're going to answer that are going to be helpful to all of you so i'm the end as well. So if you have any questions, please drop them in the chat and we'll get to them towards the Q&A portion of this session. So I'm going to go ahead and get us started. So today we're going to be talking about this idea of enabling memory for agents. And we're going to be talking about this as a core component of agentic systems. For those of you who have been developing LLM applications since the beginning, we started first by prompting an LLM by making API calls. We then started exploring this idea of improving the way LLM responds by including a database. So we talked about this idea of building retrieval augmented generation. And then the next step we took was to give LLMs access to tools. And we defined this in broad terms as agents, where LLMs have the capacity to autonomously decide which tools to use to complete a specific purpose. So let's get started. Today we're going to be talking about the idea of enabling memory for agents. And we're going to take the next step in that evolution. And we're going to be talking about how we can incorporate memory in agentic systems. So in LLMA index, we can typically customize memory by using an existing base memory class. So this idea of the base memory class is something that Tawana is going to be telling us a little bit more about. And we can also create custom ones. So we'll have code snippets towards a few slides after this, where Tawana will explain to us this idea of what the base memory class is and how we can create a custom one. So let's get started. So within this system, as the agent is running, the agent will make calls to the memory.put method to store information and then use the memory.get method to retrieve information. So we have this memory component, and we're going to have two methods, put and get, to enable the agent to retrieve information. To configure memory for an agent, again, we're going to be passing it to a run method. So we'll get a chance to see a few code snippets in a little bit. But first, let's bit so um juana can you let's let's let's explore this code snippet for a little bit so i'm working with llama index which as many of you know it is a python it provides a python package for you to build llm applications initially we worked with these applications uh with a focus on rag and later evolved to incorporate agentic applications so tell me tell me more a little bit about what this code snippet is doing and and how we can incorporate memory in as part of our agentic applications so this is the most basic form of memory i would say so you can think of this as short term memory um and i think we can start looking into more complex types of memory in a bit but what's going on here is ultimately whenever we talk with or chat with an llm or an agent let's let's just call it agent from now on we hopefully wanted to be able to retain some of the information that we had from the original code snippet but we did not have enough to do everything but we did have to do everything from the original code snippet to the original code snippet to the of our past conversations while the chat is ongoing. Later on in future, we maybe also want to come back to that chat, but let's assume we're just within one chat instance. And what's happening here is we're basically providing our agent with a memory block. And by default, this memory block in Lama index is limited by a certain token limit. So after that token limit, we will want to flush that memory to something else, but let's ignore that for now. Basically, what we're doing here is we're allowing our agent to retain chat history given this memory block, this default memory block in Lama index. So tell me this idea. So tell me more a little bit about this from defaults method. So explain a little bit more what's going on. So it seems like I'm setting a token limit. I can choose. I can choose the size of the token limit. So tell me a little bit more about the role of a token limit in the memory. I think from defaults is an implementation detail of Lama index. So I wouldn't focus too much on that. But basically what we're doing here is we're telling our code that there is a memory component within Lama index. And I want to make use of that memory component by providing it some of my own specific token limit and setting. Okay, effect, so in Orbit code, I'm going to sleep with feedback. And then I'm just going to introduce the maximum amount of time to which this is happening at every layer. So I'm going to keep that in memory, focusing on one area. And in public, somehow it may expect my random disappear force to shut for a small one, so I can sort of cause more automation or jerk there. Then I can\u591c wait 24 hours, then I get the\u0438\u043b\u044f, then I \u043f\u0435\u0440w Zeit, and close every website. Hold on just a minute. Okay. One last thing to note, token limit, we're basically telling our agent that this is the amount I'm going to allow my in memory, short term memory to hold. And after that point, I want you to actually start storing memory somewhere else somewhere maybe a bit more sophisticated that I can also access as the agent is running. So that is the main thing that's happening here. Okay, and I just have a quick question. So maybe just to summarize my understanding of what you're sharing with us. So I will define my LLM, you know, typically, I'll be making some kind of API call to either open AI models from hugging phase models hosted on Azure, AWS, etc, I will define my tools. And then I will say, Okay, so you you're this system is going to retain memory up to this token limit. So any messages from that that are covered within that token limit will be remembered as a way to put it. You can even put it this way. Any messages. Up to that token limit, I don't need to implement any extra memory implementation at all, my short term memory will just simply be holding that for me. And if you want some more detail, we can maybe talk about this a bit later as well. By default in LLM index, that short term memory is basically a in memory SQL database. All right, so it's just been memory SQL database. Okay, great. Okay, so let's keep going. Okay, so as as you discussed, you sort of started hinting on this idea of short term memory. And from our discussion, it sounds like this idea of short term memory is, you know, the in memory SQL database. And if we want to have something like long term memory, we would go for something more sophisticated, like Postgres or other database types. So, John, I want to focus to you with with you today about talking, you know, both kinds of memory and explore more in depth what this looks like. So, you know, we talked a little bit about this idea of, you know, how to implement memory within within the agentic system. So what what does it mean to give the agent memory? So is it is it is it only recalling the messages that were provided to it? Is it also recalling any tool usage, any decision making process? Can you tell us how this happens in relation to how the LLM is making decisions with respect to what tool to use given the prompt it was given? Yes. So this maybe we can start by discussing the way we make choices around how we implement long term memory. So I also see that there's a question in the chat about what happens after we reach the token limit. And this is a great time to actually answer that question as well. Okay. So once we hit the token limit of what we allow our short term memory to hold, this is where we start to see the impact of the LLM. So we can see that the LLM is actually a very good example of how we can implement long term memory. So we can see, how LLM is directly affected by long term memory. So in this example we can see it isattending we are actually showing the on the non beta API portion of the shell code of a block of primordial information that why is it attackable. We can also see that this is why this API to actually work under the entire form of a string initial lastly because we need to actually track those limits. We can also see that the LLM of the of the LW is employed for the in Scott. And next I'm going to test out how it's\u3053\u308c bit of a oneself on this a three step process. We can see which of the effectiveness, which of these events is minimal. PJ Slauson down there. Okay. a few default long-term memory blocks for you, so you don't really have to think about the implementation. But I can start with maybe something that is the easiest to relate to. Most of the times what we see people use agents for is that they want to have a chat with the agent, and that agent maybe has access to some external information. We can provide a vector memory block. So you can think of this as a regular vector store where we store embeddings of information. That's the most classic way we do RAG as well, for example. But we can also start using vector stores as memory blocks themselves. So that is one of our long-term memory implementations. We literally call it the vector memory block. What this does is that while the chat is ongoing, if we hit the token limit, like this user asked, what happens when you hit that token limit? Behind the scenes, without you actually as a developer having to do anything about how you implement this, behind the scenes what the Lama Index agent will do is that all of the chat that happens so far will be indexed and will be written into a vector memory block, so a vector store itself. From this point on, every interaction we have ongoing, the agent will also have a 24HI fundamental instance that we put up here. It is probably very new, and it's not something we want or want to change in space forces and it's not necessarily wanted. Now I want to say that with the user there are versions of Lama Index \ud558\ub098 and Lama Index Forks that we can implement long-term memory again. Great. Although a\u5f85\u3063\u3066 of doing the same decision and having a use case or000 file, and also that it's more streamlined in this case than with the user itself here. So one way Lama Index can work is when you zoom in, lazy One-time memory of something like a CSV file, is when we say hey, create this such and today we're looking into summary end use case you want to provide to your users or your end application you want to provide to your users, let's assume the chat history or the exact things you talked about with that agent is not so important, but facts are. For example, how old you are, where you live, etc. So in some cases, we might see situations where long-term memory is more about keeping track of facts about that user rather than keeping track of the chat messages that users send specifically. So for this, we also have implemented a long-term memory block called fact memory block, for example, where this is designed to simply store, think of it as like a index of facts that we ask the agent to extract along the way. So that is another implementation. So yeah, to answer your question, I think it would be a lot more... Use case based, which type of long-term memory you want? It depends. It sounds like it's quite flexible and varied. So we have the option to host memory in a simple in-memory SQL database, but there's also more sophisticated forms, not just Postgres, but you can also set up even a vector database where you can create embeddings for different either the chat components or more complex forms of information and enable, give that access, give that to the agent to best implement.\u017a Okay. parameters. And we touched a little bit about I think this will sort of addressed in your answer. The difference between the short-term and long-term memory. So maybe let's continue. Let's explore a little bit more in depth. How we can start implementing both types of memory. So from the documentation, the memory class will store the last X messages that fit into the token limit. We already talked about this already, so hang up and let's do some more memories. bit about that token limit as we went through through this a little bit more so let's move onto onto this idea of the long-term memory and so from the documentation this idea of the memory block object is introduced uh to implement long-term memory so can you tell us a little bit more about this memory block object yes and i think this is probably my favorite thing about how we've implemented long-term memory um so uh as we discussed before there are some pre-implemented long-term memory blocks like the vector memory block we discussed uh fact memory block we discussed my favorite which is not in llama index core but it is provided as an integration one of the latest ones that my colleague logan added is called artifact memory block which we can chat about if you're interested in it but the most important thing about these is that they're simply extension of the memory block object so if you're interested in that you can go to the library and look at the library and look at the memory block object and look at the memory block object and then you can see that the memory block object is actually an extension of a memory block object that we call the memory block object um i think there should be a code snippet that shows you as an example of how you could implement your own memory by extending the base memory block uh class so the idea is we wanted to we wanted to provide some implementations of long-term memory i don't think it's this one um so these are these are the the three memory blocks you mentioned the static fact extraction and vector memory block so so yeah let's explore this a little bit more in depth so the the point of the memory block is these these specific uh vector memory blocks static memory block etc that you're seeing on the screen here these are our uh implementations that we have provided to our users as like packaged implementations of a memory block but the most important thing is that you can implement your own memory blocks by extending the base memory block so in these use cases we've provided you some but if there is a memory implementation that makes a lot more sense for your use case the idea is that you can simply extend the base memory block class which has a very few set of predefined functions and completely custom implement your own memory block i think so to answer your question about what the memory block is it is a um how do i put those brackets back into remember group okay does that make sense i think it does yes i know it's simply by using the Idag data borders b\u4f0a this time to the uh binary for scenario and i'm using um flow project that builds memory blocks and after procedures we have you know instructor something that includes progressive writing and you can try separate versions of the taylor conceptions to see if these can work so as and when you when you start this program it will be an easy problem right now not because you'll not find a place for state or value understand correctly. What do I choose each? You don't necessarily have to choose. You can actually even use all of them. So an agent isn't limited to how many memory blocks it uses. So if you want to, you can actually have an agent that makes use of all of these memory blocks in one go. But let me give you some examples. If I have an application where I want to pay more attention to past discussions with an end, you want to make sure that I'm never losing any context about what that discussion contained. I want the user to be able to ask questions like, what was that thing that I said that I asked you about, I don't know, olive oil the other day. And you want to make sure that the agent remembers that discussion point. I would definitely suggest using a vector memory block, because this allows you to vectorize all of the chat messages. Okay. And at each iteration, yeah, so you're basically doing semantic similarity search at each iteration to make sure that the agent knows if there's any past discussion that's happened there. If, however, let's say you have an application, where the main use case is to make sure that you always have the most up to date facts about your user or about a topic. Maybe this is a point where you simply use a fact memory block. And maybe you can even say, okay, I'm going to do this. And then you can do this. And then you can do this. And then you can skip the vector memory block. Maybe a good example of that is that let's say you are a you're building an application, where the main use case is that you want to provide airline support, like you want to provide support to people who are taking flights. And maybe in this case, the most important information is very simple facts. What day are they flying? What's the flight number? Which city are they flying to? From where, etc. And maybe in this situation, past chat is not that important, more than the short term memory. So it's more about deciding what exactly do I want to provide the end user and picking long term memory blocks accordingly. But like I said, you don't actually have to pick you can also use a mixture of all of them. Yeah. And like you mentioned before, this is a this is a pretty powerful new extension to agentic applications. Yeah. Because as you noted before, you know, we can build an agentic application where the agent uses a rag database as a tool, and is also enhanced by vectorized chat histories, where if you're if you're if you're if you have chat histories that span not just one user, but multiple users over a long period of time, the agent can recall information from a very complex source of information, not just a rag database, but also history of conversation with multiple users across a long period of time. So this is I think this is a really, really nice addition to the agentic stack. Okay, so so you mentioned before that we can also define our own custom memory blocks, and you mentioned that we can do so by by using the base memory block class. So this is a I would assume this is a simple example of how to how to define your own custom memory blocks. Is this correct? Yes, exactly. So this is actually quite a funny example. I'm not sure if this actually, you know, would ever exist in the real world, maybe. But this time, the memory block itself is actually a mention counter. So in this implementation, again, this was provided by Logan, this memory block, the main thing it's doing is it's keeping track of how many times a name was mentioned. It's kind of silly, but it's a good example of showing how easy it is to implement your own memory. So once you've implemented this, which is an extension of the base memory block, this mention counter memory block can be used as like any other memory block you've seen before, you simply provide it to the agent, and it does the job of keeping track in this case of counting how many times Logan was mentioned. Okay, but I think at the core of this is, you know, you're not just limited by those three memory blocks, you can create your own custom one by inheriting from the base memory block class. I think that is a key message. Okay, so we talked a little bit about this. So maybe maybe let's move on to some of the more complex questions. So you mentioned in the past, this can either be in memory, Postgres, or through your own vector database. So if you wanted to use something like Postgres, you can use an async database, URI. You talk from the memory block implementations, we also saw we can choose if we want to do something like vector memory, we can use quadrant or any other vector DB. And just to stay on track, I think I want to explore some of the more complex questions. So So how how does a new memory component decide which chat messages migrate from short term to long term storage? And can that policy be customized? So for now, it's solely based on token limit. That's not to say that that will be the case forever. But for now, the migration so while you can very much customize long term memory, our implementation of short term memory, and when information from short term memory is flushed into long term memory is based on token count, you can customize the token count yourself. So you can decide how, how long the token count is. Or how short the token count is before you flush to long term memory. That is something that you can customize the, the way we move from short term to long term. term to long term is currently limited to token count. Okay. Yeah. And let's suppose let's suppose that I have a distributed agent deployment. So let's suppose I'm not dealing with just not just one agent, but let's say I have an architecture or like, let's say I have, I don't know, some kind of architecture system where I have more than one agent, maybe there's a supervisor agent trying to determine which which task if which other team of agents, how do you ensure memory consistency across replicas? You know, how is memory? How is memory being handled when I have more than one agent in the picture? So I thought about this question before. And to me, it's, it's not such a straightforward question. So for one, maybe we can start with short term memory, which, in my view, this should probably not be replicated or shared in any way, because this short term memory should be for that agent instance. But let's talk about long term memory. When we say distributed agents, we're probably referring to a system, potentially, that should not even have shared memory. For example, if an agent is used across multiple users, we probably want to segregate long term memory across those users anyway. However, if we do want to make sure that we're retaining shared memory, I think this is where we're going to have to do a lot of work. So this is where it's going to become super important, specifically how you design your long term memory. Examples of this could be vector memory blocks, where we're making sure that for every user, we have separate collections or separate indices, while for maybe if we do want to be retaining context across agent implementations, we have a separate global index for that. So again, this really much this very much depends on the specific long term memory implementation that you have for your agent, if that makes sense. But when it comes to short term memory, I don't think this is something we actually even want to have any shared implementation across. Okay, so it's only for a system where you'd have long term memory implementation. And that makes sense, because if short term memory is just going to be in, you know, like a SQLite, SQLite in memory instance, it doesn't really make sense to use that kind of memory for something like a distributed agent system. Yes. Okay, so okay, so let's talk a little bit about tracing. So so what you know, is there any implementation or what would you recommend developers can use for tracing memory hits or misses? Is it possible to debug if an agent forgot something? Yes and no. So again, this was one of those questions that I thought long and hard about before we started the live stream. And I actually had a discussion with my colleague about this as well. I don't think that this is a good idea. Because I think it's not a good idea, because it's not a good idea. So I think actually, currently, there are great implementations to debug memory specifically, but that's not to say that we cannot debug memory. And again, I'm going to sound like a parrot repeating itself, but it will depend on the specific memory implementation. And again, let's take vector memory blocks as a great example, because we have great ways to debug semantic search, for example, we have a lot of tracing tools, we have a lot of observability tools. It's not like we're just making data Copy guess if we've fix something,ocus this data source. It is an amazing way to configure a name, let's agree, so we're already the date map. This is well defined from how we can define a topic, and we got a lot of interesting data out here. So the same thing goes if we are describing a topic in linearregistration with nodes, shall be consider put field, so it has retrieving from our memory the most relevant information. So I think I would boil this question down to, while I don't think there is the best solution to debug memory as a whole, depending on our long-term memory implementation, there are already ways to incorporate observability tracing and evaluation for those memory implementations specifically. Okay, so it would depend more on the mechanism used. So it sounds like debugging short-term memory is essentially not tenable, but you could implement mechanisms for debugging long-term memory through something like vector store and semantic similarity search. Yes, and on top of that, we do have observability tools where we can also make sure to see, for example, here's the input from the user. And what... Notice that the memory chunks we got is just irrelevant. There could be many reasons to that, and one of the reasons could be that simply the underlying LLMO agent provided a nonsensical query to actually query the memory in the first place, and we can always trace that in existing tracing tools. So again, this is not necessarily tracing and evaluating memory itself, but it is very closely coupled. If the memory block is received... If the memory block is receiving the wrong query to actually extract memory blocks from, then maybe that's something to debug. If it is receiving the correct query, then that will depend on the actual long-term memory implementation, how we evaluate it. Okay. And as we are now very, very close to the end, we'll just do this question, and then we'll address maybe one or two questions from the chat. Actually, before we get to this question, let me make a little bit of space and time to see questions from the chat. So from the audience, or to one, if you're following the chat, are there any questions you'd like to address? I'm not seeing the questions that come through YouTube specifically, so let's see. Okay. So if that's the case, maybe let's just end the stream with this final question. Okay. Thanks. So, do you think there are any lessons learned from applications where memory did improve outcomes? Yes. I would say, again, it depends. I think a lot of the problems that I've seen when it comes to memory is the memory implementation itself not being a reflection of what the application should actually do. So an example of this, and I took note of this so I can actually read it out to you, but... An example of this was an implementation, without giving a lot of detail on it, that relied on past interactions with the user, but also the most up-to-date information, specifically in this case from manuals. So you can imagine manuals, they have a lot of information, and it's about a software or hardware tool. But let's say that that software or hardware tool goes through regular updates. So the manual is changing regularly. Now, the memory implementation in this case actually skipped a beat, because there was no implementation about which memory block to give higher priority to. So in this case, imagine a user interacting with an agent about the information in this manual, and they've had questions answered in the past by the LLM based on the manual. Now that manual has changed. 10 months down the road. So the user has a very similar question. In this case, the agent kept replying based on its past interaction, which is doing the right job in terms of memory. Like the LLM, the agent is not at fault here, because it's actually checking past messages between itself and the user. However, in the meantime, the manual has changed, and the agent is not giving priority to checking the manual first before referring to the memory implementation that stores the chat. 10 months down the road. So the user is now in the chat history. Does that make sense? Yes, that makes sense. So let's make it a very simple example. Does the vehicle have four wheels or six? It used to have four wheels. So the answer was four wheels. But the manual has been updated to say it has six wheels. However, the agent is referring to the chat history initially before referring to the manual itself. So in this case, the implementation was only a very short miss in that we had to provide basically a priority queue as to what to refer to, which memory to refer to first. In this case, actually, the memory is one of them is technically actual memory, and the other is more rag. We had to refer to a vector store that retains some information for us. So the users had to go back and rethink the workflow architecture of what we search through before we answer the user. Okay. So it sounds like this is a situation where if you have constant updates to the database or to the knowledge base, if the memory is outdated, then it doesn't make a lot of sense to use. So you have to find essentially a way to keep updating either the vector store with the memory components in addition to any other tools of reference set. Is that correct? Yes, I would say so. And again, this depends on the use case because there's not a lot of application. Okay. So maybe that's where the main information base changes so rapidly. In this case, it was changing rapidly, so they had to rethink about how the memory access implementation and workflow worked. All right. So these are all my questions. So I just wanted to do a few final talking points with you, Tawana, before we finish up the session. So it sounds like this is a very powerful new addition to agentic systems, and this sort of reflects on this idea of how we're constantly evolving. And constantly pushing the boundaries of what we can do with LLMs. You know, as I mentioned at the beginning, the first implementation of working with LLMs was all about prompting them. Then we realized the LLMs hallucinates. We have to ground them. We started adding vector DBs and we developed RAG. Then we realized, oh, but what if we give access to tools to the LLMs? We then made the next jump to agent systems. And now we have this new step where in addition to having an LLM have access to RAG or an agent access to a RAG database as a tool. We can now improve. Okay. So we can now improve the LLMs ability or the agent system, the agentic systems ability to answer questions by incorporating memory. And in this session, we talked about incorporating short-term memory where the LLM or the agent will have access to chat history up to a certain token limit. But we can also add more sophisticated forms of memory that include long-term memory through something like vector stores and even custom made memory blocks. So I just want to finish up. The event. Thank you so much to Tawana for joining us today. So this was the very first Laura the legend series. I'm always on the lookout to invite new speakers into Tawana. I was so honored to have you as the very first speaker to kick off our series. So for those of you who are tuning in for the first time, who've been following Tawana through LLMA index. So we are the AI makerspace team. We are on a mission to build the world's leading community for people who want to build, ship and share like legends. We are on YouTube and Discord. So if you want to join us, you can do that. We are on Twitter and Instagram. We are on Facebook. We are on Instagram. We are on Twitter. We are on Twitter. We are on Twitter. We are on Twitter. We are on LinkedIn as well. Every week Dr. Greg and the Wiz explore cutting edge topics on AI. And I am now on the lookout for developer advocates who would like to come onto our show, tell us about their application. We'll have a chance to share with our audience what's new and we're always on the lookout to continue finding ways to build, ship and share. Like legends together. So thank you so much Tawana for joining us and thank you everyone for tuning in. We'll see you next week. Thank you. For everyone by the way who joined through YouTube, I wasn't logged in, but I will refer back to the questions you had in there because I can't see them where we're streaming it right now, but I will try to answer. That sounds great. Thank you so much. All right. Till next week, everyone. Bye bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye.",
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 6.68,
      "text": " Welcome, everybody. We've got a legendary show for you today, and I'm really excited",
      "tokens": [
        50365,
        4027,
        11,
        2201,
        13,
        492,
        600,
        658,
        257,
        16698,
        855,
        337,
        291,
        965,
        11,
        293,
        286,
        478,
        534,
        2919,
        50699
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12276661953079367,
      "compression_ratio": 1.5450980392156863,
      "no_speech_prob": 0.04426371306180954
    },
    {
      "id": 1,
      "seek": 0,
      "start": 6.68,
      "end": 15.64,
      "text": " to hand off today's session to AI Makerspace's newest YouTube star, Laura, the legend,",
      "tokens": [
        50699,
        281,
        1011,
        766,
        965,
        311,
        5481,
        281,
        7318,
        16576,
        433,
        17940,
        311,
        17569,
        3088,
        3543,
        11,
        13220,
        11,
        264,
        9451,
        11,
        51147
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12276661953079367,
      "compression_ratio": 1.5450980392156863,
      "no_speech_prob": 0.04426371306180954
    },
    {
      "id": 2,
      "seek": 0,
      "start": 15.88,
      "end": 20.400000000000002,
      "text": " Thunderbird. Welcome up, Laura. You've been part of the community for a long time.",
      "tokens": [
        51159,
        21023,
        18080,
        13,
        4027,
        493,
        11,
        13220,
        13,
        509,
        600,
        668,
        644,
        295,
        264,
        1768,
        337,
        257,
        938,
        565,
        13,
        51385
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12276661953079367,
      "compression_ratio": 1.5450980392156863,
      "no_speech_prob": 0.04426371306180954
    },
    {
      "id": 3,
      "seek": 0,
      "start": 20.76,
      "end": 24.68,
      "text": " Tell us a little bit about yourself and what we can expect today.",
      "tokens": [
        51403,
        5115,
        505,
        257,
        707,
        857,
        466,
        1803,
        293,
        437,
        321,
        393,
        2066,
        965,
        13,
        51599
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12276661953079367,
      "compression_ratio": 1.5450980392156863,
      "no_speech_prob": 0.04426371306180954
    },
    {
      "id": 4,
      "seek": 0,
      "start": 25.32,
      "end": 28.64,
      "text": " Yeah. Well, thank you so much for having me, Greg. I mean, you and I have",
      "tokens": [
        51631,
        865,
        13,
        1042,
        11,
        1309,
        291,
        370,
        709,
        337,
        1419,
        385,
        11,
        11490,
        13,
        286,
        914,
        11,
        291,
        293,
        286,
        362,
        51797
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12276661953079367,
      "compression_ratio": 1.5450980392156863,
      "no_speech_prob": 0.04426371306180954
    },
    {
      "id": 5,
      "seek": 2864,
      "start": 28.64,
      "end": 33.8,
      "text": " met each other for a few years now. You guys were teaching at Fourth Brain back in the day,",
      "tokens": [
        50365,
        1131,
        1184,
        661,
        337,
        257,
        1326,
        924,
        586,
        13,
        509,
        1074,
        645,
        4571,
        412,
        23773,
        29783,
        646,
        294,
        264,
        786,
        11,
        50623
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14104791368756975,
      "compression_ratio": 1.715151515151515,
      "no_speech_prob": 0.0026650617364794016
    },
    {
      "id": 6,
      "seek": 2864,
      "start": 33.980000000000004,
      "end": 40.68,
      "text": " and I met you through one of your bootcamps. You spun up AI Makerspace, and I kept following you",
      "tokens": [
        50632,
        293,
        286,
        1131,
        291,
        807,
        472,
        295,
        428,
        11450,
        66,
        23150,
        13,
        509,
        37038,
        493,
        7318,
        16576,
        433,
        17940,
        11,
        293,
        286,
        4305,
        3480,
        291,
        50967
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14104791368756975,
      "compression_ratio": 1.715151515151515,
      "no_speech_prob": 0.0026650617364794016
    },
    {
      "id": 7,
      "seek": 2864,
      "start": 40.68,
      "end": 45.78,
      "text": " guys. And we did a few collaborations back when I started working as a dev role at Bitewax.",
      "tokens": [
        50967,
        1074,
        13,
        400,
        321,
        630,
        257,
        1326,
        36908,
        646,
        562,
        286,
        1409,
        1364,
        382,
        257,
        1905,
        3090,
        412,
        48012,
        86,
        2797,
        13,
        51222
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14104791368756975,
      "compression_ratio": 1.715151515151515,
      "no_speech_prob": 0.0026650617364794016
    },
    {
      "id": 8,
      "seek": 2864,
      "start": 46.72,
      "end": 52.2,
      "text": " And I recently joined as a peer supporter for the AI engineering bootcamp and absolutely loved it.",
      "tokens": [
        51269,
        400,
        286,
        3938,
        6869,
        382,
        257,
        15108,
        28600,
        337,
        264,
        7318,
        7043,
        11450,
        24640,
        293,
        3122,
        4333,
        309,
        13,
        51543
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14104791368756975,
      "compression_ratio": 1.715151515151515,
      "no_speech_prob": 0.0026650617364794016
    },
    {
      "id": 9,
      "seek": 2864,
      "start": 52.6,
      "end": 56.46,
      "text": " And I think this is the first time I'll get to share this with the community, but I recently",
      "tokens": [
        51563,
        400,
        286,
        519,
        341,
        307,
        264,
        700,
        565,
        286,
        603,
        483,
        281,
        2073,
        341,
        365,
        264,
        1768,
        11,
        457,
        286,
        3938,
        51756
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14104791368756975,
      "compression_ratio": 1.715151515151515,
      "no_speech_prob": 0.0026650617364794016
    },
    {
      "id": 10,
      "seek": 2864,
      "start": 56.46,
      "end": 58.620000000000005,
      "text": " joined as a developer advocate and community leader. And I'm really excited to be here today.",
      "tokens": [
        51756,
        6869,
        382,
        257,
        10754,
        14608,
        293,
        1768,
        5263,
        13,
        400,
        286,
        478,
        534,
        2919,
        281,
        312,
        510,
        965,
        13,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14104791368756975,
      "compression_ratio": 1.715151515151515,
      "no_speech_prob": 0.0026650617364794016
    },
    {
      "id": 11,
      "seek": 5864,
      "start": 58.64,
      "end": 58.7,
      "text": " And I'm really excited to be here today. And I think this is the first time I'll get to share this with the community,",
      "tokens": [
        50365,
        400,
        286,
        478,
        534,
        2919,
        281,
        312,
        510,
        965,
        13,
        400,
        286,
        519,
        341,
        307,
        264,
        700,
        565,
        286,
        603,
        483,
        281,
        2073,
        341,
        365,
        264,
        1768,
        11,
        50368
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 12,
      "seek": 5864,
      "start": 58.7,
      "end": 58.72,
      "text": " And I think this is the first time I'll get to share this with the community,",
      "tokens": [
        50368,
        400,
        286,
        519,
        341,
        307,
        264,
        700,
        565,
        286,
        603,
        483,
        281,
        2073,
        341,
        365,
        264,
        1768,
        11,
        50369
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 13,
      "seek": 5864,
      "start": 58.72,
      "end": 65.04,
      "text": " for the AI Makerspace team, where my focus is not just doing events, but I'm also doing things like",
      "tokens": [
        50369,
        337,
        264,
        7318,
        16576,
        433,
        17940,
        1469,
        11,
        689,
        452,
        1879,
        307,
        406,
        445,
        884,
        3931,
        11,
        457,
        286,
        478,
        611,
        884,
        721,
        411,
        50685
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 14,
      "seek": 5864,
      "start": 65.04,
      "end": 68.94,
      "text": " creating social media posts, doing the newsletter, engaging with folks on Discord,",
      "tokens": [
        50685,
        4084,
        2093,
        3021,
        12300,
        11,
        884,
        264,
        26469,
        11,
        11268,
        365,
        4024,
        322,
        32623,
        11,
        50880
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 15,
      "seek": 5864,
      "start": 69.24,
      "end": 72.72,
      "text": " and highlighting the awesome things that people build, ship, and share every day with the",
      "tokens": [
        50895,
        293,
        26551,
        264,
        3476,
        721,
        300,
        561,
        1322,
        11,
        5374,
        11,
        293,
        2073,
        633,
        786,
        365,
        264,
        51069
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 16,
      "seek": 5864,
      "start": 72.72,
      "end": 78.88,
      "text": " community. Yeah. And you have made a huge impact already. And I'm really excited to start this",
      "tokens": [
        51069,
        1768,
        13,
        865,
        13,
        400,
        291,
        362,
        1027,
        257,
        2603,
        2712,
        1217,
        13,
        400,
        286,
        478,
        534,
        2919,
        281,
        722,
        341,
        51377
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 17,
      "seek": 5864,
      "start": 78.88,
      "end": 84.42,
      "text": " series, this Laura the Legend series, where you're going to be looking at different tools from",
      "tokens": [
        51377,
        2638,
        11,
        341,
        13220,
        264,
        21480,
        2638,
        11,
        689,
        291,
        434,
        516,
        281,
        312,
        1237,
        412,
        819,
        3873,
        490,
        51654
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 18,
      "seek": 5864,
      "start": 84.42,
      "end": 88.62,
      "text": " different tool stacks with different tools. And I'm really excited to be here today. And I'm really",
      "tokens": [
        51654,
        819,
        2290,
        30792,
        365,
        819,
        3873,
        13,
        400,
        286,
        478,
        534,
        2919,
        281,
        312,
        510,
        965,
        13,
        400,
        286,
        478,
        534,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.18267686631944444,
      "compression_ratio": 2.1623931623931623,
      "no_speech_prob": 0.003849298693239689
    },
    {
      "id": 19,
      "seek": 8864,
      "start": 89.06,
      "end": 95.44,
      "text": " grateful for the many different developer advocates that you've met along the way and that",
      "tokens": [
        50386,
        7941,
        337,
        264,
        867,
        819,
        10754,
        25160,
        300,
        291,
        600,
        1131,
        2051,
        264,
        636,
        293,
        300,
        50705
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 20,
      "seek": 8864,
      "start": 95.44,
      "end": 101.8,
      "text": " you've yet to meet out in the industry, tools that we just don't really have time in 52 weeks of",
      "tokens": [
        50705,
        291,
        600,
        1939,
        281,
        1677,
        484,
        294,
        264,
        3518,
        11,
        3873,
        300,
        321,
        445,
        500,
        380,
        534,
        362,
        565,
        294,
        18079,
        3259,
        295,
        51023
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 21,
      "seek": 8864,
      "start": 101.8,
      "end": 106.42,
      "text": " YouTube events to cover. The first tool today. What are we looking at and who are we going to",
      "tokens": [
        51023,
        3088,
        3931,
        281,
        2060,
        13,
        440,
        700,
        2290,
        965,
        13,
        708,
        366,
        321,
        1237,
        412,
        293,
        567,
        366,
        321,
        516,
        281,
        51254
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 22,
      "seek": 8864,
      "start": 106.42,
      "end": 111.94,
      "text": " meet? Yeah, so today we're going to be working with Tawanna Chelic from LAMA index. Tawanna",
      "tokens": [
        51254,
        1677,
        30,
        865,
        11,
        370,
        965,
        321,
        434,
        516,
        281,
        312,
        1364,
        365,
        314,
        1607,
        1795,
        761,
        338,
        299,
        490,
        441,
        38136,
        8186,
        13,
        314,
        1607,
        1795,
        51530
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 23,
      "seek": 8864,
      "start": 111.94,
      "end": 117.9,
      "text": " works as a senior developer advocate engineer. And she and I have worked a few times in the past,",
      "tokens": [
        51530,
        1985,
        382,
        257,
        7965,
        10754,
        14608,
        11403,
        13,
        400,
        750,
        293,
        286,
        362,
        2732,
        257,
        1326,
        1413,
        294,
        264,
        1791,
        11,
        51828
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 24,
      "seek": 8864,
      "start": 117.9,
      "end": 118.24000000000001,
      "text": " both in our developer advocacy roles at different places. And it's difficult for her to work my way device",
      "tokens": [
        51828,
        1293,
        294,
        527,
        10754,
        22011,
        9604,
        412,
        819,
        3190,
        13,
        400,
        309,
        311,
        2252,
        337,
        720,
        281,
        589,
        452,
        636,
        4302,
        51845
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 25,
      "seek": 8864,
      "start": 118.24000000000001,
      "end": 118.44,
      "text": " The amount appropriately I don't reckon we've ever worked in this program has had a benign challenge to",
      "tokens": [
        51845,
        440,
        2372,
        23505,
        286,
        500,
        380,
        29548,
        321,
        600,
        1562,
        2732,
        294,
        341,
        1461,
        575,
        632,
        257,
        3271,
        788,
        3430,
        281,
        51855
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 26,
      "seek": 8864,
      "start": 118.44,
      "end": 118.6,
      "text": " play. She's never had a safe job as a rol erp many decades ago. But she's a\u0412e heavyweight and she could",
      "tokens": [
        51855,
        862,
        13,
        1240,
        311,
        1128,
        632,
        257,
        3273,
        1691,
        382,
        257,
        34109,
        1189,
        79,
        867,
        7878,
        2057,
        13,
        583,
        750,
        311,
        257,
        8578,
        68,
        4676,
        12329,
        293,
        750,
        727,
        51863
      ],
      "temperature": 1.0,
      "avg_logprob": -2.012095087846833,
      "compression_ratio": 1.7971887550200802,
      "no_speech_prob": 0.008985861204564571
    },
    {
      "id": 27,
      "seek": 11860,
      "start": 118.6,
      "end": 123.08,
      "text": " points in our lives and i've always enjoyed working with her so i'm very very happy that",
      "tokens": [
        50365,
        2793,
        294,
        527,
        2909,
        293,
        741,
        600,
        1009,
        4626,
        1364,
        365,
        720,
        370,
        741,
        478,
        588,
        588,
        2055,
        300,
        50589
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07996323149083025,
      "compression_ratio": 1.8359375,
      "no_speech_prob": 0.11063824594020844
    },
    {
      "id": 28,
      "seek": 11860,
      "start": 123.08,
      "end": 128.68,
      "text": " she's the first person that's invited onto the laura the legend show let's go well welcome to",
      "tokens": [
        50589,
        750,
        311,
        264,
        700,
        954,
        300,
        311,
        9185,
        3911,
        264,
        635,
        2991,
        264,
        9451,
        855,
        718,
        311,
        352,
        731,
        2928,
        281,
        50869
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07996323149083025,
      "compression_ratio": 1.8359375,
      "no_speech_prob": 0.11063824594020844
    },
    {
      "id": 29,
      "seek": 11860,
      "start": 128.68,
      "end": 134.44,
      "text": " anna and hello to anna can you tell us a little bit about what we're going to talk about today",
      "tokens": [
        50869,
        364,
        629,
        293,
        7751,
        281,
        364,
        629,
        393,
        291,
        980,
        505,
        257,
        707,
        857,
        466,
        437,
        321,
        434,
        516,
        281,
        751,
        466,
        965,
        51157
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07996323149083025,
      "compression_ratio": 1.8359375,
      "no_speech_prob": 0.11063824594020844
    },
    {
      "id": 30,
      "seek": 11860,
      "start": 134.44,
      "end": 139.95999999999998,
      "text": " agent memory is that kind of like when chad gbt remembers your birthday is that is that what",
      "tokens": [
        51157,
        9461,
        4675,
        307,
        300,
        733,
        295,
        411,
        562,
        417,
        345,
        290,
        4517,
        26228,
        428,
        6154,
        307,
        300,
        307,
        300,
        437,
        51433
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07996323149083025,
      "compression_ratio": 1.8359375,
      "no_speech_prob": 0.11063824594020844
    },
    {
      "id": 31,
      "seek": 11860,
      "start": 139.95999999999998,
      "end": 146.35999999999999,
      "text": " that's all about uh can you kind of the high level yeah so uh we're going to be talking about agent",
      "tokens": [
        51433,
        300,
        311,
        439,
        466,
        2232,
        393,
        291,
        733,
        295,
        264,
        1090,
        1496,
        1338,
        370,
        2232,
        321,
        434,
        516,
        281,
        312,
        1417,
        466,
        9461,
        51753
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07996323149083025,
      "compression_ratio": 1.8359375,
      "no_speech_prob": 0.11063824594020844
    },
    {
      "id": 32,
      "seek": 14636,
      "start": 146.36,
      "end": 152.20000000000002,
      "text": " memory specifically um a few of the latest implementations we have in llama index and",
      "tokens": [
        50365,
        4675,
        4682,
        1105,
        257,
        1326,
        295,
        264,
        6792,
        4445,
        763,
        321,
        362,
        294,
        23272,
        8186,
        293,
        50657
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0333037487296171,
      "compression_ratio": 1.6782608695652175,
      "no_speech_prob": 0.0005447018193081021
    },
    {
      "id": 33,
      "seek": 14636,
      "start": 152.20000000000002,
      "end": 157.96,
      "text": " basically you think about yeah kind of chat gpt remembering your birthday but maybe we want",
      "tokens": [
        50657,
        1936,
        291,
        519,
        466,
        1338,
        733,
        295,
        5081,
        290,
        662,
        20719,
        428,
        6154,
        457,
        1310,
        321,
        528,
        50945
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0333037487296171,
      "compression_ratio": 1.6782608695652175,
      "no_speech_prob": 0.0005447018193081021
    },
    {
      "id": 34,
      "seek": 14636,
      "start": 158.60000000000002,
      "end": 165.32000000000002,
      "text": " agents to remember different kinds of stuff so we're going to be talking about that awesome well",
      "tokens": [
        50977,
        12554,
        281,
        1604,
        819,
        3685,
        295,
        1507,
        370,
        321,
        434,
        516,
        281,
        312,
        1417,
        466,
        300,
        3476,
        731,
        51313
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0333037487296171,
      "compression_ratio": 1.6782608695652175,
      "no_speech_prob": 0.0005447018193081021
    },
    {
      "id": 35,
      "seek": 14636,
      "start": 165.96,
      "end": 173.96,
      "text": " laura off to you it's your show thank you everybody for joining the very first laura the legend",
      "tokens": [
        51345,
        635,
        2991,
        766,
        281,
        291,
        309,
        311,
        428,
        855,
        1309,
        291,
        2201,
        337,
        5549,
        264,
        588,
        700,
        635,
        2991,
        264,
        9451,
        51745
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0333037487296171,
      "compression_ratio": 1.6782608695652175,
      "no_speech_prob": 0.0005447018193081021
    },
    {
      "id": 36,
      "seek": 14636,
      "start": 174.52,
      "end": 176.20000000000002,
      "text": " show number one",
      "tokens": [
        51773,
        855,
        1230,
        472,
        51857
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0333037487296171,
      "compression_ratio": 1.6782608695652175,
      "no_speech_prob": 0.0005447018193081021
    },
    {
      "id": 37,
      "seek": 17636,
      "start": 176.36,
      "end": 181.08,
      "text": " thank you greg all right everyone so today's session is going to be a little bit different",
      "tokens": [
        50365,
        1309,
        291,
        290,
        3375,
        439,
        558,
        1518,
        370,
        965,
        311,
        5481,
        307,
        516,
        281,
        312,
        257,
        707,
        857,
        819,
        50601
      ],
      "temperature": 0.6,
      "avg_logprob": -0.3921519965882514,
      "compression_ratio": 2.1206349206349207,
      "no_speech_prob": 0.0005859140655957162
    },
    {
      "id": 38,
      "seek": 17636,
      "start": 181.08,
      "end": 185.08,
      "text": " than what you're used to seeing with dr greg and the wiz typically you know you're used to the",
      "tokens": [
        50601,
        813,
        437,
        291,
        434,
        1143,
        281,
        2577,
        365,
        1224,
        290,
        3375,
        293,
        264,
        40808,
        5850,
        291,
        458,
        291,
        434,
        1143,
        281,
        264,
        50801
      ],
      "temperature": 0.6,
      "avg_logprob": -0.3921519965882514,
      "compression_ratio": 2.1206349206349207,
      "no_speech_prob": 0.0005859140655957162
    },
    {
      "id": 39,
      "seek": 17636,
      "start": 185.08,
      "end": 189.8,
      "text": " sessions having greg talking for quite a bit and then wiz showing some code snippets today i want",
      "tokens": [
        50801,
        11081,
        1419,
        290,
        3375,
        1417,
        337,
        1596,
        257,
        857,
        293,
        550,
        40808,
        4099,
        512,
        3089,
        35623,
        1385,
        965,
        741,
        528,
        51037
      ],
      "temperature": 0.6,
      "avg_logprob": -0.3921519965882514,
      "compression_ratio": 2.1206349206349207,
      "no_speech_prob": 0.0005859140655957162
    },
    {
      "id": 40,
      "seek": 17636,
      "start": 189.8,
      "end": 193.96,
      "text": " the session to be a little bit more interactive and to one is going to be engaging in discussion",
      "tokens": [
        51037,
        264,
        5481,
        281,
        312,
        257,
        707,
        857,
        544,
        15141,
        293,
        281,
        472,
        307,
        516,
        281,
        312,
        11268,
        294,
        5017,
        51245
      ],
      "temperature": 0.6,
      "avg_logprob": -0.3921519965882514,
      "compression_ratio": 2.1206349206349207,
      "no_speech_prob": 0.0005859140655957162
    },
    {
      "id": 41,
      "seek": 17636,
      "start": 193.96,
      "end": 199.4,
      "text": " with all of you and with me uh during the session so i've prepared some talking points and a few",
      "tokens": [
        51245,
        365,
        439,
        295,
        291,
        293,
        365,
        385,
        2232,
        1830,
        264,
        5481,
        370,
        741,
        600,
        4927,
        512,
        1417,
        2793,
        293,
        257,
        1326,
        51517
      ],
      "temperature": 0.6,
      "avg_logprob": -0.3921519965882514,
      "compression_ratio": 2.1206349206349207,
      "no_speech_prob": 0.0005859140655957162
    },
    {
      "id": 42,
      "seek": 17636,
      "start": 199.4,
      "end": 204.52,
      "text": " questions we're going to be addressing your questions at the end of the session and there's",
      "tokens": [
        51517,
        1651,
        321,
        434,
        516,
        281,
        312,
        14329,
        428,
        1651,
        412,
        264,
        917,
        295,
        264,
        5481,
        293,
        456,
        311,
        51773
      ],
      "temperature": 0.6,
      "avg_logprob": -0.3921519965882514,
      "compression_ratio": 2.1206349206349207,
      "no_speech_prob": 0.0005859140655957162
    },
    {
      "id": 43,
      "seek": 17636,
      "start": 204.52,
      "end": 206.28000000000003,
      "text": " a couple of questions that we're going to answer that are going to be helpful to all of you so i'm",
      "tokens": [
        51773,
        257,
        1916,
        295,
        1651,
        300,
        321,
        434,
        516,
        281,
        1867,
        300,
        366,
        516,
        281,
        312,
        4961,
        281,
        439,
        295,
        291,
        370,
        741,
        478,
        51861
      ],
      "temperature": 0.6,
      "avg_logprob": -0.3921519965882514,
      "compression_ratio": 2.1206349206349207,
      "no_speech_prob": 0.0005859140655957162
    },
    {
      "id": 44,
      "seek": 20636,
      "start": 206.36,
      "end": 210.14000000000001,
      "text": " the end as well. So if you have any questions, please drop them in the chat and we'll get to",
      "tokens": [
        50365,
        264,
        917,
        382,
        731,
        13,
        407,
        498,
        291,
        362,
        604,
        1651,
        11,
        1767,
        3270,
        552,
        294,
        264,
        5081,
        293,
        321,
        603,
        483,
        281,
        50554
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05522788873239726,
      "compression_ratio": 1.721189591078067,
      "no_speech_prob": 0.061560384929180145
    },
    {
      "id": 45,
      "seek": 20636,
      "start": 210.14000000000001,
      "end": 214.8,
      "text": " them towards the Q&A portion of this session. So I'm going to go ahead and get us started.",
      "tokens": [
        50554,
        552,
        3030,
        264,
        1249,
        5,
        32,
        8044,
        295,
        341,
        5481,
        13,
        407,
        286,
        478,
        516,
        281,
        352,
        2286,
        293,
        483,
        505,
        1409,
        13,
        50787
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05522788873239726,
      "compression_ratio": 1.721189591078067,
      "no_speech_prob": 0.061560384929180145
    },
    {
      "id": 46,
      "seek": 20636,
      "start": 216.22000000000003,
      "end": 223.74,
      "text": " So today we're going to be talking about this idea of enabling memory for agents. And we're",
      "tokens": [
        50858,
        407,
        965,
        321,
        434,
        516,
        281,
        312,
        1417,
        466,
        341,
        1558,
        295,
        23148,
        4675,
        337,
        12554,
        13,
        400,
        321,
        434,
        51234
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05522788873239726,
      "compression_ratio": 1.721189591078067,
      "no_speech_prob": 0.061560384929180145
    },
    {
      "id": 47,
      "seek": 20636,
      "start": 223.74,
      "end": 228.84,
      "text": " going to be talking about this as a core component of agentic systems. For those of you who have",
      "tokens": [
        51234,
        516,
        281,
        312,
        1417,
        466,
        341,
        382,
        257,
        4965,
        6542,
        295,
        9461,
        299,
        3652,
        13,
        1171,
        729,
        295,
        291,
        567,
        362,
        51489
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05522788873239726,
      "compression_ratio": 1.721189591078067,
      "no_speech_prob": 0.061560384929180145
    },
    {
      "id": 48,
      "seek": 20636,
      "start": 228.84,
      "end": 234.24,
      "text": " been developing LLM applications since the beginning, we started first by prompting an LLM",
      "tokens": [
        51489,
        668,
        6416,
        441,
        43,
        44,
        5821,
        1670,
        264,
        2863,
        11,
        321,
        1409,
        700,
        538,
        12391,
        278,
        364,
        441,
        43,
        44,
        51759
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05522788873239726,
      "compression_ratio": 1.721189591078067,
      "no_speech_prob": 0.061560384929180145
    },
    {
      "id": 49,
      "seek": 23424,
      "start": 234.24,
      "end": 242.14000000000001,
      "text": " by making API calls. We then started exploring this idea of improving the way LLM responds by",
      "tokens": [
        50365,
        538,
        1455,
        9362,
        5498,
        13,
        492,
        550,
        1409,
        12736,
        341,
        1558,
        295,
        11470,
        264,
        636,
        441,
        43,
        44,
        27331,
        538,
        50760
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14071929454803467,
      "compression_ratio": 1.6970684039087949,
      "no_speech_prob": 0.002546179573982954
    },
    {
      "id": 50,
      "seek": 23424,
      "start": 242.14000000000001,
      "end": 248.02,
      "text": " including a database. So we talked about this idea of building retrieval augmented generation.",
      "tokens": [
        50760,
        3009,
        257,
        8149,
        13,
        407,
        321,
        2825,
        466,
        341,
        1558,
        295,
        2390,
        19817,
        3337,
        36155,
        5125,
        13,
        51054
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14071929454803467,
      "compression_ratio": 1.6970684039087949,
      "no_speech_prob": 0.002546179573982954
    },
    {
      "id": 51,
      "seek": 23424,
      "start": 248.60000000000002,
      "end": 254.94,
      "text": " And then the next step we took was to give LLMs access to tools. And we defined this in broad",
      "tokens": [
        51083,
        400,
        550,
        264,
        958,
        1823,
        321,
        1890,
        390,
        281,
        976,
        441,
        43,
        26386,
        2105,
        281,
        3873,
        13,
        400,
        321,
        7642,
        341,
        294,
        4152,
        51400
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14071929454803467,
      "compression_ratio": 1.6970684039087949,
      "no_speech_prob": 0.002546179573982954
    },
    {
      "id": 52,
      "seek": 23424,
      "start": 254.94,
      "end": 261.72,
      "text": " terms as agents, where LLMs have the capacity to autonomously decide which tools to use to",
      "tokens": [
        51400,
        2115,
        382,
        12554,
        11,
        689,
        441,
        43,
        26386,
        362,
        264,
        6042,
        281,
        18203,
        5098,
        4536,
        597,
        3873,
        281,
        764,
        281,
        51739
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14071929454803467,
      "compression_ratio": 1.6970684039087949,
      "no_speech_prob": 0.002546179573982954
    },
    {
      "id": 53,
      "seek": 23424,
      "start": 261.72,
      "end": 263.14,
      "text": " complete a specific purpose.",
      "tokens": [
        51739,
        3566,
        257,
        2685,
        4334,
        13,
        51810
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14071929454803467,
      "compression_ratio": 1.6970684039087949,
      "no_speech_prob": 0.002546179573982954
    },
    {
      "id": 54,
      "seek": 23424,
      "start": 263.72,
      "end": 263.94,
      "text": " So let's get started.",
      "tokens": [
        51839,
        407,
        718,
        311,
        483,
        1409,
        13,
        51850
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14071929454803467,
      "compression_ratio": 1.6970684039087949,
      "no_speech_prob": 0.002546179573982954
    },
    {
      "id": 55,
      "seek": 23424,
      "start": 263.94,
      "end": 264.22,
      "text": " Today we're going to be talking about the idea of enabling memory for agents. And we're going to",
      "tokens": [
        51850,
        2692,
        321,
        434,
        516,
        281,
        312,
        1417,
        466,
        264,
        1558,
        295,
        23148,
        4675,
        337,
        12554,
        13,
        400,
        321,
        434,
        516,
        281,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14071929454803467,
      "compression_ratio": 1.6970684039087949,
      "no_speech_prob": 0.002546179573982954
    },
    {
      "id": 56,
      "seek": 26422,
      "start": 264.22,
      "end": 269.52000000000004,
      "text": " take the next step in that evolution. And we're going to be talking about how we can incorporate",
      "tokens": [
        50365,
        747,
        264,
        958,
        1823,
        294,
        300,
        9303,
        13,
        400,
        321,
        434,
        516,
        281,
        312,
        1417,
        466,
        577,
        321,
        393,
        16091,
        50630
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08091897964477539,
      "compression_ratio": 1.8758389261744965,
      "no_speech_prob": 0.03042946197092533
    },
    {
      "id": 57,
      "seek": 26422,
      "start": 269.52000000000004,
      "end": 276.92,
      "text": " memory in agentic systems. So in LLMA index, we can typically customize memory by using an existing",
      "tokens": [
        50630,
        4675,
        294,
        9461,
        299,
        3652,
        13,
        407,
        294,
        441,
        43,
        9998,
        8186,
        11,
        321,
        393,
        5850,
        19734,
        4675,
        538,
        1228,
        364,
        6741,
        51000
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08091897964477539,
      "compression_ratio": 1.8758389261744965,
      "no_speech_prob": 0.03042946197092533
    },
    {
      "id": 58,
      "seek": 26422,
      "start": 276.92,
      "end": 282.06,
      "text": " base memory class. So this idea of the base memory class is something that Tawana is going to be",
      "tokens": [
        51000,
        3096,
        4675,
        1508,
        13,
        407,
        341,
        1558,
        295,
        264,
        3096,
        4675,
        1508,
        307,
        746,
        300,
        314,
        1607,
        2095,
        307,
        516,
        281,
        312,
        51257
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08091897964477539,
      "compression_ratio": 1.8758389261744965,
      "no_speech_prob": 0.03042946197092533
    },
    {
      "id": 59,
      "seek": 26422,
      "start": 282.06,
      "end": 286.5,
      "text": " telling us a little bit more about. And we can also create custom ones. So we'll have code snippets",
      "tokens": [
        51257,
        3585,
        505,
        257,
        707,
        857,
        544,
        466,
        13,
        400,
        321,
        393,
        611,
        1884,
        2375,
        2306,
        13,
        407,
        321,
        603,
        362,
        3089,
        35623,
        1385,
        51479
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08091897964477539,
      "compression_ratio": 1.8758389261744965,
      "no_speech_prob": 0.03042946197092533
    },
    {
      "id": 60,
      "seek": 26422,
      "start": 286.5,
      "end": 292.24,
      "text": " towards a few slides after this, where Tawana will explain to us this idea of what the base memory",
      "tokens": [
        51479,
        3030,
        257,
        1326,
        9788,
        934,
        341,
        11,
        689,
        314,
        1607,
        2095,
        486,
        2903,
        281,
        505,
        341,
        1558,
        295,
        437,
        264,
        3096,
        4675,
        51766
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08091897964477539,
      "compression_ratio": 1.8758389261744965,
      "no_speech_prob": 0.03042946197092533
    },
    {
      "id": 61,
      "seek": 26422,
      "start": 292.24,
      "end": 293.92,
      "text": " class is and how we can create a custom one. So let's get started.",
      "tokens": [
        51766,
        1508,
        307,
        293,
        577,
        321,
        393,
        1884,
        257,
        2375,
        472,
        13,
        407,
        718,
        311,
        483,
        1409,
        13,
        51850
      ],
      "temperature": 0.0,
      "avg_logprob": -0.08091897964477539,
      "compression_ratio": 1.8758389261744965,
      "no_speech_prob": 0.03042946197092533
    },
    {
      "id": 62,
      "seek": 29392,
      "start": 293.92,
      "end": 302.6,
      "text": " So within this system, as the agent is running, the agent will make calls to the memory.put method",
      "tokens": [
        50365,
        407,
        1951,
        341,
        1185,
        11,
        382,
        264,
        9461,
        307,
        2614,
        11,
        264,
        9461,
        486,
        652,
        5498,
        281,
        264,
        4675,
        13,
        2582,
        3170,
        50799
      ],
      "temperature": 0.6,
      "avg_logprob": -0.14872260921257585,
      "compression_ratio": 1.904382470119522,
      "no_speech_prob": 0.020620496943593025
    },
    {
      "id": 63,
      "seek": 29392,
      "start": 302.6,
      "end": 307.8,
      "text": " to store information and then use the memory.get method to retrieve information. So we have this",
      "tokens": [
        50799,
        281,
        3531,
        1589,
        293,
        550,
        764,
        264,
        4675,
        13,
        847,
        3170,
        281,
        30254,
        1589,
        13,
        407,
        321,
        362,
        341,
        51059
      ],
      "temperature": 0.6,
      "avg_logprob": -0.14872260921257585,
      "compression_ratio": 1.904382470119522,
      "no_speech_prob": 0.020620496943593025
    },
    {
      "id": 64,
      "seek": 29392,
      "start": 307.8,
      "end": 313.36,
      "text": " memory component, and we're going to have two methods, put and get, to enable the agent to",
      "tokens": [
        51059,
        4675,
        6542,
        11,
        293,
        321,
        434,
        516,
        281,
        362,
        732,
        7150,
        11,
        829,
        293,
        483,
        11,
        281,
        9528,
        264,
        9461,
        281,
        51337
      ],
      "temperature": 0.6,
      "avg_logprob": -0.14872260921257585,
      "compression_ratio": 1.904382470119522,
      "no_speech_prob": 0.020620496943593025
    },
    {
      "id": 65,
      "seek": 29392,
      "start": 313.36,
      "end": 320.48,
      "text": " retrieve information. To configure memory for an agent, again, we're going to be passing it to",
      "tokens": [
        51337,
        30254,
        1589,
        13,
        1407,
        22162,
        4675,
        337,
        364,
        9461,
        11,
        797,
        11,
        321,
        434,
        516,
        281,
        312,
        8437,
        309,
        281,
        51693
      ],
      "temperature": 0.6,
      "avg_logprob": -0.14872260921257585,
      "compression_ratio": 1.904382470119522,
      "no_speech_prob": 0.020620496943593025
    },
    {
      "id": 66,
      "seek": 29392,
      "start": 320.48,
      "end": 323.90000000000003,
      "text": " a run method. So we'll get a chance to see a few code snippets in a little bit. But first, let's",
      "tokens": [
        51693,
        257,
        1190,
        3170,
        13,
        407,
        321,
        603,
        483,
        257,
        2931,
        281,
        536,
        257,
        1326,
        3089,
        35623,
        1385,
        294,
        257,
        707,
        857,
        13,
        583,
        700,
        11,
        718,
        311,
        51864
      ],
      "temperature": 0.6,
      "avg_logprob": -0.14872260921257585,
      "compression_ratio": 1.904382470119522,
      "no_speech_prob": 0.020620496943593025
    },
    {
      "id": 67,
      "seek": 32390,
      "start": 323.9,
      "end": 331.73999999999995,
      "text": " bit so um juana can you let's let's let's explore this code snippet for a little bit so i'm working",
      "tokens": [
        50365,
        857,
        370,
        1105,
        3649,
        2095,
        393,
        291,
        718,
        311,
        718,
        311,
        718,
        311,
        6839,
        341,
        3089,
        35623,
        302,
        337,
        257,
        707,
        857,
        370,
        741,
        478,
        1364,
        50757
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0643189990002176,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 0.08594700694084167
    },
    {
      "id": 68,
      "seek": 32390,
      "start": 331.73999999999995,
      "end": 337.65999999999997,
      "text": " with llama index which as many of you know it is a python it provides a python package for you to",
      "tokens": [
        50757,
        365,
        23272,
        8186,
        597,
        382,
        867,
        295,
        291,
        458,
        309,
        307,
        257,
        38797,
        309,
        6417,
        257,
        38797,
        7372,
        337,
        291,
        281,
        51053
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0643189990002176,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 0.08594700694084167
    },
    {
      "id": 69,
      "seek": 32390,
      "start": 337.65999999999997,
      "end": 344.29999999999995,
      "text": " build llm applications initially we worked with these applications uh with a focus on rag and",
      "tokens": [
        51053,
        1322,
        4849,
        76,
        5821,
        9105,
        321,
        2732,
        365,
        613,
        5821,
        2232,
        365,
        257,
        1879,
        322,
        17539,
        293,
        51385
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0643189990002176,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 0.08594700694084167
    },
    {
      "id": 70,
      "seek": 32390,
      "start": 344.29999999999995,
      "end": 349.82,
      "text": " later evolved to incorporate agentic applications so tell me tell me more a little bit about what",
      "tokens": [
        51385,
        1780,
        14178,
        281,
        16091,
        9461,
        299,
        5821,
        370,
        980,
        385,
        980,
        385,
        544,
        257,
        707,
        857,
        466,
        437,
        51661
      ],
      "temperature": 0.0,
      "avg_logprob": -0.0643189990002176,
      "compression_ratio": 1.792626728110599,
      "no_speech_prob": 0.08594700694084167
    },
    {
      "id": 71,
      "seek": 34982,
      "start": 349.82,
      "end": 355.5,
      "text": " this code snippet is doing and and how we can incorporate memory in as part of our agentic",
      "tokens": [
        50365,
        341,
        3089,
        35623,
        302,
        307,
        884,
        293,
        293,
        577,
        321,
        393,
        16091,
        4675,
        294,
        382,
        644,
        295,
        527,
        9461,
        299,
        50649
      ],
      "temperature": 0.6,
      "avg_logprob": -0.4160210951319281,
      "compression_ratio": 2.0521472392638036,
      "no_speech_prob": 0.0009856449905782938
    },
    {
      "id": 72,
      "seek": 34982,
      "start": 355.5,
      "end": 361.09999999999997,
      "text": " applications so this is the most basic form of memory i would say so you can think of this as",
      "tokens": [
        50649,
        5821,
        370,
        341,
        307,
        264,
        881,
        3875,
        1254,
        295,
        4675,
        741,
        576,
        584,
        370,
        291,
        393,
        519,
        295,
        341,
        382,
        50929
      ],
      "temperature": 0.6,
      "avg_logprob": -0.4160210951319281,
      "compression_ratio": 2.0521472392638036,
      "no_speech_prob": 0.0009856449905782938
    },
    {
      "id": 73,
      "seek": 34982,
      "start": 361.09999999999997,
      "end": 367.02,
      "text": " short term memory um and i think we can start looking into more complex types of memory in",
      "tokens": [
        50929,
        2099,
        1433,
        4675,
        1105,
        293,
        741,
        519,
        321,
        393,
        722,
        1237,
        666,
        544,
        3997,
        3467,
        295,
        4675,
        294,
        51225
      ],
      "temperature": 0.6,
      "avg_logprob": -0.4160210951319281,
      "compression_ratio": 2.0521472392638036,
      "no_speech_prob": 0.0009856449905782938
    },
    {
      "id": 74,
      "seek": 34982,
      "start": 367.02,
      "end": 373.26,
      "text": " a bit but what's going on here is ultimately whenever we talk with or chat with an llm or",
      "tokens": [
        51225,
        257,
        857,
        457,
        437,
        311,
        516,
        322,
        510,
        307,
        6284,
        5699,
        321,
        751,
        365,
        420,
        5081,
        365,
        364,
        4849,
        76,
        420,
        51537
      ],
      "temperature": 0.6,
      "avg_logprob": -0.4160210951319281,
      "compression_ratio": 2.0521472392638036,
      "no_speech_prob": 0.0009856449905782938
    },
    {
      "id": 75,
      "seek": 34982,
      "start": 373.26,
      "end": 379.18,
      "text": " an agent let's let's just call it agent from now on we hopefully wanted to be able to retain some",
      "tokens": [
        51537,
        364,
        9461,
        718,
        311,
        718,
        311,
        445,
        818,
        309,
        9461,
        490,
        586,
        322,
        321,
        4696,
        1415,
        281,
        312,
        1075,
        281,
        18340,
        512,
        51833
      ],
      "temperature": 0.6,
      "avg_logprob": -0.4160210951319281,
      "compression_ratio": 2.0521472392638036,
      "no_speech_prob": 0.0009856449905782938
    },
    {
      "id": 76,
      "seek": 34982,
      "start": 379.18,
      "end": 379.74,
      "text": " of the information that we had from the original code snippet but we did not have enough to do everything",
      "tokens": [
        51833,
        295,
        264,
        1589,
        300,
        321,
        632,
        490,
        264,
        3380,
        3089,
        35623,
        302,
        457,
        321,
        630,
        406,
        362,
        1547,
        281,
        360,
        1203,
        51861
      ],
      "temperature": 0.6,
      "avg_logprob": -0.4160210951319281,
      "compression_ratio": 2.0521472392638036,
      "no_speech_prob": 0.0009856449905782938
    },
    {
      "id": 77,
      "seek": 34982,
      "start": 379.74,
      "end": 379.78,
      "text": " but we did have to do everything from the original code snippet to the original code snippet to the",
      "tokens": [
        51861,
        457,
        321,
        630,
        362,
        281,
        360,
        1203,
        490,
        264,
        3380,
        3089,
        35623,
        302,
        281,
        264,
        3380,
        3089,
        35623,
        302,
        281,
        264,
        51863
      ],
      "temperature": 0.6,
      "avg_logprob": -0.4160210951319281,
      "compression_ratio": 2.0521472392638036,
      "no_speech_prob": 0.0009856449905782938
    },
    {
      "id": 78,
      "seek": 37978,
      "start": 379.78,
      "end": 383.84,
      "text": " of our past conversations while the chat is ongoing.",
      "tokens": [
        50365,
        295,
        527,
        1791,
        7315,
        1339,
        264,
        5081,
        307,
        10452,
        13,
        50568
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1927891625298394,
      "compression_ratio": 1.573394495412844,
      "no_speech_prob": 0.039680253714323044
    },
    {
      "id": 79,
      "seek": 37978,
      "start": 385.46,
      "end": 389.2,
      "text": " Later on in future, we maybe also want to come back to that chat,",
      "tokens": [
        50649,
        11965,
        322,
        294,
        2027,
        11,
        321,
        1310,
        611,
        528,
        281,
        808,
        646,
        281,
        300,
        5081,
        11,
        50836
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1927891625298394,
      "compression_ratio": 1.573394495412844,
      "no_speech_prob": 0.039680253714323044
    },
    {
      "id": 80,
      "seek": 37978,
      "start": 389.29999999999995,
      "end": 392.0,
      "text": " but let's assume we're just within one chat instance.",
      "tokens": [
        50841,
        457,
        718,
        311,
        6552,
        321,
        434,
        445,
        1951,
        472,
        5081,
        5197,
        13,
        50976
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1927891625298394,
      "compression_ratio": 1.573394495412844,
      "no_speech_prob": 0.039680253714323044
    },
    {
      "id": 81,
      "seek": 37978,
      "start": 392.88,
      "end": 396.28,
      "text": " And what's happening here is we're basically providing our agent",
      "tokens": [
        51020,
        400,
        437,
        311,
        2737,
        510,
        307,
        321,
        434,
        1936,
        6530,
        527,
        9461,
        51190
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1927891625298394,
      "compression_ratio": 1.573394495412844,
      "no_speech_prob": 0.039680253714323044
    },
    {
      "id": 82,
      "seek": 37978,
      "start": 396.28,
      "end": 398.11999999999995,
      "text": " with a memory block.",
      "tokens": [
        51190,
        365,
        257,
        4675,
        3461,
        13,
        51282
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1927891625298394,
      "compression_ratio": 1.573394495412844,
      "no_speech_prob": 0.039680253714323044
    },
    {
      "id": 83,
      "seek": 37978,
      "start": 398.7,
      "end": 403.35999999999996,
      "text": " And by default, this memory block in Lama index",
      "tokens": [
        51311,
        400,
        538,
        7576,
        11,
        341,
        4675,
        3461,
        294,
        441,
        2404,
        8186,
        51544
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1927891625298394,
      "compression_ratio": 1.573394495412844,
      "no_speech_prob": 0.039680253714323044
    },
    {
      "id": 84,
      "seek": 37978,
      "start": 403.35999999999996,
      "end": 406.7,
      "text": " is limited by a certain token limit.",
      "tokens": [
        51544,
        307,
        5567,
        538,
        257,
        1629,
        14862,
        4948,
        13,
        51711
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1927891625298394,
      "compression_ratio": 1.573394495412844,
      "no_speech_prob": 0.039680253714323044
    },
    {
      "id": 85,
      "seek": 40670,
      "start": 406.7,
      "end": 412.18,
      "text": " So after that token limit, we will want to flush that memory to something else,",
      "tokens": [
        50365,
        407,
        934,
        300,
        14862,
        4948,
        11,
        321,
        486,
        528,
        281,
        19568,
        300,
        4675,
        281,
        746,
        1646,
        11,
        50639
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 86,
      "seek": 40670,
      "start": 412.26,
      "end": 413.84,
      "text": " but let's ignore that for now.",
      "tokens": [
        50643,
        457,
        718,
        311,
        11200,
        300,
        337,
        586,
        13,
        50722
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 87,
      "seek": 40670,
      "start": 414.5,
      "end": 417.5,
      "text": " Basically, what we're doing here is we're allowing our agent",
      "tokens": [
        50755,
        8537,
        11,
        437,
        321,
        434,
        884,
        510,
        307,
        321,
        434,
        8293,
        527,
        9461,
        50905
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 88,
      "seek": 40670,
      "start": 417.5,
      "end": 421.3,
      "text": " to retain chat history given this memory block,",
      "tokens": [
        50905,
        281,
        18340,
        5081,
        2503,
        2212,
        341,
        4675,
        3461,
        11,
        51095
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 89,
      "seek": 40670,
      "start": 421.5,
      "end": 423.74,
      "text": " this default memory block in Lama index.",
      "tokens": [
        51105,
        341,
        7576,
        4675,
        3461,
        294,
        441,
        2404,
        8186,
        13,
        51217
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 90,
      "seek": 40670,
      "start": 425.74,
      "end": 427.06,
      "text": " So tell me this idea.",
      "tokens": [
        51317,
        407,
        980,
        385,
        341,
        1558,
        13,
        51383
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 91,
      "seek": 40670,
      "start": 427.36,
      "end": 431.24,
      "text": " So tell me more a little bit about this from defaults method.",
      "tokens": [
        51398,
        407,
        980,
        385,
        544,
        257,
        707,
        857,
        466,
        341,
        490,
        7576,
        82,
        3170,
        13,
        51592
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 92,
      "seek": 40670,
      "start": 431.44,
      "end": 434.09999999999997,
      "text": " So explain a little bit more what's going on.",
      "tokens": [
        51602,
        407,
        2903,
        257,
        707,
        857,
        544,
        437,
        311,
        516,
        322,
        13,
        51735
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 93,
      "seek": 40670,
      "start": 434.12,
      "end": 435.8,
      "text": " So it seems like I'm setting a token limit.",
      "tokens": [
        51736,
        407,
        309,
        2544,
        411,
        286,
        478,
        3287,
        257,
        14862,
        4948,
        13,
        51820
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 94,
      "seek": 40670,
      "start": 435.8,
      "end": 436.59999999999997,
      "text": " I can choose.",
      "tokens": [
        51820,
        286,
        393,
        2826,
        13,
        51860
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12649343847259273,
      "compression_ratio": 1.7707509881422925,
      "no_speech_prob": 3.7097535823704675e-05
    },
    {
      "id": 95,
      "seek": 43670,
      "start": 436.7,
      "end": 439.15999999999997,
      "text": " I can choose the size of the token limit.",
      "tokens": [
        50365,
        286,
        393,
        2826,
        264,
        2744,
        295,
        264,
        14862,
        4948,
        13,
        50488
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 96,
      "seek": 43670,
      "start": 439.59999999999997,
      "end": 442.88,
      "text": " So tell me a little bit more about the role of a token limit in the memory.",
      "tokens": [
        50510,
        407,
        980,
        385,
        257,
        707,
        857,
        544,
        466,
        264,
        3090,
        295,
        257,
        14862,
        4948,
        294,
        264,
        4675,
        13,
        50674
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 97,
      "seek": 43670,
      "start": 443.12,
      "end": 447.09999999999997,
      "text": " I think from defaults is an implementation detail of Lama index.",
      "tokens": [
        50686,
        286,
        519,
        490,
        7576,
        82,
        307,
        364,
        11420,
        2607,
        295,
        441,
        2404,
        8186,
        13,
        50885
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 98,
      "seek": 43670,
      "start": 447.21999999999997,
      "end": 449.7,
      "text": " So I wouldn't focus too much on that.",
      "tokens": [
        50891,
        407,
        286,
        2759,
        380,
        1879,
        886,
        709,
        322,
        300,
        13,
        51015
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 99,
      "seek": 43670,
      "start": 449.76,
      "end": 453.82,
      "text": " But basically what we're doing here is we're telling our code",
      "tokens": [
        51018,
        583,
        1936,
        437,
        321,
        434,
        884,
        510,
        307,
        321,
        434,
        3585,
        527,
        3089,
        51221
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 100,
      "seek": 43670,
      "start": 453.82,
      "end": 457.14,
      "text": " that there is a memory component within Lama index.",
      "tokens": [
        51221,
        300,
        456,
        307,
        257,
        4675,
        6542,
        1951,
        441,
        2404,
        8186,
        13,
        51387
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 101,
      "seek": 43670,
      "start": 457.32,
      "end": 460.64,
      "text": " And I want to make use of that memory component",
      "tokens": [
        51396,
        400,
        286,
        528,
        281,
        652,
        764,
        295,
        300,
        4675,
        6542,
        51562
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 102,
      "seek": 43670,
      "start": 460.64,
      "end": 465.94,
      "text": " by providing it some of my own specific token limit",
      "tokens": [
        51562,
        538,
        6530,
        309,
        512,
        295,
        452,
        1065,
        2685,
        14862,
        4948,
        51827
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 103,
      "seek": 43670,
      "start": 465.94,
      "end": 466.64,
      "text": " and setting.",
      "tokens": [
        51827,
        293,
        3287,
        13,
        51862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14341551570569055,
      "compression_ratio": 1.7392996108949417,
      "no_speech_prob": 0.00016944872913882136
    },
    {
      "id": 104,
      "seek": 46670,
      "start": 466.71999999999997,
      "end": 469.12,
      "text": " Okay, effect,",
      "tokens": [
        50366,
        1033,
        11,
        1802,
        11,
        50486
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 105,
      "seek": 46670,
      "start": 469.18,
      "end": 471.78,
      "text": " so in Orbit code,",
      "tokens": [
        50489,
        370,
        294,
        1610,
        5260,
        3089,
        11,
        50619
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 106,
      "seek": 46670,
      "start": 471.82,
      "end": 473.36,
      "text": " I'm going to sleep with feedback.",
      "tokens": [
        50621,
        286,
        478,
        516,
        281,
        2817,
        365,
        5824,
        13,
        50698
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 107,
      "seek": 46670,
      "start": 473.4,
      "end": 476.28,
      "text": " And then I'm just going to introduce the maximum amount of time",
      "tokens": [
        50700,
        400,
        550,
        286,
        478,
        445,
        516,
        281,
        5366,
        264,
        6674,
        2372,
        295,
        565,
        50844
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 108,
      "seek": 46670,
      "start": 476.3,
      "end": 478.36,
      "text": " to which this is happening at every layer.",
      "tokens": [
        50845,
        281,
        597,
        341,
        307,
        2737,
        412,
        633,
        4583,
        13,
        50948
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 109,
      "seek": 46670,
      "start": 478.62,
      "end": 480.59999999999997,
      "text": " So I'm going to keep that in memory,",
      "tokens": [
        50961,
        407,
        286,
        478,
        516,
        281,
        1066,
        300,
        294,
        4675,
        11,
        51060
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 110,
      "seek": 46670,
      "start": 480.62,
      "end": 481.76,
      "text": " focusing on one area.",
      "tokens": [
        51061,
        8416,
        322,
        472,
        1859,
        13,
        51118
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 111,
      "seek": 46670,
      "start": 481.76,
      "end": 482.82,
      "text": " And in public,",
      "tokens": [
        51118,
        400,
        294,
        1908,
        11,
        51171
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 112,
      "seek": 46670,
      "start": 482.84,
      "end": 487.06,
      "text": " somehow it may expect my random disappear force to shut for a small one,",
      "tokens": [
        51172,
        6063,
        309,
        815,
        2066,
        452,
        4974,
        11596,
        3464,
        281,
        5309,
        337,
        257,
        1359,
        472,
        11,
        51383
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 113,
      "seek": 46670,
      "start": 487.46,
      "end": 487.9,
      "text": " so I can sort of cause more automation or jerk",
      "tokens": [
        51403,
        370,
        286,
        393,
        1333,
        295,
        3082,
        544,
        17769,
        420,
        25197,
        51425
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 114,
      "seek": 46670,
      "start": 489.12,
      "end": 489.18,
      "text": " there.",
      "tokens": [
        51486,
        456,
        13,
        51489
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 115,
      "seek": 46670,
      "start": 490.0,
      "end": 491.65999999999997,
      "text": " Then I can\u591c wait 24 hours,",
      "tokens": [
        51530,
        1396,
        286,
        393,
        30124,
        1699,
        4022,
        2496,
        11,
        51613
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 116,
      "seek": 46670,
      "start": 491.71999999999997,
      "end": 492.58,
      "text": " then I get the\u0438\u043b\u044f,",
      "tokens": [
        51616,
        550,
        286,
        483,
        264,
        50211,
        11,
        51659
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 117,
      "seek": 46670,
      "start": 492.59999999999997,
      "end": 493.09999999999997,
      "text": " then I \u043f\u0435\u0440w Zeit,",
      "tokens": [
        51660,
        550,
        286,
        4321,
        86,
        9394,
        11,
        51685
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 118,
      "seek": 46670,
      "start": 493.12,
      "end": 495.32,
      "text": " and close every website.",
      "tokens": [
        51686,
        293,
        1998,
        633,
        3144,
        13,
        51796
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 119,
      "seek": 46670,
      "start": 495.58,
      "end": 496.02,
      "text": " Hold on just a minute.",
      "tokens": [
        51809,
        6962,
        322,
        445,
        257,
        3456,
        13,
        51831
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 120,
      "seek": 46670,
      "start": 496.02,
      "end": 496.09999999999997,
      "text": " Okay.",
      "tokens": [
        51831,
        1033,
        13,
        51835
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 121,
      "seek": 46670,
      "start": 496.09999999999997,
      "end": 496.58,
      "text": " One last thing to note,",
      "tokens": [
        51835,
        1485,
        1036,
        551,
        281,
        3637,
        11,
        51859
      ],
      "temperature": 1.0,
      "avg_logprob": -3.86831945875671,
      "compression_ratio": 1.5247813411078717,
      "no_speech_prob": 0.001308016711845994
    },
    {
      "id": 122,
      "seek": 49658,
      "start": 496.58,
      "end": 500.7,
      "text": " token limit, we're basically telling our agent that this is the amount I'm going to allow",
      "tokens": [
        50365,
        14862,
        4948,
        11,
        321,
        434,
        1936,
        3585,
        527,
        9461,
        300,
        341,
        307,
        264,
        2372,
        286,
        478,
        516,
        281,
        2089,
        50571
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12587436975217334,
      "compression_ratio": 1.6452830188679246,
      "no_speech_prob": 0.025610582903027534
    },
    {
      "id": 123,
      "seek": 49658,
      "start": 500.7,
      "end": 508.52,
      "text": " my in memory, short term memory to hold. And after that point, I want you to actually start",
      "tokens": [
        50571,
        452,
        294,
        4675,
        11,
        2099,
        1433,
        4675,
        281,
        1797,
        13,
        400,
        934,
        300,
        935,
        11,
        286,
        528,
        291,
        281,
        767,
        722,
        50962
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12587436975217334,
      "compression_ratio": 1.6452830188679246,
      "no_speech_prob": 0.025610582903027534
    },
    {
      "id": 124,
      "seek": 49658,
      "start": 508.52,
      "end": 514.34,
      "text": " storing memory somewhere else somewhere maybe a bit more sophisticated that I can also access",
      "tokens": [
        50962,
        26085,
        4675,
        4079,
        1646,
        4079,
        1310,
        257,
        857,
        544,
        16950,
        300,
        286,
        393,
        611,
        2105,
        51253
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12587436975217334,
      "compression_ratio": 1.6452830188679246,
      "no_speech_prob": 0.025610582903027534
    },
    {
      "id": 125,
      "seek": 49658,
      "start": 514.34,
      "end": 518.8199999999999,
      "text": " as the agent is running. So that is the main thing that's happening here.",
      "tokens": [
        51253,
        382,
        264,
        9461,
        307,
        2614,
        13,
        407,
        300,
        307,
        264,
        2135,
        551,
        300,
        311,
        2737,
        510,
        13,
        51477
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12587436975217334,
      "compression_ratio": 1.6452830188679246,
      "no_speech_prob": 0.025610582903027534
    },
    {
      "id": 126,
      "seek": 49658,
      "start": 518.8199999999999,
      "end": 523.68,
      "text": " Okay, and I just have a quick question. So maybe just to summarize my understanding of",
      "tokens": [
        51477,
        1033,
        11,
        293,
        286,
        445,
        362,
        257,
        1702,
        1168,
        13,
        407,
        1310,
        445,
        281,
        20858,
        452,
        3701,
        295,
        51720
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12587436975217334,
      "compression_ratio": 1.6452830188679246,
      "no_speech_prob": 0.025610582903027534
    },
    {
      "id": 127,
      "seek": 52368,
      "start": 523.68,
      "end": 528.6999999999999,
      "text": " what you're sharing with us. So I will define my LLM, you know, typically, I'll be making",
      "tokens": [
        50365,
        437,
        291,
        434,
        5414,
        365,
        505,
        13,
        407,
        286,
        486,
        6964,
        452,
        441,
        43,
        44,
        11,
        291,
        458,
        11,
        5850,
        11,
        286,
        603,
        312,
        1455,
        50616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16315996365284358,
      "compression_ratio": 1.6884057971014492,
      "no_speech_prob": 0.0038789622485637665
    },
    {
      "id": 128,
      "seek": 52368,
      "start": 528.6999999999999,
      "end": 534.5,
      "text": " some kind of API call to either open AI models from hugging phase models hosted on Azure,",
      "tokens": [
        50616,
        512,
        733,
        295,
        9362,
        818,
        281,
        2139,
        1269,
        7318,
        5245,
        490,
        41706,
        5574,
        5245,
        19204,
        322,
        11969,
        11,
        50906
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16315996365284358,
      "compression_ratio": 1.6884057971014492,
      "no_speech_prob": 0.0038789622485637665
    },
    {
      "id": 129,
      "seek": 52368,
      "start": 534.5,
      "end": 539.9599999999999,
      "text": " AWS, etc, I will define my tools. And then I will say, Okay, so you you're this system",
      "tokens": [
        50906,
        17650,
        11,
        5183,
        11,
        286,
        486,
        6964,
        452,
        3873,
        13,
        400,
        550,
        286,
        486,
        584,
        11,
        1033,
        11,
        370,
        291,
        291,
        434,
        341,
        1185,
        51179
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16315996365284358,
      "compression_ratio": 1.6884057971014492,
      "no_speech_prob": 0.0038789622485637665
    },
    {
      "id": 130,
      "seek": 52368,
      "start": 539.9599999999999,
      "end": 545.4599999999999,
      "text": " is going to retain memory up to this token limit. So any messages from that that are",
      "tokens": [
        51179,
        307,
        516,
        281,
        18340,
        4675,
        493,
        281,
        341,
        14862,
        4948,
        13,
        407,
        604,
        7897,
        490,
        300,
        300,
        366,
        51454
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16315996365284358,
      "compression_ratio": 1.6884057971014492,
      "no_speech_prob": 0.0038789622485637665
    },
    {
      "id": 131,
      "seek": 52368,
      "start": 545.4599999999999,
      "end": 551.4,
      "text": " covered within that token limit will be remembered as a way to put it.",
      "tokens": [
        51454,
        5343,
        1951,
        300,
        14862,
        4948,
        486,
        312,
        13745,
        382,
        257,
        636,
        281,
        829,
        309,
        13,
        51751
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16315996365284358,
      "compression_ratio": 1.6884057971014492,
      "no_speech_prob": 0.0038789622485637665
    },
    {
      "id": 132,
      "seek": 52368,
      "start": 551.4,
      "end": 552.9399999999999,
      "text": " You can even put it this way.",
      "tokens": [
        51751,
        509,
        393,
        754,
        829,
        309,
        341,
        636,
        13,
        51828
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16315996365284358,
      "compression_ratio": 1.6884057971014492,
      "no_speech_prob": 0.0038789622485637665
    },
    {
      "id": 133,
      "seek": 52368,
      "start": 552.9399999999999,
      "end": 553.66,
      "text": " Any messages.",
      "tokens": [
        51828,
        2639,
        7897,
        13,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16315996365284358,
      "compression_ratio": 1.6884057971014492,
      "no_speech_prob": 0.0038789622485637665
    },
    {
      "id": 134,
      "seek": 55366,
      "start": 553.66,
      "end": 560.7199999999999,
      "text": " Up to that token limit, I don't need to implement any extra memory implementation at all, my",
      "tokens": [
        50365,
        5858,
        281,
        300,
        14862,
        4948,
        11,
        286,
        500,
        380,
        643,
        281,
        4445,
        604,
        2857,
        4675,
        11420,
        412,
        439,
        11,
        452,
        50718
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16039286237774472,
      "compression_ratio": 1.759581881533101,
      "no_speech_prob": 0.0016570003936067224
    },
    {
      "id": 135,
      "seek": 55366,
      "start": 560.7199999999999,
      "end": 565.26,
      "text": " short term memory will just simply be holding that for me. And if you want some more detail,",
      "tokens": [
        50718,
        2099,
        1433,
        4675,
        486,
        445,
        2935,
        312,
        5061,
        300,
        337,
        385,
        13,
        400,
        498,
        291,
        528,
        512,
        544,
        2607,
        11,
        50945
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16039286237774472,
      "compression_ratio": 1.759581881533101,
      "no_speech_prob": 0.0016570003936067224
    },
    {
      "id": 136,
      "seek": 55366,
      "start": 565.26,
      "end": 569.3199999999999,
      "text": " we can maybe talk about this a bit later as well. By default in LLM index, that short",
      "tokens": [
        50945,
        321,
        393,
        1310,
        751,
        466,
        341,
        257,
        857,
        1780,
        382,
        731,
        13,
        3146,
        7576,
        294,
        441,
        43,
        44,
        8186,
        11,
        300,
        2099,
        51148
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16039286237774472,
      "compression_ratio": 1.759581881533101,
      "no_speech_prob": 0.0016570003936067224
    },
    {
      "id": 137,
      "seek": 55366,
      "start": 569.3199999999999,
      "end": 572.86,
      "text": " term memory is basically a in memory SQL database.",
      "tokens": [
        51148,
        1433,
        4675,
        307,
        1936,
        257,
        294,
        4675,
        19200,
        8149,
        13,
        51325
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16039286237774472,
      "compression_ratio": 1.759581881533101,
      "no_speech_prob": 0.0016570003936067224
    },
    {
      "id": 138,
      "seek": 55366,
      "start": 572.86,
      "end": 579.1999999999999,
      "text": " All right, so it's just been memory SQL database. Okay, great. Okay, so let's keep going. Okay,",
      "tokens": [
        51325,
        1057,
        558,
        11,
        370,
        309,
        311,
        445,
        668,
        4675,
        19200,
        8149,
        13,
        1033,
        11,
        869,
        13,
        1033,
        11,
        370,
        718,
        311,
        1066,
        516,
        13,
        1033,
        11,
        51642
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16039286237774472,
      "compression_ratio": 1.759581881533101,
      "no_speech_prob": 0.0016570003936067224
    },
    {
      "id": 139,
      "seek": 55366,
      "start": 579.1999999999999,
      "end": 582.92,
      "text": " so as as you discussed, you sort of started hinting on this idea of short term memory.",
      "tokens": [
        51642,
        370,
        382,
        382,
        291,
        7152,
        11,
        291,
        1333,
        295,
        1409,
        12075,
        278,
        322,
        341,
        1558,
        295,
        2099,
        1433,
        4675,
        13,
        51828
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16039286237774472,
      "compression_ratio": 1.759581881533101,
      "no_speech_prob": 0.0016570003936067224
    },
    {
      "id": 140,
      "seek": 58292,
      "start": 582.92,
      "end": 589.4799999999999,
      "text": " And from our discussion, it sounds like this idea of short term memory is, you know, the",
      "tokens": [
        50365,
        400,
        490,
        527,
        5017,
        11,
        309,
        3263,
        411,
        341,
        1558,
        295,
        2099,
        1433,
        4675,
        307,
        11,
        291,
        458,
        11,
        264,
        50693
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15153499981304547,
      "compression_ratio": 1.7372549019607844,
      "no_speech_prob": 0.009203284047544003
    },
    {
      "id": 141,
      "seek": 58292,
      "start": 589.4799999999999,
      "end": 594.16,
      "text": " in memory SQL database. And if we want to have something like long term memory, we would",
      "tokens": [
        50693,
        294,
        4675,
        19200,
        8149,
        13,
        400,
        498,
        321,
        528,
        281,
        362,
        746,
        411,
        938,
        1433,
        4675,
        11,
        321,
        576,
        50927
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15153499981304547,
      "compression_ratio": 1.7372549019607844,
      "no_speech_prob": 0.009203284047544003
    },
    {
      "id": 142,
      "seek": 58292,
      "start": 594.16,
      "end": 600.38,
      "text": " go for something more sophisticated, like Postgres or other database types. So, John,",
      "tokens": [
        50927,
        352,
        337,
        746,
        544,
        16950,
        11,
        411,
        10223,
        45189,
        420,
        661,
        8149,
        3467,
        13,
        407,
        11,
        2619,
        11,
        51238
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15153499981304547,
      "compression_ratio": 1.7372549019607844,
      "no_speech_prob": 0.009203284047544003
    },
    {
      "id": 143,
      "seek": 58292,
      "start": 600.38,
      "end": 606.18,
      "text": " I want to focus to you with with you today about talking, you know, both kinds of memory",
      "tokens": [
        51238,
        286,
        528,
        281,
        1879,
        281,
        291,
        365,
        365,
        291,
        965,
        466,
        1417,
        11,
        291,
        458,
        11,
        1293,
        3685,
        295,
        4675,
        51528
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15153499981304547,
      "compression_ratio": 1.7372549019607844,
      "no_speech_prob": 0.009203284047544003
    },
    {
      "id": 144,
      "seek": 58292,
      "start": 606.18,
      "end": 611.56,
      "text": " and explore more in depth what this looks like. So, you know, we talked a little bit",
      "tokens": [
        51528,
        293,
        6839,
        544,
        294,
        7161,
        437,
        341,
        1542,
        411,
        13,
        407,
        11,
        291,
        458,
        11,
        321,
        2825,
        257,
        707,
        857,
        51797
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15153499981304547,
      "compression_ratio": 1.7372549019607844,
      "no_speech_prob": 0.009203284047544003
    },
    {
      "id": 145,
      "seek": 58292,
      "start": 611.56,
      "end": 612.56,
      "text": " about",
      "tokens": [
        51797,
        466,
        51847
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15153499981304547,
      "compression_ratio": 1.7372549019607844,
      "no_speech_prob": 0.009203284047544003
    },
    {
      "id": 146,
      "seek": 61292,
      "start": 612.92,
      "end": 622.78,
      "text": " this idea of, you know, how to implement memory within within the agentic system. So what",
      "tokens": [
        50365,
        341,
        1558,
        295,
        11,
        291,
        458,
        11,
        577,
        281,
        4445,
        4675,
        1951,
        1951,
        264,
        9461,
        299,
        1185,
        13,
        407,
        437,
        50858
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12229467119489397,
      "compression_ratio": 1.771186440677966,
      "no_speech_prob": 0.004306512419134378
    },
    {
      "id": 147,
      "seek": 61292,
      "start": 622.78,
      "end": 628.3399999999999,
      "text": " what does it mean to give the agent memory? So is it is it is it only recalling the messages",
      "tokens": [
        50858,
        437,
        775,
        309,
        914,
        281,
        976,
        264,
        9461,
        4675,
        30,
        407,
        307,
        309,
        307,
        309,
        307,
        309,
        787,
        9901,
        278,
        264,
        7897,
        51136
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12229467119489397,
      "compression_ratio": 1.771186440677966,
      "no_speech_prob": 0.004306512419134378
    },
    {
      "id": 148,
      "seek": 61292,
      "start": 628.3399999999999,
      "end": 633.78,
      "text": " that were provided to it? Is it also recalling any tool usage, any decision making process?",
      "tokens": [
        51136,
        300,
        645,
        5649,
        281,
        309,
        30,
        1119,
        309,
        611,
        9901,
        278,
        604,
        2290,
        14924,
        11,
        604,
        3537,
        1455,
        1399,
        30,
        51408
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12229467119489397,
      "compression_ratio": 1.771186440677966,
      "no_speech_prob": 0.004306512419134378
    },
    {
      "id": 149,
      "seek": 61292,
      "start": 633.78,
      "end": 639.16,
      "text": " Can you tell us how this happens in relation to how the LLM is making decisions with respect",
      "tokens": [
        51408,
        1664,
        291,
        980,
        505,
        577,
        341,
        2314,
        294,
        9721,
        281,
        577,
        264,
        441,
        43,
        44,
        307,
        1455,
        5327,
        365,
        3104,
        51677
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12229467119489397,
      "compression_ratio": 1.771186440677966,
      "no_speech_prob": 0.004306512419134378
    },
    {
      "id": 150,
      "seek": 61292,
      "start": 639.16,
      "end": 642.92,
      "text": " to what tool to use given the prompt it was given?",
      "tokens": [
        51677,
        281,
        437,
        2290,
        281,
        764,
        2212,
        264,
        12391,
        309,
        390,
        2212,
        30,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12229467119489397,
      "compression_ratio": 1.771186440677966,
      "no_speech_prob": 0.004306512419134378
    },
    {
      "id": 151,
      "seek": 64292,
      "start": 642.92,
      "end": 652.9399999999999,
      "text": " Yes. So this maybe we can start by discussing the way we make choices around how we implement",
      "tokens": [
        50365,
        1079,
        13,
        407,
        341,
        1310,
        321,
        393,
        722,
        538,
        10850,
        264,
        636,
        321,
        652,
        7994,
        926,
        577,
        321,
        4445,
        50866
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4517328318427591,
      "compression_ratio": 1.9103942652329748,
      "no_speech_prob": 0.028690848499536514
    },
    {
      "id": 152,
      "seek": 64292,
      "start": 652.9399999999999,
      "end": 658.8399999999999,
      "text": " long term memory. So I also see that there's a question in the chat about what happens",
      "tokens": [
        50866,
        938,
        1433,
        4675,
        13,
        407,
        286,
        611,
        536,
        300,
        456,
        311,
        257,
        1168,
        294,
        264,
        5081,
        466,
        437,
        2314,
        51161
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4517328318427591,
      "compression_ratio": 1.9103942652329748,
      "no_speech_prob": 0.028690848499536514
    },
    {
      "id": 153,
      "seek": 64292,
      "start": 658.8399999999999,
      "end": 664.36,
      "text": " after we reach the token limit. And this is a great time to actually answer that question",
      "tokens": [
        51161,
        934,
        321,
        2524,
        264,
        14862,
        4948,
        13,
        400,
        341,
        307,
        257,
        869,
        565,
        281,
        767,
        1867,
        300,
        1168,
        51437
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4517328318427591,
      "compression_ratio": 1.9103942652329748,
      "no_speech_prob": 0.028690848499536514
    },
    {
      "id": 154,
      "seek": 64292,
      "start": 664.36,
      "end": 670.92,
      "text": " as well. Okay. So once we hit the token limit of what we allow our short term memory to",
      "tokens": [
        51437,
        382,
        731,
        13,
        1033,
        13,
        407,
        1564,
        321,
        2045,
        264,
        14862,
        4948,
        295,
        437,
        321,
        2089,
        527,
        2099,
        1433,
        4675,
        281,
        51765
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4517328318427591,
      "compression_ratio": 1.9103942652329748,
      "no_speech_prob": 0.028690848499536514
    },
    {
      "id": 155,
      "seek": 64292,
      "start": 670.92,
      "end": 671.92,
      "text": " hold, this is where we start to see the impact of the LLM. So we can see that the LLM is",
      "tokens": [
        51765,
        1797,
        11,
        341,
        307,
        689,
        321,
        722,
        281,
        536,
        264,
        2712,
        295,
        264,
        441,
        43,
        44,
        13,
        407,
        321,
        393,
        536,
        300,
        264,
        441,
        43,
        44,
        307,
        51815
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4517328318427591,
      "compression_ratio": 1.9103942652329748,
      "no_speech_prob": 0.028690848499536514
    },
    {
      "id": 156,
      "seek": 64292,
      "start": 671.92,
      "end": 672.92,
      "text": " actually a very good example of how we can implement long term memory. So we can see,",
      "tokens": [
        51815,
        767,
        257,
        588,
        665,
        1365,
        295,
        577,
        321,
        393,
        4445,
        938,
        1433,
        4675,
        13,
        407,
        321,
        393,
        536,
        11,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4517328318427591,
      "compression_ratio": 1.9103942652329748,
      "no_speech_prob": 0.028690848499536514
    },
    {
      "id": 157,
      "seek": 67292,
      "start": 672.92,
      "end": 673.76,
      "text": " how LLM is directly affected by long term memory. So in this example we can see it",
      "tokens": [
        50365,
        577,
        441,
        43,
        44,
        307,
        3838,
        8028,
        538,
        938,
        1433,
        4675,
        13,
        407,
        294,
        341,
        1365,
        321,
        393,
        536,
        309,
        50407
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 158,
      "seek": 67292,
      "start": 673.76,
      "end": 678.76,
      "text": " isattending we are actually showing the on the non beta API portion of the shell code",
      "tokens": [
        50407,
        307,
        1591,
        2029,
        321,
        366,
        767,
        4099,
        264,
        322,
        264,
        2107,
        9861,
        9362,
        8044,
        295,
        264,
        8720,
        3089,
        50657
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 159,
      "seek": 67292,
      "start": 678.76,
      "end": 683.5,
      "text": " of a block of primordial information that why is it attackable. We can also see that",
      "tokens": [
        50657,
        295,
        257,
        3461,
        295,
        2886,
        765,
        831,
        1589,
        300,
        983,
        307,
        309,
        2690,
        712,
        13,
        492,
        393,
        611,
        536,
        300,
        50894
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 160,
      "seek": 67292,
      "start": 683.5,
      "end": 686.9,
      "text": " this is why this API to actually work under the entire form of a string initial lastly",
      "tokens": [
        50894,
        341,
        307,
        983,
        341,
        9362,
        281,
        767,
        589,
        833,
        264,
        2302,
        1254,
        295,
        257,
        6798,
        5883,
        16386,
        51064
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 161,
      "seek": 67292,
      "start": 686.9,
      "end": 692.9,
      "text": " because we need to actually track those limits. We can also see that the LLM of the of the",
      "tokens": [
        51064,
        570,
        321,
        643,
        281,
        767,
        2837,
        729,
        10406,
        13,
        492,
        393,
        611,
        536,
        300,
        264,
        441,
        43,
        44,
        295,
        264,
        295,
        264,
        51364
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 162,
      "seek": 67292,
      "start": 692.9,
      "end": 696.4399999999999,
      "text": " LW is employed for the in Scott. And next I'm going to test out how it's\u3053\u308c",
      "tokens": [
        51364,
        441,
        54,
        307,
        20115,
        337,
        264,
        294,
        6659,
        13,
        400,
        958,
        286,
        478,
        516,
        281,
        1500,
        484,
        577,
        309,
        311,
        9329,
        51541
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 163,
      "seek": 67292,
      "start": 696.4399999999999,
      "end": 700.76,
      "text": " bit of a oneself on this a three step process. We can see which of the",
      "tokens": [
        51541,
        857,
        295,
        257,
        32265,
        322,
        341,
        257,
        1045,
        1823,
        1399,
        13,
        492,
        393,
        536,
        597,
        295,
        264,
        51757
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 164,
      "seek": 67292,
      "start": 700.76,
      "end": 702.16,
      "text": " effectiveness, which of these events is minimal.",
      "tokens": [
        51757,
        21208,
        11,
        597,
        295,
        613,
        3931,
        307,
        13206,
        13,
        51827
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 165,
      "seek": 67292,
      "start": 702.16,
      "end": 702.66,
      "text": " PJ Slauson down there.",
      "tokens": [
        51827,
        30549,
        318,
        22590,
        266,
        760,
        456,
        13,
        51852
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 166,
      "seek": 67292,
      "start": 702.66,
      "end": 702.86,
      "text": " Okay.",
      "tokens": [
        51852,
        1033,
        13,
        51862
      ],
      "temperature": 1.0,
      "avg_logprob": -3.5417609514508928,
      "compression_ratio": 1.752659574468085,
      "no_speech_prob": 0.0015294939512386918
    },
    {
      "id": 167,
      "seek": 70286,
      "start": 702.86,
      "end": 709.12,
      "text": " a few default long-term memory blocks for you, so you don't really have to think about",
      "tokens": [
        50365,
        257,
        1326,
        7576,
        938,
        12,
        7039,
        4675,
        8474,
        337,
        291,
        11,
        370,
        291,
        500,
        380,
        534,
        362,
        281,
        519,
        466,
        50678
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1321877994756589,
      "compression_ratio": 1.625,
      "no_speech_prob": 0.003247063374146819
    },
    {
      "id": 168,
      "seek": 70286,
      "start": 709.12,
      "end": 714.74,
      "text": " the implementation. But I can start with maybe something that is the easiest to relate to.",
      "tokens": [
        50678,
        264,
        11420,
        13,
        583,
        286,
        393,
        722,
        365,
        1310,
        746,
        300,
        307,
        264,
        12889,
        281,
        10961,
        281,
        13,
        50959
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1321877994756589,
      "compression_ratio": 1.625,
      "no_speech_prob": 0.003247063374146819
    },
    {
      "id": 169,
      "seek": 70286,
      "start": 715.28,
      "end": 721.86,
      "text": " Most of the times what we see people use agents for is that they want to have a chat with the",
      "tokens": [
        50986,
        4534,
        295,
        264,
        1413,
        437,
        321,
        536,
        561,
        764,
        12554,
        337,
        307,
        300,
        436,
        528,
        281,
        362,
        257,
        5081,
        365,
        264,
        51315
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1321877994756589,
      "compression_ratio": 1.625,
      "no_speech_prob": 0.003247063374146819
    },
    {
      "id": 170,
      "seek": 70286,
      "start": 721.86,
      "end": 728.66,
      "text": " agent, and that agent maybe has access to some external information. We can provide a vector",
      "tokens": [
        51315,
        9461,
        11,
        293,
        300,
        9461,
        1310,
        575,
        2105,
        281,
        512,
        8320,
        1589,
        13,
        492,
        393,
        2893,
        257,
        8062,
        51655
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1321877994756589,
      "compression_ratio": 1.625,
      "no_speech_prob": 0.003247063374146819
    },
    {
      "id": 171,
      "seek": 72866,
      "start": 728.66,
      "end": 734.54,
      "text": " memory block. So you can think of this as a regular vector store where we store embeddings",
      "tokens": [
        50365,
        4675,
        3461,
        13,
        407,
        291,
        393,
        519,
        295,
        341,
        382,
        257,
        3890,
        8062,
        3531,
        689,
        321,
        3531,
        12240,
        29432,
        50659
      ],
      "temperature": 0.0,
      "avg_logprob": -0.04318450265011545,
      "compression_ratio": 1.740072202166065,
      "no_speech_prob": 2.5634008125052787e-05
    },
    {
      "id": 172,
      "seek": 72866,
      "start": 734.54,
      "end": 741.74,
      "text": " of information. That's the most classic way we do RAG as well, for example. But we can also start",
      "tokens": [
        50659,
        295,
        1589,
        13,
        663,
        311,
        264,
        881,
        7230,
        636,
        321,
        360,
        14626,
        38,
        382,
        731,
        11,
        337,
        1365,
        13,
        583,
        321,
        393,
        611,
        722,
        51019
      ],
      "temperature": 0.0,
      "avg_logprob": -0.04318450265011545,
      "compression_ratio": 1.740072202166065,
      "no_speech_prob": 2.5634008125052787e-05
    },
    {
      "id": 173,
      "seek": 72866,
      "start": 741.74,
      "end": 747.3,
      "text": " using vector stores as memory blocks themselves. So that is one of our long-term memory",
      "tokens": [
        51019,
        1228,
        8062,
        9512,
        382,
        4675,
        8474,
        2969,
        13,
        407,
        300,
        307,
        472,
        295,
        527,
        938,
        12,
        7039,
        4675,
        51297
      ],
      "temperature": 0.0,
      "avg_logprob": -0.04318450265011545,
      "compression_ratio": 1.740072202166065,
      "no_speech_prob": 2.5634008125052787e-05
    },
    {
      "id": 174,
      "seek": 72866,
      "start": 747.3,
      "end": 753.6,
      "text": " implementations. We literally call it the vector memory block. What this does is that while the",
      "tokens": [
        51297,
        4445,
        763,
        13,
        492,
        3736,
        818,
        309,
        264,
        8062,
        4675,
        3461,
        13,
        708,
        341,
        775,
        307,
        300,
        1339,
        264,
        51612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.04318450265011545,
      "compression_ratio": 1.740072202166065,
      "no_speech_prob": 2.5634008125052787e-05
    },
    {
      "id": 175,
      "seek": 72866,
      "start": 753.6,
      "end": 758.16,
      "text": " chat is ongoing, if we hit the token limit, like this user asked, what happens when you hit that",
      "tokens": [
        51612,
        5081,
        307,
        10452,
        11,
        498,
        321,
        2045,
        264,
        14862,
        4948,
        11,
        411,
        341,
        4195,
        2351,
        11,
        437,
        2314,
        562,
        291,
        2045,
        300,
        51840
      ],
      "temperature": 0.0,
      "avg_logprob": -0.04318450265011545,
      "compression_ratio": 1.740072202166065,
      "no_speech_prob": 2.5634008125052787e-05
    },
    {
      "id": 176,
      "seek": 72866,
      "start": 758.16,
      "end": 758.64,
      "text": " token limit?",
      "tokens": [
        51840,
        14862,
        4948,
        30,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.04318450265011545,
      "compression_ratio": 1.740072202166065,
      "no_speech_prob": 2.5634008125052787e-05
    },
    {
      "id": 177,
      "seek": 75866,
      "start": 758.66,
      "end": 765.66,
      "text": " Behind the scenes, without you actually as a developer having to do anything about how you",
      "tokens": [
        50365,
        20475,
        264,
        8026,
        11,
        1553,
        291,
        767,
        382,
        257,
        10754,
        1419,
        281,
        360,
        1340,
        466,
        577,
        291,
        50715
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1051931819696536,
      "compression_ratio": 1.6696428571428572,
      "no_speech_prob": 0.0009771599434316158
    },
    {
      "id": 178,
      "seek": 75866,
      "start": 765.66,
      "end": 771.92,
      "text": " implement this, behind the scenes what the Lama Index agent will do is that all of the chat that",
      "tokens": [
        50715,
        4445,
        341,
        11,
        2261,
        264,
        8026,
        437,
        264,
        441,
        2404,
        33552,
        9461,
        486,
        360,
        307,
        300,
        439,
        295,
        264,
        5081,
        300,
        51028
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1051931819696536,
      "compression_ratio": 1.6696428571428572,
      "no_speech_prob": 0.0009771599434316158
    },
    {
      "id": 179,
      "seek": 75866,
      "start": 771.92,
      "end": 778.74,
      "text": " happens so far will be indexed and will be written into a vector memory block, so a vector store",
      "tokens": [
        51028,
        2314,
        370,
        1400,
        486,
        312,
        8186,
        292,
        293,
        486,
        312,
        3720,
        666,
        257,
        8062,
        4675,
        3461,
        11,
        370,
        257,
        8062,
        3531,
        51369
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1051931819696536,
      "compression_ratio": 1.6696428571428572,
      "no_speech_prob": 0.0009771599434316158
    },
    {
      "id": 180,
      "seek": 75866,
      "start": 778.74,
      "end": 788.64,
      "text": " itself. From this point on, every interaction we have ongoing, the agent will also have a",
      "tokens": [
        51369,
        2564,
        13,
        3358,
        341,
        935,
        322,
        11,
        633,
        9285,
        321,
        362,
        10452,
        11,
        264,
        9461,
        486,
        611,
        362,
        257,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1051931819696536,
      "compression_ratio": 1.6696428571428572,
      "no_speech_prob": 0.0009771599434316158
    },
    {
      "id": 181,
      "seek": 78866,
      "start": 788.66,
      "end": 790.8,
      "text": " 24HI fundamental instance that we put up here. It is probably very new, and it's not something we",
      "tokens": [
        50365,
        4022,
        49038,
        8088,
        5197,
        300,
        321,
        829,
        493,
        510,
        13,
        467,
        307,
        1391,
        588,
        777,
        11,
        293,
        309,
        311,
        406,
        746,
        321,
        50472
      ],
      "temperature": 1.0,
      "avg_logprob": -3.6171062110250842,
      "compression_ratio": 1.7116451016635859,
      "no_speech_prob": 0.001019602408632636
    },
    {
      "id": 182,
      "seek": 78866,
      "start": 790.8,
      "end": 795.6,
      "text": " want or want to change in space forces and it's not necessarily wanted. Now I want to say that",
      "tokens": [
        50472,
        528,
        420,
        528,
        281,
        1319,
        294,
        1901,
        5874,
        293,
        309,
        311,
        406,
        4725,
        1415,
        13,
        823,
        286,
        528,
        281,
        584,
        300,
        50712
      ],
      "temperature": 1.0,
      "avg_logprob": -3.6171062110250842,
      "compression_ratio": 1.7116451016635859,
      "no_speech_prob": 0.001019602408632636
    },
    {
      "id": 183,
      "seek": 78866,
      "start": 795.6,
      "end": 800.0,
      "text": " with the user there are versions of Lama Index \ud558\ub098 and Lama Index Forks that we can implement",
      "tokens": [
        50712,
        365,
        264,
        4195,
        456,
        366,
        9606,
        295,
        441,
        2404,
        33552,
        12261,
        293,
        441,
        2404,
        33552,
        1171,
        1694,
        300,
        321,
        393,
        4445,
        50932
      ],
      "temperature": 1.0,
      "avg_logprob": -3.6171062110250842,
      "compression_ratio": 1.7116451016635859,
      "no_speech_prob": 0.001019602408632636
    },
    {
      "id": 184,
      "seek": 78866,
      "start": 800.0,
      "end": 806.26,
      "text": " long-term memory again. Great. Although a\u5f85\u3063\u3066 of doing the same decision and having a",
      "tokens": [
        50932,
        938,
        12,
        7039,
        4675,
        797,
        13,
        3769,
        13,
        5780,
        257,
        33393,
        295,
        884,
        264,
        912,
        3537,
        293,
        1419,
        257,
        51245
      ],
      "temperature": 1.0,
      "avg_logprob": -3.6171062110250842,
      "compression_ratio": 1.7116451016635859,
      "no_speech_prob": 0.001019602408632636
    },
    {
      "id": 185,
      "seek": 78866,
      "start": 806.26,
      "end": 809.4,
      "text": " use case or000 file, and also that it's more streamlined in this case than with the user itself",
      "tokens": [
        51245,
        764,
        1389,
        420,
        1360,
        3991,
        11,
        293,
        611,
        300,
        309,
        311,
        544,
        48155,
        294,
        341,
        1389,
        813,
        365,
        264,
        4195,
        2564,
        51402
      ],
      "temperature": 1.0,
      "avg_logprob": -3.6171062110250842,
      "compression_ratio": 1.7116451016635859,
      "no_speech_prob": 0.001019602408632636
    },
    {
      "id": 186,
      "seek": 78866,
      "start": 809.4,
      "end": 812.6999999999999,
      "text": " here. So one way Lama Index can work is when you zoom in, lazy One-time memory of something",
      "tokens": [
        51402,
        510,
        13,
        407,
        472,
        636,
        441,
        2404,
        33552,
        393,
        589,
        307,
        562,
        291,
        8863,
        294,
        11,
        14847,
        1485,
        12,
        3766,
        4675,
        295,
        746,
        51567
      ],
      "temperature": 1.0,
      "avg_logprob": -3.6171062110250842,
      "compression_ratio": 1.7116451016635859,
      "no_speech_prob": 0.001019602408632636
    },
    {
      "id": 187,
      "seek": 78866,
      "start": 812.6999999999999,
      "end": 817.5799999999999,
      "text": " like a CSV file, is when we say hey, create this such and today we're looking into summary",
      "tokens": [
        51567,
        411,
        257,
        48814,
        3991,
        11,
        307,
        562,
        321,
        584,
        4177,
        11,
        1884,
        341,
        1270,
        293,
        965,
        321,
        434,
        1237,
        666,
        12691,
        51811
      ],
      "temperature": 1.0,
      "avg_logprob": -3.6171062110250842,
      "compression_ratio": 1.7116451016635859,
      "no_speech_prob": 0.001019602408632636
    },
    {
      "id": 188,
      "seek": 81758,
      "start": 817.58,
      "end": 822.3000000000001,
      "text": " end use case you want to provide to your users or your end application you want to provide to",
      "tokens": [
        50365,
        917,
        764,
        1389,
        291,
        528,
        281,
        2893,
        281,
        428,
        5022,
        420,
        428,
        917,
        3861,
        291,
        528,
        281,
        2893,
        281,
        50601
      ],
      "temperature": 0.0,
      "avg_logprob": -0.054177324395430715,
      "compression_ratio": 1.7236842105263157,
      "no_speech_prob": 0.028079817071557045
    },
    {
      "id": 189,
      "seek": 81758,
      "start": 822.3000000000001,
      "end": 829.72,
      "text": " your users, let's assume the chat history or the exact things you talked about with that agent is",
      "tokens": [
        50601,
        428,
        5022,
        11,
        718,
        311,
        6552,
        264,
        5081,
        2503,
        420,
        264,
        1900,
        721,
        291,
        2825,
        466,
        365,
        300,
        9461,
        307,
        50972
      ],
      "temperature": 0.0,
      "avg_logprob": -0.054177324395430715,
      "compression_ratio": 1.7236842105263157,
      "no_speech_prob": 0.028079817071557045
    },
    {
      "id": 190,
      "seek": 81758,
      "start": 829.72,
      "end": 839.6800000000001,
      "text": " not so important, but facts are. For example, how old you are, where you live, etc. So in some cases,",
      "tokens": [
        50972,
        406,
        370,
        1021,
        11,
        457,
        9130,
        366,
        13,
        1171,
        1365,
        11,
        577,
        1331,
        291,
        366,
        11,
        689,
        291,
        1621,
        11,
        5183,
        13,
        407,
        294,
        512,
        3331,
        11,
        51470
      ],
      "temperature": 0.0,
      "avg_logprob": -0.054177324395430715,
      "compression_ratio": 1.7236842105263157,
      "no_speech_prob": 0.028079817071557045
    },
    {
      "id": 191,
      "seek": 81758,
      "start": 839.6800000000001,
      "end": 846.46,
      "text": " we might see situations where long-term memory is more about keeping track of facts about that user",
      "tokens": [
        51470,
        321,
        1062,
        536,
        6851,
        689,
        938,
        12,
        7039,
        4675,
        307,
        544,
        466,
        5145,
        2837,
        295,
        9130,
        466,
        300,
        4195,
        51809
      ],
      "temperature": 0.0,
      "avg_logprob": -0.054177324395430715,
      "compression_ratio": 1.7236842105263157,
      "no_speech_prob": 0.028079817071557045
    },
    {
      "id": 192,
      "seek": 84646,
      "start": 846.46,
      "end": 853.4000000000001,
      "text": " rather than keeping track of the chat messages that users send specifically. So for this, we also",
      "tokens": [
        50365,
        2831,
        813,
        5145,
        2837,
        295,
        264,
        5081,
        7897,
        300,
        5022,
        2845,
        4682,
        13,
        407,
        337,
        341,
        11,
        321,
        611,
        50712
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06447780746774576,
      "compression_ratio": 1.6846473029045643,
      "no_speech_prob": 0.0012714492622762918
    },
    {
      "id": 193,
      "seek": 84646,
      "start": 853.4000000000001,
      "end": 858.7,
      "text": " have implemented a long-term memory block called fact memory block, for example, where this is",
      "tokens": [
        50712,
        362,
        12270,
        257,
        938,
        12,
        7039,
        4675,
        3461,
        1219,
        1186,
        4675,
        3461,
        11,
        337,
        1365,
        11,
        689,
        341,
        307,
        50977
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06447780746774576,
      "compression_ratio": 1.6846473029045643,
      "no_speech_prob": 0.0012714492622762918
    },
    {
      "id": 194,
      "seek": 84646,
      "start": 858.7,
      "end": 866.6800000000001,
      "text": " designed to simply store, think of it as like a index of facts that we ask the agent to extract",
      "tokens": [
        50977,
        4761,
        281,
        2935,
        3531,
        11,
        519,
        295,
        309,
        382,
        411,
        257,
        8186,
        295,
        9130,
        300,
        321,
        1029,
        264,
        9461,
        281,
        8947,
        51376
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06447780746774576,
      "compression_ratio": 1.6846473029045643,
      "no_speech_prob": 0.0012714492622762918
    },
    {
      "id": 195,
      "seek": 84646,
      "start": 866.6800000000001,
      "end": 875.2800000000001,
      "text": " along the way. So that is another implementation. So yeah, to answer your question, I think it would",
      "tokens": [
        51376,
        2051,
        264,
        636,
        13,
        407,
        300,
        307,
        1071,
        11420,
        13,
        407,
        1338,
        11,
        281,
        1867,
        428,
        1168,
        11,
        286,
        519,
        309,
        576,
        51806
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06447780746774576,
      "compression_ratio": 1.6846473029045643,
      "no_speech_prob": 0.0012714492622762918
    },
    {
      "id": 196,
      "seek": 84646,
      "start": 875.2800000000001,
      "end": 876.12,
      "text": " be a lot more...",
      "tokens": [
        51806,
        312,
        257,
        688,
        544,
        485,
        51848
      ],
      "temperature": 0.0,
      "avg_logprob": -0.06447780746774576,
      "compression_ratio": 1.6846473029045643,
      "no_speech_prob": 0.0012714492622762918
    },
    {
      "id": 197,
      "seek": 87646,
      "start": 876.46,
      "end": 879.6600000000001,
      "text": " Use case based, which type of long-term memory you want?",
      "tokens": [
        50365,
        8278,
        1389,
        2361,
        11,
        597,
        2010,
        295,
        938,
        12,
        7039,
        4675,
        291,
        528,
        30,
        50525
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21969984303350035,
      "compression_ratio": 1.6517241379310346,
      "no_speech_prob": 0.003594591049477458
    },
    {
      "id": 198,
      "seek": 87646,
      "start": 879.6600000000001,
      "end": 886.94,
      "text": " It depends. It sounds like it's quite flexible and varied. So we have the option to host memory in a",
      "tokens": [
        50525,
        467,
        5946,
        13,
        467,
        3263,
        411,
        309,
        311,
        1596,
        11358,
        293,
        22877,
        13,
        407,
        321,
        362,
        264,
        3614,
        281,
        3975,
        4675,
        294,
        257,
        50889
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21969984303350035,
      "compression_ratio": 1.6517241379310346,
      "no_speech_prob": 0.003594591049477458
    },
    {
      "id": 199,
      "seek": 87646,
      "start": 886.94,
      "end": 891.74,
      "text": " simple in-memory SQL database, but there's also more sophisticated forms, not just Postgres, but you",
      "tokens": [
        50889,
        2199,
        294,
        12,
        17886,
        827,
        19200,
        8149,
        11,
        457,
        456,
        311,
        611,
        544,
        16950,
        6422,
        11,
        406,
        445,
        10223,
        45189,
        11,
        457,
        291,
        51129
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21969984303350035,
      "compression_ratio": 1.6517241379310346,
      "no_speech_prob": 0.003594591049477458
    },
    {
      "id": 200,
      "seek": 87646,
      "start": 891.74,
      "end": 898.38,
      "text": " can also set up even a vector database where you can create embeddings for different either the chat",
      "tokens": [
        51129,
        393,
        611,
        992,
        493,
        754,
        257,
        8062,
        8149,
        689,
        291,
        393,
        1884,
        12240,
        29432,
        337,
        819,
        2139,
        264,
        5081,
        51461
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21969984303350035,
      "compression_ratio": 1.6517241379310346,
      "no_speech_prob": 0.003594591049477458
    },
    {
      "id": 201,
      "seek": 87646,
      "start": 898.38,
      "end": 905.26,
      "text": " components or more complex forms of information and enable, give that access, give that to the agent",
      "tokens": [
        51461,
        6677,
        420,
        544,
        3997,
        6422,
        295,
        1589,
        293,
        9528,
        11,
        976,
        300,
        2105,
        11,
        976,
        300,
        281,
        264,
        9461,
        51805
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21969984303350035,
      "compression_ratio": 1.6517241379310346,
      "no_speech_prob": 0.003594591049477458
    },
    {
      "id": 202,
      "seek": 87646,
      "start": 905.26,
      "end": 906.46,
      "text": " to best implement.",
      "tokens": [
        51805,
        281,
        1151,
        4445,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.21969984303350035,
      "compression_ratio": 1.6517241379310346,
      "no_speech_prob": 0.003594591049477458
    },
    {
      "id": 203,
      "seek": 90646,
      "start": 906.46,
      "end": 907.1800000000001,
      "text": "\u017a",
      "tokens": [
        50365,
        10659,
        50401
      ],
      "temperature": 1.0,
      "avg_logprob": -1.3776587770696271,
      "compression_ratio": 1.6254295532646048,
      "no_speech_prob": 0.009570702910423279
    },
    {
      "id": 204,
      "seek": 90646,
      "start": 926.5600000000001,
      "end": 928.2,
      "text": " Okay. parameters.",
      "tokens": [
        51370,
        1033,
        13,
        9834,
        13,
        51452
      ],
      "temperature": 1.0,
      "avg_logprob": -1.3776587770696271,
      "compression_ratio": 1.6254295532646048,
      "no_speech_prob": 0.009570702910423279
    },
    {
      "id": 205,
      "seek": 90646,
      "start": 928.2,
      "end": 933.34,
      "text": " And we touched a little bit about I think this will sort of addressed in your answer. The difference between",
      "tokens": [
        51452,
        400,
        321,
        9828,
        257,
        707,
        857,
        466,
        286,
        519,
        341,
        486,
        1333,
        295,
        13847,
        294,
        428,
        1867,
        13,
        440,
        2649,
        1296,
        51709
      ],
      "temperature": 1.0,
      "avg_logprob": -1.3776587770696271,
      "compression_ratio": 1.6254295532646048,
      "no_speech_prob": 0.009570702910423279
    },
    {
      "id": 206,
      "seek": 90646,
      "start": 933.34,
      "end": 934.48,
      "text": " the short-term and long-term memory.",
      "tokens": [
        51709,
        264,
        2099,
        12,
        7039,
        293,
        938,
        12,
        7039,
        4675,
        13,
        51766
      ],
      "temperature": 1.0,
      "avg_logprob": -1.3776587770696271,
      "compression_ratio": 1.6254295532646048,
      "no_speech_prob": 0.009570702910423279
    },
    {
      "id": 207,
      "seek": 90646,
      "start": 934.48,
      "end": 935.5,
      "text": " So maybe let's continue.",
      "tokens": [
        51766,
        407,
        1310,
        718,
        311,
        2354,
        13,
        51817
      ],
      "temperature": 1.0,
      "avg_logprob": -1.3776587770696271,
      "compression_ratio": 1.6254295532646048,
      "no_speech_prob": 0.009570702910423279
    },
    {
      "id": 208,
      "seek": 90646,
      "start": 935.5,
      "end": 935.74,
      "text": " Let's explore a little bit more in depth. How we can start implementing both types of memory. So from the documentation, the memory class will store the last X messages that fit into the token limit. We already talked about this already, so hang up and let's do some more memories.",
      "tokens": [
        51817,
        961,
        311,
        6839,
        257,
        707,
        857,
        544,
        294,
        7161,
        13,
        1012,
        321,
        393,
        722,
        18114,
        1293,
        3467,
        295,
        4675,
        13,
        407,
        490,
        264,
        14333,
        11,
        264,
        4675,
        1508,
        486,
        3531,
        264,
        1036,
        1783,
        7897,
        300,
        3318,
        666,
        264,
        14862,
        4948,
        13,
        492,
        1217,
        2825,
        466,
        341,
        1217,
        11,
        370,
        3967,
        493,
        293,
        718,
        311,
        360,
        512,
        544,
        8495,
        13,
        51829
      ],
      "temperature": 1.0,
      "avg_logprob": -1.3776587770696271,
      "compression_ratio": 1.6254295532646048,
      "no_speech_prob": 0.009570702910423279
    },
    {
      "id": 209,
      "seek": 93646,
      "start": 936.46,
      "end": 940.7,
      "text": " bit about that token limit as we went through through this a little bit more so let's move",
      "tokens": [
        50365,
        857,
        466,
        300,
        14862,
        4948,
        382,
        321,
        1437,
        807,
        807,
        341,
        257,
        707,
        857,
        544,
        370,
        718,
        311,
        1286,
        50577
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05179034277450207,
      "compression_ratio": 1.9333333333333333,
      "no_speech_prob": 0.03789341822266579
    },
    {
      "id": 210,
      "seek": 93646,
      "start": 940.7,
      "end": 948.38,
      "text": " onto onto this idea of the long-term memory and so from the documentation this idea of the memory",
      "tokens": [
        50577,
        3911,
        3911,
        341,
        1558,
        295,
        264,
        938,
        12,
        7039,
        4675,
        293,
        370,
        490,
        264,
        14333,
        341,
        1558,
        295,
        264,
        4675,
        50961
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05179034277450207,
      "compression_ratio": 1.9333333333333333,
      "no_speech_prob": 0.03789341822266579
    },
    {
      "id": 211,
      "seek": 93646,
      "start": 948.38,
      "end": 954.0600000000001,
      "text": " block object is introduced uh to implement long-term memory so can you tell us a little",
      "tokens": [
        50961,
        3461,
        2657,
        307,
        7268,
        2232,
        281,
        4445,
        938,
        12,
        7039,
        4675,
        370,
        393,
        291,
        980,
        505,
        257,
        707,
        51245
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05179034277450207,
      "compression_ratio": 1.9333333333333333,
      "no_speech_prob": 0.03789341822266579
    },
    {
      "id": 212,
      "seek": 93646,
      "start": 954.0600000000001,
      "end": 961.1,
      "text": " bit more about this memory block object yes and i think this is probably my favorite thing about how",
      "tokens": [
        51245,
        857,
        544,
        466,
        341,
        4675,
        3461,
        2657,
        2086,
        293,
        741,
        519,
        341,
        307,
        1391,
        452,
        2954,
        551,
        466,
        577,
        51597
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05179034277450207,
      "compression_ratio": 1.9333333333333333,
      "no_speech_prob": 0.03789341822266579
    },
    {
      "id": 213,
      "seek": 96110,
      "start": 961.1,
      "end": 969.58,
      "text": " we've implemented long-term memory um so uh as we discussed before there are some pre-implemented",
      "tokens": [
        50365,
        321,
        600,
        12270,
        938,
        12,
        7039,
        4675,
        1105,
        370,
        2232,
        382,
        321,
        7152,
        949,
        456,
        366,
        512,
        659,
        12,
        332,
        781,
        14684,
        50789
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31982994079589844,
      "compression_ratio": 2.292517006802721,
      "no_speech_prob": 0.0047463420778512955
    },
    {
      "id": 214,
      "seek": 96110,
      "start": 969.58,
      "end": 974.46,
      "text": " long-term memory blocks like the vector memory block we discussed uh fact memory block we",
      "tokens": [
        50789,
        938,
        12,
        7039,
        4675,
        8474,
        411,
        264,
        8062,
        4675,
        3461,
        321,
        7152,
        2232,
        1186,
        4675,
        3461,
        321,
        51033
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31982994079589844,
      "compression_ratio": 2.292517006802721,
      "no_speech_prob": 0.0047463420778512955
    },
    {
      "id": 215,
      "seek": 96110,
      "start": 974.46,
      "end": 980.5400000000001,
      "text": " discussed my favorite which is not in llama index core but it is provided as an integration one of",
      "tokens": [
        51033,
        7152,
        452,
        2954,
        597,
        307,
        406,
        294,
        23272,
        8186,
        4965,
        457,
        309,
        307,
        5649,
        382,
        364,
        10980,
        472,
        295,
        51337
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31982994079589844,
      "compression_ratio": 2.292517006802721,
      "no_speech_prob": 0.0047463420778512955
    },
    {
      "id": 216,
      "seek": 96110,
      "start": 980.5400000000001,
      "end": 985.5,
      "text": " the latest ones that my colleague logan added is called artifact memory block which we can chat",
      "tokens": [
        51337,
        264,
        6792,
        2306,
        300,
        452,
        13532,
        450,
        1275,
        3869,
        307,
        1219,
        34806,
        4675,
        3461,
        597,
        321,
        393,
        5081,
        51585
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31982994079589844,
      "compression_ratio": 2.292517006802721,
      "no_speech_prob": 0.0047463420778512955
    },
    {
      "id": 217,
      "seek": 96110,
      "start": 985.5,
      "end": 990.62,
      "text": " about if you're interested in it but the most important thing about these is that they're simply",
      "tokens": [
        51585,
        466,
        498,
        291,
        434,
        3102,
        294,
        309,
        457,
        264,
        881,
        1021,
        551,
        466,
        613,
        307,
        300,
        436,
        434,
        2935,
        51841
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31982994079589844,
      "compression_ratio": 2.292517006802721,
      "no_speech_prob": 0.0047463420778512955
    },
    {
      "id": 218,
      "seek": 96110,
      "start": 990.62,
      "end": 991.02,
      "text": " extension of the memory block object so if you're interested in that you can go to the library and",
      "tokens": [
        51841,
        10320,
        295,
        264,
        4675,
        3461,
        2657,
        370,
        498,
        291,
        434,
        3102,
        294,
        300,
        291,
        393,
        352,
        281,
        264,
        6405,
        293,
        51861
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31982994079589844,
      "compression_ratio": 2.292517006802721,
      "no_speech_prob": 0.0047463420778512955
    },
    {
      "id": 219,
      "seek": 96110,
      "start": 991.02,
      "end": 991.08,
      "text": " look at the library and look at the memory block object and look at the memory block object and",
      "tokens": [
        51861,
        574,
        412,
        264,
        6405,
        293,
        574,
        412,
        264,
        4675,
        3461,
        2657,
        293,
        574,
        412,
        264,
        4675,
        3461,
        2657,
        293,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31982994079589844,
      "compression_ratio": 2.292517006802721,
      "no_speech_prob": 0.0047463420778512955
    },
    {
      "id": 220,
      "seek": 99108,
      "start": 991.08,
      "end": 993.08,
      "text": " then you can see that the memory block object is actually an extension of a memory block object",
      "tokens": [
        50365,
        550,
        291,
        393,
        536,
        300,
        264,
        4675,
        3461,
        2657,
        307,
        767,
        364,
        10320,
        295,
        257,
        4675,
        3461,
        2657,
        50465
      ],
      "temperature": 0.2,
      "avg_logprob": -0.32337060408158735,
      "compression_ratio": 2.0689655172413794,
      "no_speech_prob": 0.0038866735994815826
    },
    {
      "id": 221,
      "seek": 99108,
      "start": 993.08,
      "end": 999.08,
      "text": " that we call the memory block object um i think there should be a code snippet that shows you as",
      "tokens": [
        50465,
        300,
        321,
        818,
        264,
        4675,
        3461,
        2657,
        1105,
        741,
        519,
        456,
        820,
        312,
        257,
        3089,
        35623,
        302,
        300,
        3110,
        291,
        382,
        50765
      ],
      "temperature": 0.2,
      "avg_logprob": -0.32337060408158735,
      "compression_ratio": 2.0689655172413794,
      "no_speech_prob": 0.0038866735994815826
    },
    {
      "id": 222,
      "seek": 99108,
      "start": 999.08,
      "end": 1005.32,
      "text": " an example of how you could implement your own memory by extending the base memory block",
      "tokens": [
        50765,
        364,
        1365,
        295,
        577,
        291,
        727,
        4445,
        428,
        1065,
        4675,
        538,
        24360,
        264,
        3096,
        4675,
        3461,
        51077
      ],
      "temperature": 0.2,
      "avg_logprob": -0.32337060408158735,
      "compression_ratio": 2.0689655172413794,
      "no_speech_prob": 0.0038866735994815826
    },
    {
      "id": 223,
      "seek": 99108,
      "start": 1005.96,
      "end": 1013.32,
      "text": " uh class so the idea is we wanted to we wanted to provide some implementations of long-term memory",
      "tokens": [
        51109,
        2232,
        1508,
        370,
        264,
        1558,
        307,
        321,
        1415,
        281,
        321,
        1415,
        281,
        2893,
        512,
        4445,
        763,
        295,
        938,
        12,
        7039,
        4675,
        51477
      ],
      "temperature": 0.2,
      "avg_logprob": -0.32337060408158735,
      "compression_ratio": 2.0689655172413794,
      "no_speech_prob": 0.0038866735994815826
    },
    {
      "id": 224,
      "seek": 99108,
      "start": 1013.32,
      "end": 1021.0,
      "text": " i don't think it's this one um so these are these are the the three memory blocks you mentioned the",
      "tokens": [
        51477,
        741,
        500,
        380,
        519,
        309,
        311,
        341,
        472,
        1105,
        370,
        613,
        366,
        613,
        366,
        264,
        264,
        1045,
        4675,
        8474,
        291,
        2835,
        264,
        51861
      ],
      "temperature": 0.2,
      "avg_logprob": -0.32337060408158735,
      "compression_ratio": 2.0689655172413794,
      "no_speech_prob": 0.0038866735994815826
    },
    {
      "id": 225,
      "seek": 102100,
      "start": 1021.0,
      "end": 1025.08,
      "text": " static fact extraction and vector memory block so so yeah let's explore this a little bit more in",
      "tokens": [
        50365,
        13437,
        1186,
        30197,
        293,
        8062,
        4675,
        3461,
        370,
        370,
        1338,
        718,
        311,
        6839,
        341,
        257,
        707,
        857,
        544,
        294,
        50569
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05482027577418907,
      "compression_ratio": 2.03781512605042,
      "no_speech_prob": 0.0025700205005705357
    },
    {
      "id": 226,
      "seek": 102100,
      "start": 1025.08,
      "end": 1031.56,
      "text": " depth so the the point of the memory block is these these specific uh vector memory blocks",
      "tokens": [
        50569,
        7161,
        370,
        264,
        264,
        935,
        295,
        264,
        4675,
        3461,
        307,
        613,
        613,
        2685,
        2232,
        8062,
        4675,
        8474,
        50893
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05482027577418907,
      "compression_ratio": 2.03781512605042,
      "no_speech_prob": 0.0025700205005705357
    },
    {
      "id": 227,
      "seek": 102100,
      "start": 1031.56,
      "end": 1038.12,
      "text": " static memory block etc that you're seeing on the screen here these are our uh implementations that",
      "tokens": [
        50893,
        13437,
        4675,
        3461,
        5183,
        300,
        291,
        434,
        2577,
        322,
        264,
        2568,
        510,
        613,
        366,
        527,
        2232,
        4445,
        763,
        300,
        51221
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05482027577418907,
      "compression_ratio": 2.03781512605042,
      "no_speech_prob": 0.0025700205005705357
    },
    {
      "id": 228,
      "seek": 102100,
      "start": 1038.12,
      "end": 1043.64,
      "text": " we have provided to our users as like packaged implementations of a memory block but the most",
      "tokens": [
        51221,
        321,
        362,
        5649,
        281,
        527,
        5022,
        382,
        411,
        38162,
        4445,
        763,
        295,
        257,
        4675,
        3461,
        457,
        264,
        881,
        51497
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05482027577418907,
      "compression_ratio": 2.03781512605042,
      "no_speech_prob": 0.0025700205005705357
    },
    {
      "id": 229,
      "seek": 102100,
      "start": 1043.64,
      "end": 1050.44,
      "text": " important thing is that you can implement your own memory blocks by extending the base memory block so",
      "tokens": [
        51497,
        1021,
        551,
        307,
        300,
        291,
        393,
        4445,
        428,
        1065,
        4675,
        8474,
        538,
        24360,
        264,
        3096,
        4675,
        3461,
        370,
        51837
      ],
      "temperature": 0.0,
      "avg_logprob": -0.05482027577418907,
      "compression_ratio": 2.03781512605042,
      "no_speech_prob": 0.0025700205005705357
    },
    {
      "id": 230,
      "seek": 105100,
      "start": 1051.72,
      "end": 1057.48,
      "text": " in these use cases we've provided you some but if there is a memory implementation that makes a lot",
      "tokens": [
        50401,
        294,
        613,
        764,
        3331,
        321,
        600,
        5649,
        291,
        512,
        457,
        498,
        456,
        307,
        257,
        4675,
        11420,
        300,
        1669,
        257,
        688,
        50689
      ],
      "temperature": 0.0,
      "avg_logprob": -0.03835874042291751,
      "compression_ratio": 1.7716894977168949,
      "no_speech_prob": 0.0036027526948601007
    },
    {
      "id": 231,
      "seek": 105100,
      "start": 1057.48,
      "end": 1064.84,
      "text": " more sense for your use case the idea is that you can simply extend the base memory block class",
      "tokens": [
        50689,
        544,
        2020,
        337,
        428,
        764,
        1389,
        264,
        1558,
        307,
        300,
        291,
        393,
        2935,
        10101,
        264,
        3096,
        4675,
        3461,
        1508,
        51057
      ],
      "temperature": 0.0,
      "avg_logprob": -0.03835874042291751,
      "compression_ratio": 1.7716894977168949,
      "no_speech_prob": 0.0036027526948601007
    },
    {
      "id": 232,
      "seek": 105100,
      "start": 1065.72,
      "end": 1073.32,
      "text": " which has a very few set of predefined functions and completely custom implement your own memory",
      "tokens": [
        51101,
        597,
        575,
        257,
        588,
        1326,
        992,
        295,
        659,
        37716,
        6828,
        293,
        2584,
        2375,
        4445,
        428,
        1065,
        4675,
        51481
      ],
      "temperature": 0.0,
      "avg_logprob": -0.03835874042291751,
      "compression_ratio": 1.7716894977168949,
      "no_speech_prob": 0.0036027526948601007
    },
    {
      "id": 233,
      "seek": 105100,
      "start": 1073.32,
      "end": 1080.44,
      "text": " block i think so to answer your question about what the memory block is it is a um how do i put",
      "tokens": [
        51481,
        3461,
        741,
        519,
        370,
        281,
        1867,
        428,
        1168,
        466,
        437,
        264,
        4675,
        3461,
        307,
        309,
        307,
        257,
        1105,
        577,
        360,
        741,
        829,
        51837
      ],
      "temperature": 0.0,
      "avg_logprob": -0.03835874042291751,
      "compression_ratio": 1.7716894977168949,
      "no_speech_prob": 0.0036027526948601007
    },
    {
      "id": 234,
      "seek": 108100,
      "start": 1081.0,
      "end": 1084.52,
      "text": " those brackets back into remember group okay does that make sense i think it does yes i know",
      "tokens": [
        50365,
        729,
        26179,
        646,
        666,
        1604,
        1594,
        1392,
        775,
        300,
        652,
        2020,
        741,
        519,
        309,
        775,
        2086,
        741,
        458,
        50541
      ],
      "temperature": 1.0,
      "avg_logprob": -4.2985893987840225,
      "compression_ratio": 1.696319018404908,
      "no_speech_prob": 0.0034200558438897133
    },
    {
      "id": 235,
      "seek": 108100,
      "start": 1084.52,
      "end": 1089.0,
      "text": " it's simply by using the Idag data borders b\u4f0a this time to the uh binary for scenario",
      "tokens": [
        50541,
        309,
        311,
        2935,
        538,
        1228,
        264,
        11506,
        559,
        1412,
        16287,
        272,
        48141,
        341,
        565,
        281,
        264,
        2232,
        17434,
        337,
        9005,
        50765
      ],
      "temperature": 1.0,
      "avg_logprob": -4.2985893987840225,
      "compression_ratio": 1.696319018404908,
      "no_speech_prob": 0.0034200558438897133
    },
    {
      "id": 236,
      "seek": 108100,
      "start": 1092.12,
      "end": 1099.0,
      "text": " and i'm using um flow project that builds memory blocks and after procedures we have",
      "tokens": [
        50921,
        293,
        741,
        478,
        1228,
        1105,
        3095,
        1716,
        300,
        15182,
        4675,
        8474,
        293,
        934,
        13846,
        321,
        362,
        51265
      ],
      "temperature": 1.0,
      "avg_logprob": -4.2985893987840225,
      "compression_ratio": 1.696319018404908,
      "no_speech_prob": 0.0034200558438897133
    },
    {
      "id": 237,
      "seek": 108100,
      "start": 1099.0,
      "end": 1104.2,
      "text": " you know instructor something that includes progressive writing and you can try separate",
      "tokens": [
        51265,
        291,
        458,
        18499,
        746,
        300,
        5974,
        16131,
        3579,
        293,
        291,
        393,
        853,
        4994,
        51525
      ],
      "temperature": 1.0,
      "avg_logprob": -4.2985893987840225,
      "compression_ratio": 1.696319018404908,
      "no_speech_prob": 0.0034200558438897133
    },
    {
      "id": 238,
      "seek": 108100,
      "start": 1104.2,
      "end": 1107.72,
      "text": " versions of the taylor conceptions to see if these can work so as and when you when you start this",
      "tokens": [
        51525,
        9606,
        295,
        264,
        39224,
        6746,
        3410,
        626,
        281,
        536,
        498,
        613,
        393,
        589,
        370,
        382,
        293,
        562,
        291,
        562,
        291,
        722,
        341,
        51701
      ],
      "temperature": 1.0,
      "avg_logprob": -4.2985893987840225,
      "compression_ratio": 1.696319018404908,
      "no_speech_prob": 0.0034200558438897133
    },
    {
      "id": 239,
      "seek": 108100,
      "start": 1107.72,
      "end": 1109.72,
      "text": " program it will be an easy problem right now not because you'll not find a place for state or value",
      "tokens": [
        51701,
        1461,
        309,
        486,
        312,
        364,
        1858,
        1154,
        558,
        586,
        406,
        570,
        291,
        603,
        406,
        915,
        257,
        1081,
        337,
        1785,
        420,
        2158,
        51801
      ],
      "temperature": 1.0,
      "avg_logprob": -4.2985893987840225,
      "compression_ratio": 1.696319018404908,
      "no_speech_prob": 0.0034200558438897133
    },
    {
      "id": 240,
      "seek": 110972,
      "start": 1109.72,
      "end": 1114.66,
      "text": " understand correctly. What do I choose each? You don't necessarily have to choose. You can",
      "tokens": [
        50365,
        1223,
        8944,
        13,
        708,
        360,
        286,
        2826,
        1184,
        30,
        509,
        500,
        380,
        4725,
        362,
        281,
        2826,
        13,
        509,
        393,
        50612
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10655238502903988,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.014136052690446377
    },
    {
      "id": 241,
      "seek": 110972,
      "start": 1114.66,
      "end": 1121.02,
      "text": " actually even use all of them. So an agent isn't limited to how many memory blocks it uses. So if",
      "tokens": [
        50612,
        767,
        754,
        764,
        439,
        295,
        552,
        13,
        407,
        364,
        9461,
        1943,
        380,
        5567,
        281,
        577,
        867,
        4675,
        8474,
        309,
        4960,
        13,
        407,
        498,
        50930
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10655238502903988,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.014136052690446377
    },
    {
      "id": 242,
      "seek": 110972,
      "start": 1121.02,
      "end": 1125.8,
      "text": " you want to, you can actually have an agent that makes use of all of these memory blocks in one go.",
      "tokens": [
        50930,
        291,
        528,
        281,
        11,
        291,
        393,
        767,
        362,
        364,
        9461,
        300,
        1669,
        764,
        295,
        439,
        295,
        613,
        4675,
        8474,
        294,
        472,
        352,
        13,
        51169
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10655238502903988,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.014136052690446377
    },
    {
      "id": 243,
      "seek": 110972,
      "start": 1126.8,
      "end": 1132.98,
      "text": " But let me give you some examples. If I have an application where I want to pay more attention",
      "tokens": [
        51219,
        583,
        718,
        385,
        976,
        291,
        512,
        5110,
        13,
        759,
        286,
        362,
        364,
        3861,
        689,
        286,
        528,
        281,
        1689,
        544,
        3202,
        51528
      ],
      "temperature": 0.0,
      "avg_logprob": -0.10655238502903988,
      "compression_ratio": 1.6798245614035088,
      "no_speech_prob": 0.014136052690446377
    },
    {
      "id": 244,
      "seek": 113298,
      "start": 1132.98,
      "end": 1140.5,
      "text": " to past discussions with an end, you want to make sure that I'm never losing any context about what",
      "tokens": [
        50365,
        281,
        1791,
        11088,
        365,
        364,
        917,
        11,
        291,
        528,
        281,
        652,
        988,
        300,
        286,
        478,
        1128,
        7027,
        604,
        4319,
        466,
        437,
        50741
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07178813532779091,
      "compression_ratio": 1.7669172932330828,
      "no_speech_prob": 0.0004860734916292131
    },
    {
      "id": 245,
      "seek": 113298,
      "start": 1140.5,
      "end": 1147.28,
      "text": " that discussion contained. I want the user to be able to ask questions like, what was that thing",
      "tokens": [
        50741,
        300,
        5017,
        16212,
        13,
        286,
        528,
        264,
        4195,
        281,
        312,
        1075,
        281,
        1029,
        1651,
        411,
        11,
        437,
        390,
        300,
        551,
        51080
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07178813532779091,
      "compression_ratio": 1.7669172932330828,
      "no_speech_prob": 0.0004860734916292131
    },
    {
      "id": 246,
      "seek": 113298,
      "start": 1147.28,
      "end": 1153.8,
      "text": " that I said that I asked you about, I don't know, olive oil the other day. And you want to make sure",
      "tokens": [
        51080,
        300,
        286,
        848,
        300,
        286,
        2351,
        291,
        466,
        11,
        286,
        500,
        380,
        458,
        11,
        15981,
        3184,
        264,
        661,
        786,
        13,
        400,
        291,
        528,
        281,
        652,
        988,
        51406
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07178813532779091,
      "compression_ratio": 1.7669172932330828,
      "no_speech_prob": 0.0004860734916292131
    },
    {
      "id": 247,
      "seek": 113298,
      "start": 1153.8,
      "end": 1158.76,
      "text": " that the agent remembers that discussion point. I would definitely suggest using a vector memory",
      "tokens": [
        51406,
        300,
        264,
        9461,
        26228,
        300,
        5017,
        935,
        13,
        286,
        576,
        2138,
        3402,
        1228,
        257,
        8062,
        4675,
        51654
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07178813532779091,
      "compression_ratio": 1.7669172932330828,
      "no_speech_prob": 0.0004860734916292131
    },
    {
      "id": 248,
      "seek": 113298,
      "start": 1158.76,
      "end": 1162.14,
      "text": " block, because this allows you to vectorize all of the chat messages.",
      "tokens": [
        51654,
        3461,
        11,
        570,
        341,
        4045,
        291,
        281,
        8062,
        1125,
        439,
        295,
        264,
        5081,
        7897,
        13,
        51823
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07178813532779091,
      "compression_ratio": 1.7669172932330828,
      "no_speech_prob": 0.0004860734916292131
    },
    {
      "id": 249,
      "seek": 113298,
      "start": 1162.74,
      "end": 1162.88,
      "text": " Okay.",
      "tokens": [
        51853,
        1033,
        13,
        51860
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07178813532779091,
      "compression_ratio": 1.7669172932330828,
      "no_speech_prob": 0.0004860734916292131
    },
    {
      "id": 250,
      "seek": 116288,
      "start": 1162.88,
      "end": 1167.1000000000001,
      "text": " And at each iteration, yeah, so you're basically doing semantic similarity search at each",
      "tokens": [
        50365,
        400,
        412,
        1184,
        24784,
        11,
        1338,
        11,
        370,
        291,
        434,
        1936,
        884,
        47982,
        32194,
        3164,
        412,
        1184,
        50576
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17996834648980034,
      "compression_ratio": 1.8982456140350876,
      "no_speech_prob": 0.00041075178887695074
    },
    {
      "id": 251,
      "seek": 116288,
      "start": 1167.1000000000001,
      "end": 1172.5600000000002,
      "text": " iteration to make sure that the agent knows if there's any past discussion that's happened there.",
      "tokens": [
        50576,
        24784,
        281,
        652,
        988,
        300,
        264,
        9461,
        3255,
        498,
        456,
        311,
        604,
        1791,
        5017,
        300,
        311,
        2011,
        456,
        13,
        50849
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17996834648980034,
      "compression_ratio": 1.8982456140350876,
      "no_speech_prob": 0.00041075178887695074
    },
    {
      "id": 252,
      "seek": 116288,
      "start": 1173.2800000000002,
      "end": 1182.1200000000001,
      "text": " If, however, let's say you have an application, where the main use case is to make sure that you",
      "tokens": [
        50885,
        759,
        11,
        4461,
        11,
        718,
        311,
        584,
        291,
        362,
        364,
        3861,
        11,
        689,
        264,
        2135,
        764,
        1389,
        307,
        281,
        652,
        988,
        300,
        291,
        51327
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17996834648980034,
      "compression_ratio": 1.8982456140350876,
      "no_speech_prob": 0.00041075178887695074
    },
    {
      "id": 253,
      "seek": 116288,
      "start": 1182.1200000000001,
      "end": 1187.3400000000001,
      "text": " always have the most up to date facts about your user or about a topic.",
      "tokens": [
        51327,
        1009,
        362,
        264,
        881,
        493,
        281,
        4002,
        9130,
        466,
        428,
        4195,
        420,
        466,
        257,
        4829,
        13,
        51588
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17996834648980034,
      "compression_ratio": 1.8982456140350876,
      "no_speech_prob": 0.00041075178887695074
    },
    {
      "id": 254,
      "seek": 116288,
      "start": 1187.8400000000001,
      "end": 1192.8000000000002,
      "text": " Maybe this is a point where you simply use a fact memory block. And maybe you can even",
      "tokens": [
        51613,
        2704,
        341,
        307,
        257,
        935,
        689,
        291,
        2935,
        764,
        257,
        1186,
        4675,
        3461,
        13,
        400,
        1310,
        291,
        393,
        754,
        51861
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17996834648980034,
      "compression_ratio": 1.8982456140350876,
      "no_speech_prob": 0.00041075178887695074
    },
    {
      "id": 255,
      "seek": 116288,
      "start": 1192.8000000000002,
      "end": 1192.8600000000001,
      "text": " say, okay, I'm going to do this. And then you can do this. And then you can do this. And then you",
      "tokens": [
        51861,
        584,
        11,
        1392,
        11,
        286,
        478,
        516,
        281,
        360,
        341,
        13,
        400,
        550,
        291,
        393,
        360,
        341,
        13,
        400,
        550,
        291,
        393,
        360,
        341,
        13,
        400,
        550,
        291,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17996834648980034,
      "compression_ratio": 1.8982456140350876,
      "no_speech_prob": 0.00041075178887695074
    },
    {
      "id": 256,
      "seek": 119288,
      "start": 1192.88,
      "end": 1202.0400000000002,
      "text": " can skip the vector memory block. Maybe a good example of that is that let's say you are a you're",
      "tokens": [
        50365,
        393,
        10023,
        264,
        8062,
        4675,
        3461,
        13,
        2704,
        257,
        665,
        1365,
        295,
        300,
        307,
        300,
        718,
        311,
        584,
        291,
        366,
        257,
        291,
        434,
        50823
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09010420905219184,
      "compression_ratio": 1.646808510638298,
      "no_speech_prob": 0.0006841238355264068
    },
    {
      "id": 257,
      "seek": 119288,
      "start": 1202.0400000000002,
      "end": 1209.0200000000002,
      "text": " building an application, where the main use case is that you want to provide airline support, like",
      "tokens": [
        50823,
        2390,
        364,
        3861,
        11,
        689,
        264,
        2135,
        764,
        1389,
        307,
        300,
        291,
        528,
        281,
        2893,
        29528,
        1406,
        11,
        411,
        51172
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09010420905219184,
      "compression_ratio": 1.646808510638298,
      "no_speech_prob": 0.0006841238355264068
    },
    {
      "id": 258,
      "seek": 119288,
      "start": 1209.0200000000002,
      "end": 1215.8600000000001,
      "text": " you want to provide support to people who are taking flights. And maybe in this case, the most",
      "tokens": [
        51172,
        291,
        528,
        281,
        2893,
        1406,
        281,
        561,
        567,
        366,
        1940,
        21089,
        13,
        400,
        1310,
        294,
        341,
        1389,
        11,
        264,
        881,
        51514
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09010420905219184,
      "compression_ratio": 1.646808510638298,
      "no_speech_prob": 0.0006841238355264068
    },
    {
      "id": 259,
      "seek": 119288,
      "start": 1215.8600000000001,
      "end": 1221.5800000000002,
      "text": " important information is very simple facts. What day are they flying? What's the flight number?",
      "tokens": [
        51514,
        1021,
        1589,
        307,
        588,
        2199,
        9130,
        13,
        708,
        786,
        366,
        436,
        7137,
        30,
        708,
        311,
        264,
        7018,
        1230,
        30,
        51800
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09010420905219184,
      "compression_ratio": 1.646808510638298,
      "no_speech_prob": 0.0006841238355264068
    },
    {
      "id": 260,
      "seek": 122288,
      "start": 1222.88,
      "end": 1230.16,
      "text": " Which city are they flying to? From where, etc. And maybe in this situation, past chat is not that",
      "tokens": [
        50365,
        3013,
        2307,
        366,
        436,
        7137,
        281,
        30,
        3358,
        689,
        11,
        5183,
        13,
        400,
        1310,
        294,
        341,
        2590,
        11,
        1791,
        5081,
        307,
        406,
        300,
        50729
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15566349029541016,
      "compression_ratio": 1.6135593220338984,
      "no_speech_prob": 0.0006792111089453101
    },
    {
      "id": 261,
      "seek": 122288,
      "start": 1230.16,
      "end": 1236.4,
      "text": " important, more than the short term memory. So it's more about deciding what exactly do I want to",
      "tokens": [
        50729,
        1021,
        11,
        544,
        813,
        264,
        2099,
        1433,
        4675,
        13,
        407,
        309,
        311,
        544,
        466,
        17990,
        437,
        2293,
        360,
        286,
        528,
        281,
        51041
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15566349029541016,
      "compression_ratio": 1.6135593220338984,
      "no_speech_prob": 0.0006792111089453101
    },
    {
      "id": 262,
      "seek": 122288,
      "start": 1236.4,
      "end": 1243.1200000000001,
      "text": " provide the end user and picking long term memory blocks accordingly. But like I said, you don't",
      "tokens": [
        51041,
        2893,
        264,
        917,
        4195,
        293,
        8867,
        938,
        1433,
        4675,
        8474,
        19717,
        13,
        583,
        411,
        286,
        848,
        11,
        291,
        500,
        380,
        51377
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15566349029541016,
      "compression_ratio": 1.6135593220338984,
      "no_speech_prob": 0.0006792111089453101
    },
    {
      "id": 263,
      "seek": 122288,
      "start": 1243.1200000000001,
      "end": 1246.1200000000001,
      "text": " actually have to pick you can also use a mixture of all of them.",
      "tokens": [
        51377,
        767,
        362,
        281,
        1888,
        291,
        393,
        611,
        764,
        257,
        9925,
        295,
        439,
        295,
        552,
        13,
        51527
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15566349029541016,
      "compression_ratio": 1.6135593220338984,
      "no_speech_prob": 0.0006792111089453101
    },
    {
      "id": 264,
      "seek": 122288,
      "start": 1246.1200000000001,
      "end": 1252.16,
      "text": " Yeah. And like you mentioned before, this is a this is a pretty powerful new extension to agentic applications.",
      "tokens": [
        51527,
        865,
        13,
        400,
        411,
        291,
        2835,
        949,
        11,
        341,
        307,
        257,
        341,
        307,
        257,
        1238,
        4005,
        777,
        10320,
        281,
        9461,
        299,
        5821,
        13,
        51829
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15566349029541016,
      "compression_ratio": 1.6135593220338984,
      "no_speech_prob": 0.0006792111089453101
    },
    {
      "id": 265,
      "seek": 122288,
      "start": 1252.16,
      "end": 1252.8000000000002,
      "text": " Yeah.",
      "tokens": [
        51829,
        865,
        13,
        51861
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15566349029541016,
      "compression_ratio": 1.6135593220338984,
      "no_speech_prob": 0.0006792111089453101
    },
    {
      "id": 266,
      "seek": 125280,
      "start": 1252.8,
      "end": 1258.48,
      "text": " Because as you noted before, you know, we can build an agentic application where the agent uses a rag",
      "tokens": [
        50365,
        1436,
        382,
        291,
        12964,
        949,
        11,
        291,
        458,
        11,
        321,
        393,
        1322,
        364,
        9461,
        299,
        3861,
        689,
        264,
        9461,
        4960,
        257,
        17539,
        50649
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14257260198178498,
      "compression_ratio": 1.9642857142857142,
      "no_speech_prob": 0.004020261578261852
    },
    {
      "id": 267,
      "seek": 125280,
      "start": 1258.48,
      "end": 1265.82,
      "text": " database as a tool, and is also enhanced by vectorized chat histories, where if you're if you're if you're if",
      "tokens": [
        50649,
        8149,
        382,
        257,
        2290,
        11,
        293,
        307,
        611,
        21191,
        538,
        8062,
        1602,
        5081,
        30631,
        11,
        689,
        498,
        291,
        434,
        498,
        291,
        434,
        498,
        291,
        434,
        498,
        51016
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14257260198178498,
      "compression_ratio": 1.9642857142857142,
      "no_speech_prob": 0.004020261578261852
    },
    {
      "id": 268,
      "seek": 125280,
      "start": 1265.82,
      "end": 1271.68,
      "text": " you have chat histories that span not just one user, but multiple users over a long period of time, the agent",
      "tokens": [
        51016,
        291,
        362,
        5081,
        30631,
        300,
        16174,
        406,
        445,
        472,
        4195,
        11,
        457,
        3866,
        5022,
        670,
        257,
        938,
        2896,
        295,
        565,
        11,
        264,
        9461,
        51309
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14257260198178498,
      "compression_ratio": 1.9642857142857142,
      "no_speech_prob": 0.004020261578261852
    },
    {
      "id": 269,
      "seek": 125280,
      "start": 1271.68,
      "end": 1279.98,
      "text": " can recall information from a very complex source of information, not just a rag database, but also history of",
      "tokens": [
        51309,
        393,
        9901,
        1589,
        490,
        257,
        588,
        3997,
        4009,
        295,
        1589,
        11,
        406,
        445,
        257,
        17539,
        8149,
        11,
        457,
        611,
        2503,
        295,
        51724
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14257260198178498,
      "compression_ratio": 1.9642857142857142,
      "no_speech_prob": 0.004020261578261852
    },
    {
      "id": 270,
      "seek": 125280,
      "start": 1279.98,
      "end": 1282.08,
      "text": " conversation with multiple users across a long period of time.",
      "tokens": [
        51724,
        3761,
        365,
        3866,
        5022,
        2108,
        257,
        938,
        2896,
        295,
        565,
        13,
        51829
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14257260198178498,
      "compression_ratio": 1.9642857142857142,
      "no_speech_prob": 0.004020261578261852
    },
    {
      "id": 271,
      "seek": 128208,
      "start": 1282.08,
      "end": 1286.6799999999998,
      "text": " So this is I think this is a really, really nice addition to the agentic stack.",
      "tokens": [
        50365,
        407,
        341,
        307,
        286,
        519,
        341,
        307,
        257,
        534,
        11,
        534,
        1481,
        4500,
        281,
        264,
        9461,
        299,
        8630,
        13,
        50595
      ],
      "temperature": 1.0,
      "avg_logprob": -0.2400249252971421,
      "compression_ratio": 1.9020408163265305,
      "no_speech_prob": 0.005750017240643501
    },
    {
      "id": 272,
      "seek": 128208,
      "start": 1287.8799999999999,
      "end": 1296.52,
      "text": " Okay, so so you mentioned before that we can also define our own custom memory blocks, and you",
      "tokens": [
        50655,
        1033,
        11,
        370,
        370,
        291,
        2835,
        949,
        300,
        321,
        393,
        611,
        6964,
        527,
        1065,
        2375,
        4675,
        8474,
        11,
        293,
        291,
        51087
      ],
      "temperature": 1.0,
      "avg_logprob": -0.2400249252971421,
      "compression_ratio": 1.9020408163265305,
      "no_speech_prob": 0.005750017240643501
    },
    {
      "id": 273,
      "seek": 128208,
      "start": 1296.52,
      "end": 1302.76,
      "text": " mentioned that we can do so by by using the base memory block class. So this is a I would assume",
      "tokens": [
        51087,
        2835,
        300,
        321,
        393,
        360,
        370,
        538,
        538,
        1228,
        264,
        3096,
        4675,
        3461,
        1508,
        13,
        407,
        341,
        307,
        257,
        286,
        576,
        6552,
        51399
      ],
      "temperature": 1.0,
      "avg_logprob": -0.2400249252971421,
      "compression_ratio": 1.9020408163265305,
      "no_speech_prob": 0.005750017240643501
    },
    {
      "id": 274,
      "seek": 128208,
      "start": 1302.76,
      "end": 1307.0,
      "text": " this is a simple example of how to how to define your own custom memory blocks. Is this correct?",
      "tokens": [
        51399,
        341,
        307,
        257,
        2199,
        1365,
        295,
        577,
        281,
        577,
        281,
        6964,
        428,
        1065,
        2375,
        4675,
        8474,
        13,
        1119,
        341,
        3006,
        30,
        51611
      ],
      "temperature": 1.0,
      "avg_logprob": -0.2400249252971421,
      "compression_ratio": 1.9020408163265305,
      "no_speech_prob": 0.005750017240643501
    },
    {
      "id": 275,
      "seek": 128208,
      "start": 1307.56,
      "end": 1311.9199999999998,
      "text": " Yes, exactly. So this is actually quite a funny example. I'm not sure if this actually, you know,",
      "tokens": [
        51639,
        1079,
        11,
        2293,
        13,
        407,
        341,
        307,
        767,
        1596,
        257,
        4074,
        1365,
        13,
        286,
        478,
        406,
        988,
        498,
        341,
        767,
        11,
        291,
        458,
        11,
        51857
      ],
      "temperature": 1.0,
      "avg_logprob": -0.2400249252971421,
      "compression_ratio": 1.9020408163265305,
      "no_speech_prob": 0.005750017240643501
    },
    {
      "id": 276,
      "seek": 131208,
      "start": 1312.08,
      "end": 1319.72,
      "text": " would ever exist in the real world, maybe. But this time, the memory block itself is actually",
      "tokens": [
        50365,
        576,
        1562,
        2514,
        294,
        264,
        957,
        1002,
        11,
        1310,
        13,
        583,
        341,
        565,
        11,
        264,
        4675,
        3461,
        2564,
        307,
        767,
        50747
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07013059944234869,
      "compression_ratio": 1.6651785714285714,
      "no_speech_prob": 0.008529115468263626
    },
    {
      "id": 277,
      "seek": 131208,
      "start": 1319.72,
      "end": 1325.62,
      "text": " a mention counter. So in this implementation, again, this was provided by Logan, this memory",
      "tokens": [
        50747,
        257,
        2152,
        5682,
        13,
        407,
        294,
        341,
        11420,
        11,
        797,
        11,
        341,
        390,
        5649,
        538,
        22689,
        11,
        341,
        4675,
        51042
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07013059944234869,
      "compression_ratio": 1.6651785714285714,
      "no_speech_prob": 0.008529115468263626
    },
    {
      "id": 278,
      "seek": 131208,
      "start": 1325.62,
      "end": 1331.62,
      "text": " block, the main thing it's doing is it's keeping track of how many times a name was mentioned.",
      "tokens": [
        51042,
        3461,
        11,
        264,
        2135,
        551,
        309,
        311,
        884,
        307,
        309,
        311,
        5145,
        2837,
        295,
        577,
        867,
        1413,
        257,
        1315,
        390,
        2835,
        13,
        51342
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07013059944234869,
      "compression_ratio": 1.6651785714285714,
      "no_speech_prob": 0.008529115468263626
    },
    {
      "id": 279,
      "seek": 131208,
      "start": 1332.6,
      "end": 1337.32,
      "text": " It's kind of silly, but it's a good example of showing how easy it is to implement your own",
      "tokens": [
        51391,
        467,
        311,
        733,
        295,
        11774,
        11,
        457,
        309,
        311,
        257,
        665,
        1365,
        295,
        4099,
        577,
        1858,
        309,
        307,
        281,
        4445,
        428,
        1065,
        51627
      ],
      "temperature": 0.0,
      "avg_logprob": -0.07013059944234869,
      "compression_ratio": 1.6651785714285714,
      "no_speech_prob": 0.008529115468263626
    },
    {
      "id": 280,
      "seek": 133732,
      "start": 1337.32,
      "end": 1343.76,
      "text": " memory. So once you've implemented this, which is an extension of the base memory block,",
      "tokens": [
        50365,
        4675,
        13,
        407,
        1564,
        291,
        600,
        12270,
        341,
        11,
        597,
        307,
        364,
        10320,
        295,
        264,
        3096,
        4675,
        3461,
        11,
        50687
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09212785470681112,
      "compression_ratio": 1.8078291814946619,
      "no_speech_prob": 0.0005150869255885482
    },
    {
      "id": 281,
      "seek": 133732,
      "start": 1344.4199999999998,
      "end": 1349.5,
      "text": " this mention counter memory block can be used as like any other memory block you've seen before,",
      "tokens": [
        50720,
        341,
        2152,
        5682,
        4675,
        3461,
        393,
        312,
        1143,
        382,
        411,
        604,
        661,
        4675,
        3461,
        291,
        600,
        1612,
        949,
        11,
        50974
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09212785470681112,
      "compression_ratio": 1.8078291814946619,
      "no_speech_prob": 0.0005150869255885482
    },
    {
      "id": 282,
      "seek": 133732,
      "start": 1349.56,
      "end": 1353.9199999999998,
      "text": " you simply provide it to the agent, and it does the job of keeping track in this case of counting",
      "tokens": [
        50977,
        291,
        2935,
        2893,
        309,
        281,
        264,
        9461,
        11,
        293,
        309,
        775,
        264,
        1691,
        295,
        5145,
        2837,
        294,
        341,
        1389,
        295,
        13251,
        51195
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09212785470681112,
      "compression_ratio": 1.8078291814946619,
      "no_speech_prob": 0.0005150869255885482
    },
    {
      "id": 283,
      "seek": 133732,
      "start": 1353.9199999999998,
      "end": 1360.78,
      "text": " how many times Logan was mentioned. Okay, but I think at the core of this is, you know, you're",
      "tokens": [
        51195,
        577,
        867,
        1413,
        22689,
        390,
        2835,
        13,
        1033,
        11,
        457,
        286,
        519,
        412,
        264,
        4965,
        295,
        341,
        307,
        11,
        291,
        458,
        11,
        291,
        434,
        51538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09212785470681112,
      "compression_ratio": 1.8078291814946619,
      "no_speech_prob": 0.0005150869255885482
    },
    {
      "id": 284,
      "seek": 133732,
      "start": 1360.78,
      "end": 1365.36,
      "text": " not just limited by those three memory blocks, you can create your own custom one by inheriting",
      "tokens": [
        51538,
        406,
        445,
        5567,
        538,
        729,
        1045,
        4675,
        8474,
        11,
        291,
        393,
        1884,
        428,
        1065,
        2375,
        472,
        538,
        9484,
        1748,
        51767
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09212785470681112,
      "compression_ratio": 1.8078291814946619,
      "no_speech_prob": 0.0005150869255885482
    },
    {
      "id": 285,
      "seek": 133732,
      "start": 1365.36,
      "end": 1367.3,
      "text": " from the base memory block class.",
      "tokens": [
        51767,
        490,
        264,
        3096,
        4675,
        3461,
        1508,
        13,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.09212785470681112,
      "compression_ratio": 1.8078291814946619,
      "no_speech_prob": 0.0005150869255885482
    },
    {
      "id": 286,
      "seek": 136732,
      "start": 1367.32,
      "end": 1396.74,
      "text": " I think that is a key message. Okay, so we talked a little bit about this. So maybe maybe let's move on to some of the more complex questions. So you mentioned in the past, this can either be in memory, Postgres, or through your own vector database. So if you wanted to use something like Postgres, you can use an async database, URI. You talk from the memory block implementations, we also saw we can choose if we want to do something like vector memory, we can use quadrant or any other vector DB.",
      "tokens": [
        50365,
        286,
        519,
        300,
        307,
        257,
        2141,
        3636,
        13,
        1033,
        11,
        370,
        321,
        2825,
        257,
        707,
        857,
        466,
        341,
        13,
        407,
        1310,
        1310,
        718,
        311,
        1286,
        322,
        281,
        512,
        295,
        264,
        544,
        3997,
        1651,
        13,
        407,
        291,
        2835,
        294,
        264,
        1791,
        11,
        341,
        393,
        2139,
        312,
        294,
        4675,
        11,
        10223,
        45189,
        11,
        420,
        807,
        428,
        1065,
        8062,
        8149,
        13,
        407,
        498,
        291,
        1415,
        281,
        764,
        746,
        411,
        10223,
        45189,
        11,
        291,
        393,
        764,
        364,
        382,
        34015,
        8149,
        11,
        624,
        5577,
        13,
        509,
        751,
        490,
        264,
        4675,
        3461,
        4445,
        763,
        11,
        321,
        611,
        1866,
        321,
        393,
        2826,
        498,
        321,
        528,
        281,
        360,
        746,
        411,
        8062,
        4675,
        11,
        321,
        393,
        764,
        46856,
        420,
        604,
        661,
        8062,
        26754,
        13,
        51836
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13747095657607256,
      "compression_ratio": 1.7386759581881532,
      "no_speech_prob": 0.0010888469405472279
    },
    {
      "id": 287,
      "seek": 139732,
      "start": 1397.32,
      "end": 1413.3999999999999,
      "text": " And just to stay on track, I think I want to explore some of the more complex questions. So So how how does a new memory component decide which chat messages migrate from short term to long term storage? And can that policy be customized?",
      "tokens": [
        50365,
        400,
        445,
        281,
        1754,
        322,
        2837,
        11,
        286,
        519,
        286,
        528,
        281,
        6839,
        512,
        295,
        264,
        544,
        3997,
        1651,
        13,
        407,
        407,
        577,
        577,
        775,
        257,
        777,
        4675,
        6542,
        4536,
        597,
        5081,
        7897,
        31821,
        490,
        2099,
        1433,
        281,
        938,
        1433,
        6725,
        30,
        400,
        393,
        300,
        3897,
        312,
        30581,
        30,
        51169
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15605464348426232,
      "compression_ratio": 1.5742971887550201,
      "no_speech_prob": 0.0009876294061541557
    },
    {
      "id": 288,
      "seek": 139732,
      "start": 1414.3999999999999,
      "end": 1427.32,
      "text": " So for now, it's solely based on token limit. That's not to say that that will be the case forever. But for now, the migration so while you can very much",
      "tokens": [
        51219,
        407,
        337,
        586,
        11,
        309,
        311,
        23309,
        2361,
        322,
        14862,
        4948,
        13,
        663,
        311,
        406,
        281,
        584,
        300,
        300,
        486,
        312,
        264,
        1389,
        5680,
        13,
        583,
        337,
        586,
        11,
        264,
        17011,
        370,
        1339,
        291,
        393,
        588,
        709,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15605464348426232,
      "compression_ratio": 1.5742971887550201,
      "no_speech_prob": 0.0009876294061541557
    },
    {
      "id": 289,
      "seek": 142732,
      "start": 1427.32,
      "end": 1440.84,
      "text": " customize long term memory, our implementation of short term memory, and when information from short term memory is flushed into long term memory is based on token count, you can customize the token count yourself. So you can",
      "tokens": [
        50365,
        19734,
        938,
        1433,
        4675,
        11,
        527,
        11420,
        295,
        2099,
        1433,
        4675,
        11,
        293,
        562,
        1589,
        490,
        2099,
        1433,
        4675,
        307,
        19568,
        292,
        666,
        938,
        1433,
        4675,
        307,
        2361,
        322,
        14862,
        1207,
        11,
        291,
        393,
        19734,
        264,
        14862,
        1207,
        1803,
        13,
        407,
        291,
        393,
        51041
      ],
      "temperature": 0.8,
      "avg_logprob": -0.2539386749267578,
      "compression_ratio": 2.194871794871795,
      "no_speech_prob": 0.05412261560559273
    },
    {
      "id": 290,
      "seek": 142732,
      "start": 1440.84,
      "end": 1456.3999999999999,
      "text": " decide how, how long the token count is. Or how short the token count is before you flush to long term memory. That is something that you can customize the, the way we move from short term to long term.",
      "tokens": [
        51041,
        4536,
        577,
        11,
        577,
        938,
        264,
        14862,
        1207,
        307,
        13,
        1610,
        577,
        2099,
        264,
        14862,
        1207,
        307,
        949,
        291,
        19568,
        281,
        938,
        1433,
        4675,
        13,
        663,
        307,
        746,
        300,
        291,
        393,
        19734,
        264,
        11,
        264,
        636,
        321,
        1286,
        490,
        2099,
        1433,
        281,
        938,
        1433,
        13,
        51819
      ],
      "temperature": 0.8,
      "avg_logprob": -0.2539386749267578,
      "compression_ratio": 2.194871794871795,
      "no_speech_prob": 0.05412261560559273
    },
    {
      "id": 291,
      "seek": 145640,
      "start": 1456.4,
      "end": 1459.52,
      "text": " term to long term is currently limited to token count.",
      "tokens": [
        50365,
        1433,
        281,
        938,
        1433,
        307,
        4362,
        5567,
        281,
        14862,
        1207,
        13,
        50521
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 292,
      "seek": 145640,
      "start": 1460.1200000000001,
      "end": 1462.46,
      "text": " Okay. Yeah.",
      "tokens": [
        50551,
        1033,
        13,
        865,
        13,
        50668
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 293,
      "seek": 145640,
      "start": 1464.5600000000002,
      "end": 1467.8000000000002,
      "text": " And let's suppose let's suppose that I have a distributed agent",
      "tokens": [
        50773,
        400,
        718,
        311,
        7297,
        718,
        311,
        7297,
        300,
        286,
        362,
        257,
        12631,
        9461,
        50935
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 294,
      "seek": 145640,
      "start": 1467.8000000000002,
      "end": 1470.76,
      "text": " deployment. So let's suppose I'm not dealing with just not just",
      "tokens": [
        50935,
        19317,
        13,
        407,
        718,
        311,
        7297,
        286,
        478,
        406,
        6260,
        365,
        445,
        406,
        445,
        51083
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 295,
      "seek": 145640,
      "start": 1470.8000000000002,
      "end": 1473.8400000000001,
      "text": " one agent, but let's say I have an architecture or like, let's",
      "tokens": [
        51085,
        472,
        9461,
        11,
        457,
        718,
        311,
        584,
        286,
        362,
        364,
        9482,
        420,
        411,
        11,
        718,
        311,
        51237
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 296,
      "seek": 145640,
      "start": 1473.8400000000001,
      "end": 1476.74,
      "text": " say I have, I don't know, some kind of architecture system",
      "tokens": [
        51237,
        584,
        286,
        362,
        11,
        286,
        500,
        380,
        458,
        11,
        512,
        733,
        295,
        9482,
        1185,
        51382
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 297,
      "seek": 145640,
      "start": 1476.74,
      "end": 1479.1200000000001,
      "text": " where I have more than one agent, maybe there's a",
      "tokens": [
        51382,
        689,
        286,
        362,
        544,
        813,
        472,
        9461,
        11,
        1310,
        456,
        311,
        257,
        51501
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 298,
      "seek": 145640,
      "start": 1479.1200000000001,
      "end": 1482.9,
      "text": " supervisor agent trying to determine which which task if",
      "tokens": [
        51501,
        24610,
        9461,
        1382,
        281,
        6997,
        597,
        597,
        5633,
        498,
        51690
      ],
      "temperature": 0.0,
      "avg_logprob": -0.16814888275421416,
      "compression_ratio": 1.7625,
      "no_speech_prob": 0.04312917962670326
    },
    {
      "id": 299,
      "seek": 148290,
      "start": 1482.9,
      "end": 1487.1000000000001,
      "text": " which other team of agents, how do you ensure memory consistency",
      "tokens": [
        50365,
        597,
        661,
        1469,
        295,
        12554,
        11,
        577,
        360,
        291,
        5586,
        4675,
        14416,
        50575
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 300,
      "seek": 148290,
      "start": 1487.1000000000001,
      "end": 1490.8200000000002,
      "text": " across replicas? You know, how is memory? How is memory being",
      "tokens": [
        50575,
        2108,
        3248,
        9150,
        30,
        509,
        458,
        11,
        577,
        307,
        4675,
        30,
        1012,
        307,
        4675,
        885,
        50761
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 301,
      "seek": 148290,
      "start": 1490.8200000000002,
      "end": 1493.7800000000002,
      "text": " handled when I have more than one agent in the picture?",
      "tokens": [
        50761,
        18033,
        562,
        286,
        362,
        544,
        813,
        472,
        9461,
        294,
        264,
        3036,
        30,
        50909
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 302,
      "seek": 148290,
      "start": 1494.88,
      "end": 1499.16,
      "text": " So I thought about this question before. And to me, it's, it's",
      "tokens": [
        50964,
        407,
        286,
        1194,
        466,
        341,
        1168,
        949,
        13,
        400,
        281,
        385,
        11,
        309,
        311,
        11,
        309,
        311,
        51178
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 303,
      "seek": 148290,
      "start": 1499.16,
      "end": 1502.18,
      "text": " not such a straightforward question. So for one, maybe we",
      "tokens": [
        51178,
        406,
        1270,
        257,
        15325,
        1168,
        13,
        407,
        337,
        472,
        11,
        1310,
        321,
        51329
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 304,
      "seek": 148290,
      "start": 1502.18,
      "end": 1505.22,
      "text": " can start with short term memory, which, in my view, this",
      "tokens": [
        51329,
        393,
        722,
        365,
        2099,
        1433,
        4675,
        11,
        597,
        11,
        294,
        452,
        1910,
        11,
        341,
        51481
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 305,
      "seek": 148290,
      "start": 1505.22,
      "end": 1508.4,
      "text": " should probably not be replicated or shared in any way,",
      "tokens": [
        51481,
        820,
        1391,
        406,
        312,
        46365,
        420,
        5507,
        294,
        604,
        636,
        11,
        51640
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 306,
      "seek": 148290,
      "start": 1508.4,
      "end": 1512.24,
      "text": " because this short term memory should be for that agent",
      "tokens": [
        51640,
        570,
        341,
        2099,
        1433,
        4675,
        820,
        312,
        337,
        300,
        9461,
        51832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 307,
      "seek": 148290,
      "start": 1512.2800000000002,
      "end": 1512.88,
      "text": " instance.",
      "tokens": [
        51834,
        5197,
        13,
        51864
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1218959593003796,
      "compression_ratio": 1.7127659574468086,
      "no_speech_prob": 0.0012034912360832095
    },
    {
      "id": 308,
      "seek": 151290,
      "start": 1512.9,
      "end": 1517.0400000000002,
      "text": " But let's talk about long term memory. When we say distributed",
      "tokens": [
        50365,
        583,
        718,
        311,
        751,
        466,
        938,
        1433,
        4675,
        13,
        1133,
        321,
        584,
        12631,
        50572
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4762406655407827,
      "compression_ratio": 1.699604743083004,
      "no_speech_prob": 8.9135835878551e-05
    },
    {
      "id": 309,
      "seek": 151290,
      "start": 1517.0400000000002,
      "end": 1521.66,
      "text": " agents, we're probably referring to a system, potentially, that",
      "tokens": [
        50572,
        12554,
        11,
        321,
        434,
        1391,
        13761,
        281,
        257,
        1185,
        11,
        7263,
        11,
        300,
        50803
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4762406655407827,
      "compression_ratio": 1.699604743083004,
      "no_speech_prob": 8.9135835878551e-05
    },
    {
      "id": 310,
      "seek": 151290,
      "start": 1521.66,
      "end": 1526.2800000000002,
      "text": " should not even have shared memory. For example, if an agent",
      "tokens": [
        50803,
        820,
        406,
        754,
        362,
        5507,
        4675,
        13,
        1171,
        1365,
        11,
        498,
        364,
        9461,
        51034
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4762406655407827,
      "compression_ratio": 1.699604743083004,
      "no_speech_prob": 8.9135835878551e-05
    },
    {
      "id": 311,
      "seek": 151290,
      "start": 1526.2800000000002,
      "end": 1529.88,
      "text": " is used across multiple users, we probably want to segregate",
      "tokens": [
        51034,
        307,
        1143,
        2108,
        3866,
        5022,
        11,
        321,
        1391,
        528,
        281,
        37630,
        473,
        51214
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4762406655407827,
      "compression_ratio": 1.699604743083004,
      "no_speech_prob": 8.9135835878551e-05
    },
    {
      "id": 312,
      "seek": 151290,
      "start": 1529.88,
      "end": 1534.26,
      "text": " long term memory across those users anyway. However, if we do",
      "tokens": [
        51214,
        938,
        1433,
        4675,
        2108,
        729,
        5022,
        4033,
        13,
        2908,
        11,
        498,
        321,
        360,
        51433
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4762406655407827,
      "compression_ratio": 1.699604743083004,
      "no_speech_prob": 8.9135835878551e-05
    },
    {
      "id": 313,
      "seek": 151290,
      "start": 1534.26,
      "end": 1537.68,
      "text": " want to make sure that we're retaining shared memory, I think",
      "tokens": [
        51433,
        528,
        281,
        652,
        988,
        300,
        321,
        434,
        34936,
        5507,
        4675,
        11,
        286,
        519,
        51604
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4762406655407827,
      "compression_ratio": 1.699604743083004,
      "no_speech_prob": 8.9135835878551e-05
    },
    {
      "id": 314,
      "seek": 151290,
      "start": 1538.0400000000002,
      "end": 1542.2800000000002,
      "text": " this is where we're going to have to do a lot of work. So",
      "tokens": [
        51622,
        341,
        307,
        689,
        321,
        434,
        516,
        281,
        362,
        281,
        360,
        257,
        688,
        295,
        589,
        13,
        407,
        51834
      ],
      "temperature": 0.0,
      "avg_logprob": -0.4762406655407827,
      "compression_ratio": 1.699604743083004,
      "no_speech_prob": 8.9135835878551e-05
    },
    {
      "id": 315,
      "seek": 154228,
      "start": 1542.28,
      "end": 1545.58,
      "text": " this is where it's going to become super important,",
      "tokens": [
        50365,
        341,
        307,
        689,
        309,
        311,
        516,
        281,
        1813,
        1687,
        1021,
        11,
        50530
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15091140790917407,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.00023024542315397412
    },
    {
      "id": 316,
      "seek": 154228,
      "start": 1545.58,
      "end": 1550.8,
      "text": " specifically how you design your long term memory. Examples of",
      "tokens": [
        50530,
        4682,
        577,
        291,
        1715,
        428,
        938,
        1433,
        4675,
        13,
        48591,
        295,
        50791
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15091140790917407,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.00023024542315397412
    },
    {
      "id": 317,
      "seek": 154228,
      "start": 1550.8,
      "end": 1555.22,
      "text": " this could be vector memory blocks, where we're making sure",
      "tokens": [
        50791,
        341,
        727,
        312,
        8062,
        4675,
        8474,
        11,
        689,
        321,
        434,
        1455,
        988,
        51012
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15091140790917407,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.00023024542315397412
    },
    {
      "id": 318,
      "seek": 154228,
      "start": 1555.22,
      "end": 1558.8999999999999,
      "text": " that for every user, we have separate collections or",
      "tokens": [
        51012,
        300,
        337,
        633,
        4195,
        11,
        321,
        362,
        4994,
        16641,
        420,
        51196
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15091140790917407,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.00023024542315397412
    },
    {
      "id": 319,
      "seek": 154228,
      "start": 1558.8999999999999,
      "end": 1563.46,
      "text": " separate indices, while for maybe if we do want to be",
      "tokens": [
        51196,
        4994,
        43840,
        11,
        1339,
        337,
        1310,
        498,
        321,
        360,
        528,
        281,
        312,
        51424
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15091140790917407,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.00023024542315397412
    },
    {
      "id": 320,
      "seek": 154228,
      "start": 1563.6399999999999,
      "end": 1568.8,
      "text": " retaining context across agent implementations, we have a",
      "tokens": [
        51433,
        34936,
        4319,
        2108,
        9461,
        4445,
        763,
        11,
        321,
        362,
        257,
        51691
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15091140790917407,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.00023024542315397412
    },
    {
      "id": 321,
      "seek": 154228,
      "start": 1568.8,
      "end": 1571.92,
      "text": " separate global index for that.",
      "tokens": [
        51691,
        4994,
        4338,
        8186,
        337,
        300,
        13,
        51847
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15091140790917407,
      "compression_ratio": 1.65625,
      "no_speech_prob": 0.00023024542315397412
    },
    {
      "id": 322,
      "seek": 157228,
      "start": 1572.28,
      "end": 1577.44,
      "text": " So again, this really much this very much depends on the specific",
      "tokens": [
        50365,
        407,
        797,
        11,
        341,
        534,
        709,
        341,
        588,
        709,
        5946,
        322,
        264,
        2685,
        50623
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 323,
      "seek": 157228,
      "start": 1577.44,
      "end": 1580.68,
      "text": " long term memory implementation that you have for your agent, if",
      "tokens": [
        50623,
        938,
        1433,
        4675,
        11420,
        300,
        291,
        362,
        337,
        428,
        9461,
        11,
        498,
        50785
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 324,
      "seek": 157228,
      "start": 1580.68,
      "end": 1583.26,
      "text": " that makes sense. But when it comes to short term memory, I",
      "tokens": [
        50785,
        300,
        1669,
        2020,
        13,
        583,
        562,
        309,
        1487,
        281,
        2099,
        1433,
        4675,
        11,
        286,
        50914
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 325,
      "seek": 157228,
      "start": 1583.26,
      "end": 1586.84,
      "text": " don't think this is something we actually even want to have any",
      "tokens": [
        50914,
        500,
        380,
        519,
        341,
        307,
        746,
        321,
        767,
        754,
        528,
        281,
        362,
        604,
        51093
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 326,
      "seek": 157228,
      "start": 1586.86,
      "end": 1588.32,
      "text": " shared implementation across.",
      "tokens": [
        51094,
        5507,
        11420,
        2108,
        13,
        51167
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 327,
      "seek": 157228,
      "start": 1588.32,
      "end": 1591.12,
      "text": " Okay, so it's only for a system where you'd have long term",
      "tokens": [
        51167,
        1033,
        11,
        370,
        309,
        311,
        787,
        337,
        257,
        1185,
        689,
        291,
        1116,
        362,
        938,
        1433,
        51307
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 328,
      "seek": 157228,
      "start": 1591.12,
      "end": 1593.46,
      "text": " memory implementation. And that makes sense, because if short",
      "tokens": [
        51307,
        4675,
        11420,
        13,
        400,
        300,
        1669,
        2020,
        11,
        570,
        498,
        2099,
        51424
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 329,
      "seek": 157228,
      "start": 1593.46,
      "end": 1595.74,
      "text": " term memory is just going to be in, you know, like a SQLite,",
      "tokens": [
        51424,
        1433,
        4675,
        307,
        445,
        516,
        281,
        312,
        294,
        11,
        291,
        458,
        11,
        411,
        257,
        19200,
        642,
        11,
        51538
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 330,
      "seek": 157228,
      "start": 1595.96,
      "end": 1599.26,
      "text": " SQLite in memory instance, it doesn't really make sense to use",
      "tokens": [
        51549,
        19200,
        642,
        294,
        4675,
        5197,
        11,
        309,
        1177,
        380,
        534,
        652,
        2020,
        281,
        764,
        51714
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 331,
      "seek": 157228,
      "start": 1599.26,
      "end": 1602.22,
      "text": " that kind of memory for something like a distributed agent system.",
      "tokens": [
        51714,
        300,
        733,
        295,
        4675,
        337,
        746,
        411,
        257,
        12631,
        9461,
        1185,
        13,
        51862
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15445734050175916,
      "compression_ratio": 1.904153354632588,
      "no_speech_prob": 0.0014658935833722353
    },
    {
      "id": 332,
      "seek": 160228,
      "start": 1602.28,
      "end": 1602.78,
      "text": " Yes.",
      "tokens": [
        50365,
        1079,
        13,
        50390
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 333,
      "seek": 160228,
      "start": 1602.78,
      "end": 1605.52,
      "text": " Okay, so okay, so let's talk a little bit about tracing. So so",
      "tokens": [
        50390,
        1033,
        11,
        370,
        1392,
        11,
        370,
        718,
        311,
        751,
        257,
        707,
        857,
        466,
        25262,
        13,
        407,
        370,
        50527
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 334,
      "seek": 160228,
      "start": 1605.52,
      "end": 1608.3,
      "text": " what you know, is there any implementation or what would you",
      "tokens": [
        50527,
        437,
        291,
        458,
        11,
        307,
        456,
        604,
        11420,
        420,
        437,
        576,
        291,
        50666
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 335,
      "seek": 160228,
      "start": 1608.3,
      "end": 1611.46,
      "text": " recommend developers can use for tracing memory hits or misses?",
      "tokens": [
        50666,
        2748,
        8849,
        393,
        764,
        337,
        25262,
        4675,
        8664,
        420,
        29394,
        30,
        50824
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 336,
      "seek": 160228,
      "start": 1611.46,
      "end": 1615.06,
      "text": " Is it possible to debug if an agent forgot something?",
      "tokens": [
        50824,
        1119,
        309,
        1944,
        281,
        24083,
        498,
        364,
        9461,
        5298,
        746,
        30,
        51004
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 337,
      "seek": 160228,
      "start": 1615.06,
      "end": 1618.96,
      "text": " Yes and no. So again, this was one of those questions that I",
      "tokens": [
        51004,
        1079,
        293,
        572,
        13,
        407,
        797,
        11,
        341,
        390,
        472,
        295,
        729,
        1651,
        300,
        286,
        51199
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 338,
      "seek": 160228,
      "start": 1618.96,
      "end": 1621.8,
      "text": " thought long and hard about before we started the live",
      "tokens": [
        51199,
        1194,
        938,
        293,
        1152,
        466,
        949,
        321,
        1409,
        264,
        1621,
        51341
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 339,
      "seek": 160228,
      "start": 1621.8,
      "end": 1624.48,
      "text": " stream. And I actually had a discussion with my colleague",
      "tokens": [
        51341,
        4309,
        13,
        400,
        286,
        767,
        632,
        257,
        5017,
        365,
        452,
        13532,
        51475
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 340,
      "seek": 160228,
      "start": 1624.52,
      "end": 1628.52,
      "text": " about this as well. I don't think that this is a good idea.",
      "tokens": [
        51477,
        466,
        341,
        382,
        731,
        13,
        286,
        500,
        380,
        519,
        300,
        341,
        307,
        257,
        665,
        1558,
        13,
        51677
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 341,
      "seek": 160228,
      "start": 1628.52,
      "end": 1631.1399999999999,
      "text": " Because I think it's not a good idea, because it's not a good",
      "tokens": [
        51677,
        1436,
        286,
        519,
        309,
        311,
        406,
        257,
        665,
        1558,
        11,
        570,
        309,
        311,
        406,
        257,
        665,
        51808
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 342,
      "seek": 160228,
      "start": 1631.1399999999999,
      "end": 1631.52,
      "text": " idea.",
      "tokens": [
        51808,
        1558,
        13,
        51827
      ],
      "temperature": 0.4,
      "avg_logprob": -0.6779838457499465,
      "compression_ratio": 1.696594427244582,
      "no_speech_prob": 0.0016729357885196805
    },
    {
      "id": 343,
      "seek": 163228,
      "start": 1632.28,
      "end": 1635.52,
      "text": " So I think actually, currently, there are great implementations",
      "tokens": [
        50365,
        407,
        286,
        519,
        767,
        11,
        4362,
        11,
        456,
        366,
        869,
        4445,
        763,
        50527
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 344,
      "seek": 163228,
      "start": 1635.52,
      "end": 1638.6399999999999,
      "text": " to debug memory specifically, but that's not to say that we",
      "tokens": [
        50527,
        281,
        24083,
        4675,
        4682,
        11,
        457,
        300,
        311,
        406,
        281,
        584,
        300,
        321,
        50683
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 345,
      "seek": 163228,
      "start": 1638.6399999999999,
      "end": 1643.44,
      "text": " cannot debug memory. And again, I'm going to sound like a parrot",
      "tokens": [
        50683,
        2644,
        24083,
        4675,
        13,
        400,
        797,
        11,
        286,
        478,
        516,
        281,
        1626,
        411,
        257,
        42462,
        50923
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 346,
      "seek": 163228,
      "start": 1643.44,
      "end": 1648.06,
      "text": " repeating itself, but it will depend on the specific memory",
      "tokens": [
        50923,
        18617,
        2564,
        11,
        457,
        309,
        486,
        5672,
        322,
        264,
        2685,
        4675,
        51154
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 347,
      "seek": 163228,
      "start": 1648.06,
      "end": 1651.66,
      "text": " implementation. And again, let's take vector memory blocks as a",
      "tokens": [
        51154,
        11420,
        13,
        400,
        797,
        11,
        718,
        311,
        747,
        8062,
        4675,
        8474,
        382,
        257,
        51334
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 348,
      "seek": 163228,
      "start": 1651.66,
      "end": 1656.28,
      "text": " great example, because we have great ways to debug semantic",
      "tokens": [
        51334,
        869,
        1365,
        11,
        570,
        321,
        362,
        869,
        2098,
        281,
        24083,
        47982,
        51565
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 349,
      "seek": 163228,
      "start": 1656.28,
      "end": 1659.8799999999999,
      "text": " search, for example, we have a lot of tracing tools, we have a",
      "tokens": [
        51565,
        3164,
        11,
        337,
        1365,
        11,
        321,
        362,
        257,
        688,
        295,
        25262,
        3873,
        11,
        321,
        362,
        257,
        51745
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 350,
      "seek": 163228,
      "start": 1659.8799999999999,
      "end": 1662.04,
      "text": " lot of observability tools.",
      "tokens": [
        51745,
        688,
        295,
        9951,
        2310,
        3873,
        13,
        51853
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1964061134739926,
      "compression_ratio": 1.859437751004016,
      "no_speech_prob": 0.001650583348236978
    },
    {
      "id": 351,
      "seek": 166228,
      "start": 1662.28,
      "end": 1666.08,
      "text": " It's not like we're just making data Copy guess if we've fix",
      "tokens": [
        50365,
        467,
        311,
        406,
        411,
        321,
        434,
        445,
        1455,
        1412,
        25653,
        2041,
        498,
        321,
        600,
        3191,
        50555
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 352,
      "seek": 166228,
      "start": 1666.08,
      "end": 1667.48,
      "text": " something,ocus this data source.",
      "tokens": [
        50555,
        746,
        11,
        15206,
        341,
        1412,
        4009,
        13,
        50625
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 353,
      "seek": 166228,
      "start": 1667.5,
      "end": 1676.8999999999999,
      "text": " It is an amazing way to configure a name, let's agree,",
      "tokens": [
        50626,
        467,
        307,
        364,
        2243,
        636,
        281,
        22162,
        257,
        1315,
        11,
        718,
        311,
        3986,
        11,
        51096
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 354,
      "seek": 166228,
      "start": 1676.8999999999999,
      "end": 1678.6399999999999,
      "text": " so we're already the date map.",
      "tokens": [
        51096,
        370,
        321,
        434,
        1217,
        264,
        4002,
        4471,
        13,
        51183
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 355,
      "seek": 166228,
      "start": 1678.6399999999999,
      "end": 1682.84,
      "text": " This is well defined from how we can define a topic, and we got",
      "tokens": [
        51183,
        639,
        307,
        731,
        7642,
        490,
        577,
        321,
        393,
        6964,
        257,
        4829,
        11,
        293,
        321,
        658,
        51393
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 356,
      "seek": 166228,
      "start": 1682.8999999999999,
      "end": 1683.84,
      "text": " a lot of interesting data out here.",
      "tokens": [
        51396,
        257,
        688,
        295,
        1880,
        1412,
        484,
        510,
        13,
        51443
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 357,
      "seek": 166228,
      "start": 1683.84,
      "end": 1688.08,
      "text": " So the same thing goes if we are describing a topic in linear",
      "tokens": [
        51443,
        407,
        264,
        912,
        551,
        1709,
        498,
        321,
        366,
        16141,
        257,
        4829,
        294,
        8213,
        51655
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 358,
      "seek": 166228,
      "start": 1688.08,
      "end": 1692.12,
      "text": "registration with nodes, shall be consider put field, so it has",
      "tokens": [
        51655,
        48933,
        2405,
        365,
        13891,
        11,
        4393,
        312,
        1949,
        829,
        2519,
        11,
        370,
        309,
        575,
        51857
      ],
      "temperature": 1.0,
      "avg_logprob": -3.915663855416434,
      "compression_ratio": 1.5968379446640317,
      "no_speech_prob": 0.0002653683186508715
    },
    {
      "id": 359,
      "seek": 169212,
      "start": 1692.12,
      "end": 1695.62,
      "text": " retrieving from our memory the most relevant information.",
      "tokens": [
        50365,
        19817,
        798,
        490,
        527,
        4675,
        264,
        881,
        7340,
        1589,
        13,
        50540
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20383712223597936,
      "compression_ratio": 1.6325581395348838,
      "no_speech_prob": 0.02269929274916649
    },
    {
      "id": 360,
      "seek": 169212,
      "start": 1695.62,
      "end": 1702.3,
      "text": " So I think I would boil this question down to, while I don't think there is the best",
      "tokens": [
        50540,
        407,
        286,
        519,
        286,
        576,
        13329,
        341,
        1168,
        760,
        281,
        11,
        1339,
        286,
        500,
        380,
        519,
        456,
        307,
        264,
        1151,
        50874
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20383712223597936,
      "compression_ratio": 1.6325581395348838,
      "no_speech_prob": 0.02269929274916649
    },
    {
      "id": 361,
      "seek": 169212,
      "start": 1702.3,
      "end": 1708.7199999999998,
      "text": " solution to debug memory as a whole, depending on our long-term memory implementation, there",
      "tokens": [
        50874,
        3827,
        281,
        24083,
        4675,
        382,
        257,
        1379,
        11,
        5413,
        322,
        527,
        938,
        12,
        7039,
        4675,
        11420,
        11,
        456,
        51195
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20383712223597936,
      "compression_ratio": 1.6325581395348838,
      "no_speech_prob": 0.02269929274916649
    },
    {
      "id": 362,
      "seek": 169212,
      "start": 1708.7199999999998,
      "end": 1717.7399999999998,
      "text": " are already ways to incorporate observability tracing and evaluation for those memory implementations",
      "tokens": [
        51195,
        366,
        1217,
        2098,
        281,
        16091,
        9951,
        2310,
        25262,
        293,
        13344,
        337,
        729,
        4675,
        4445,
        763,
        51646
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20383712223597936,
      "compression_ratio": 1.6325581395348838,
      "no_speech_prob": 0.02269929274916649
    },
    {
      "id": 363,
      "seek": 169212,
      "start": 1717.7399999999998,
      "end": 1718.7399999999998,
      "text": " specifically.",
      "tokens": [
        51646,
        4682,
        13,
        51696
      ],
      "temperature": 0.0,
      "avg_logprob": -0.20383712223597936,
      "compression_ratio": 1.6325581395348838,
      "no_speech_prob": 0.02269929274916649
    },
    {
      "id": 364,
      "seek": 171874,
      "start": 1718.74,
      "end": 1721.8,
      "text": " Okay, so it would depend more on the mechanism used.",
      "tokens": [
        50365,
        1033,
        11,
        370,
        309,
        576,
        5672,
        544,
        322,
        264,
        7513,
        1143,
        13,
        50518
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1470710829933091,
      "compression_ratio": 1.616600790513834,
      "no_speech_prob": 0.003930257633328438
    },
    {
      "id": 365,
      "seek": 171874,
      "start": 1721.8,
      "end": 1726.52,
      "text": " So it sounds like debugging short-term memory is essentially not tenable, but you could",
      "tokens": [
        50518,
        407,
        309,
        3263,
        411,
        45592,
        2099,
        12,
        7039,
        4675,
        307,
        4476,
        406,
        2064,
        712,
        11,
        457,
        291,
        727,
        50754
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1470710829933091,
      "compression_ratio": 1.616600790513834,
      "no_speech_prob": 0.003930257633328438
    },
    {
      "id": 366,
      "seek": 171874,
      "start": 1726.52,
      "end": 1732.78,
      "text": " implement mechanisms for debugging long-term memory through something like vector store",
      "tokens": [
        50754,
        4445,
        15902,
        337,
        45592,
        938,
        12,
        7039,
        4675,
        807,
        746,
        411,
        8062,
        3531,
        51067
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1470710829933091,
      "compression_ratio": 1.616600790513834,
      "no_speech_prob": 0.003930257633328438
    },
    {
      "id": 367,
      "seek": 171874,
      "start": 1732.78,
      "end": 1734.7,
      "text": " and semantic similarity search.",
      "tokens": [
        51067,
        293,
        47982,
        32194,
        3164,
        13,
        51163
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1470710829933091,
      "compression_ratio": 1.616600790513834,
      "no_speech_prob": 0.003930257633328438
    },
    {
      "id": 368,
      "seek": 171874,
      "start": 1734.7,
      "end": 1741.84,
      "text": " Yes, and on top of that, we do have observability tools where we can also make",
      "tokens": [
        51163,
        1079,
        11,
        293,
        322,
        1192,
        295,
        300,
        11,
        321,
        360,
        362,
        9951,
        2310,
        3873,
        689,
        321,
        393,
        611,
        652,
        51520
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1470710829933091,
      "compression_ratio": 1.616600790513834,
      "no_speech_prob": 0.003930257633328438
    },
    {
      "id": 369,
      "seek": 171874,
      "start": 1741.84,
      "end": 1747.54,
      "text": " sure to see, for example, here's the input from the user.",
      "tokens": [
        51520,
        988,
        281,
        536,
        11,
        337,
        1365,
        11,
        510,
        311,
        264,
        4846,
        490,
        264,
        4195,
        13,
        51805
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1470710829933091,
      "compression_ratio": 1.616600790513834,
      "no_speech_prob": 0.003930257633328438
    },
    {
      "id": 370,
      "seek": 171874,
      "start": 1747.54,
      "end": 1748.54,
      "text": " And what...",
      "tokens": [
        51805,
        400,
        437,
        485,
        51855
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1470710829933091,
      "compression_ratio": 1.616600790513834,
      "no_speech_prob": 0.003930257633328438
    },
    {
      "id": 371,
      "seek": 174854,
      "start": 1748.54,
      "end": 1753.98,
      "text": " Notice that the memory chunks we got is just irrelevant.",
      "tokens": [
        50365,
        13428,
        300,
        264,
        4675,
        24004,
        321,
        658,
        307,
        445,
        28682,
        13,
        50637
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1906206585112072,
      "compression_ratio": 1.6833976833976834,
      "no_speech_prob": 0.0022980100475251675
    },
    {
      "id": 372,
      "seek": 174854,
      "start": 1753.98,
      "end": 1759.56,
      "text": " There could be many reasons to that, and one of the reasons could be that simply the underlying",
      "tokens": [
        50637,
        821,
        727,
        312,
        867,
        4112,
        281,
        300,
        11,
        293,
        472,
        295,
        264,
        4112,
        727,
        312,
        300,
        2935,
        264,
        14217,
        50916
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1906206585112072,
      "compression_ratio": 1.6833976833976834,
      "no_speech_prob": 0.0022980100475251675
    },
    {
      "id": 373,
      "seek": 174854,
      "start": 1759.56,
      "end": 1766.22,
      "text": " LLMO agent provided a nonsensical query to actually query the memory in the first place,",
      "tokens": [
        50916,
        441,
        43,
        18976,
        9461,
        5649,
        257,
        297,
        892,
        694,
        804,
        14581,
        281,
        767,
        14581,
        264,
        4675,
        294,
        264,
        700,
        1081,
        11,
        51249
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1906206585112072,
      "compression_ratio": 1.6833976833976834,
      "no_speech_prob": 0.0022980100475251675
    },
    {
      "id": 374,
      "seek": 174854,
      "start": 1766.22,
      "end": 1769.34,
      "text": " and we can always trace that in existing tracing tools.",
      "tokens": [
        51249,
        293,
        321,
        393,
        1009,
        13508,
        300,
        294,
        6741,
        25262,
        3873,
        13,
        51405
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1906206585112072,
      "compression_ratio": 1.6833976833976834,
      "no_speech_prob": 0.0022980100475251675
    },
    {
      "id": 375,
      "seek": 174854,
      "start": 1769.34,
      "end": 1775.3999999999999,
      "text": " So again, this is not necessarily tracing and evaluating memory itself, but it is very",
      "tokens": [
        51405,
        407,
        797,
        11,
        341,
        307,
        406,
        4725,
        25262,
        293,
        27479,
        4675,
        2564,
        11,
        457,
        309,
        307,
        588,
        51708
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1906206585112072,
      "compression_ratio": 1.6833976833976834,
      "no_speech_prob": 0.0022980100475251675
    },
    {
      "id": 376,
      "seek": 174854,
      "start": 1775.3999999999999,
      "end": 1776.58,
      "text": " closely coupled.",
      "tokens": [
        51708,
        8185,
        29482,
        13,
        51767
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1906206585112072,
      "compression_ratio": 1.6833976833976834,
      "no_speech_prob": 0.0022980100475251675
    },
    {
      "id": 377,
      "seek": 174854,
      "start": 1776.58,
      "end": 1778.32,
      "text": " If the memory block is received...",
      "tokens": [
        51767,
        759,
        264,
        4675,
        3461,
        307,
        4613,
        485,
        51854
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1906206585112072,
      "compression_ratio": 1.6833976833976834,
      "no_speech_prob": 0.0022980100475251675
    },
    {
      "id": 378,
      "seek": 177832,
      "start": 1778.32,
      "end": 1782.4199999999998,
      "text": " If the memory block is receiving the wrong query to actually extract memory blocks from,",
      "tokens": [
        50365,
        759,
        264,
        4675,
        3461,
        307,
        10040,
        264,
        2085,
        14581,
        281,
        767,
        8947,
        4675,
        8474,
        490,
        11,
        50570
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 379,
      "seek": 177832,
      "start": 1782.4199999999998,
      "end": 1784.4199999999998,
      "text": " then maybe that's something to debug.",
      "tokens": [
        50570,
        550,
        1310,
        300,
        311,
        746,
        281,
        24083,
        13,
        50670
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 380,
      "seek": 177832,
      "start": 1784.4199999999998,
      "end": 1789.62,
      "text": " If it is receiving the correct query, then that will depend on the actual long-term memory",
      "tokens": [
        50670,
        759,
        309,
        307,
        10040,
        264,
        3006,
        14581,
        11,
        550,
        300,
        486,
        5672,
        322,
        264,
        3539,
        938,
        12,
        7039,
        4675,
        50930
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 381,
      "seek": 177832,
      "start": 1789.62,
      "end": 1792.56,
      "text": " implementation, how we evaluate it.",
      "tokens": [
        50930,
        11420,
        11,
        577,
        321,
        13059,
        309,
        13,
        51077
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 382,
      "seek": 177832,
      "start": 1792.56,
      "end": 1793.7,
      "text": " Okay.",
      "tokens": [
        51077,
        1033,
        13,
        51134
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 383,
      "seek": 177832,
      "start": 1793.7,
      "end": 1799.5,
      "text": " And as we are now very, very close to the end, we'll just do this question, and then",
      "tokens": [
        51134,
        400,
        382,
        321,
        366,
        586,
        588,
        11,
        588,
        1998,
        281,
        264,
        917,
        11,
        321,
        603,
        445,
        360,
        341,
        1168,
        11,
        293,
        550,
        51424
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 384,
      "seek": 177832,
      "start": 1799.5,
      "end": 1802.0,
      "text": " we'll address maybe one or two questions from the chat.",
      "tokens": [
        51424,
        321,
        603,
        2985,
        1310,
        472,
        420,
        732,
        1651,
        490,
        264,
        5081,
        13,
        51549
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 385,
      "seek": 177832,
      "start": 1802.0,
      "end": 1806.04,
      "text": " Actually, before we get to this question, let me make a little bit of space and time",
      "tokens": [
        51549,
        5135,
        11,
        949,
        321,
        483,
        281,
        341,
        1168,
        11,
        718,
        385,
        652,
        257,
        707,
        857,
        295,
        1901,
        293,
        565,
        51751
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 386,
      "seek": 177832,
      "start": 1806.04,
      "end": 1808.32,
      "text": " to see questions from the chat.",
      "tokens": [
        51751,
        281,
        536,
        1651,
        490,
        264,
        5081,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.12556317744364265,
      "compression_ratio": 1.8731884057971016,
      "no_speech_prob": 0.002530355704948306
    },
    {
      "id": 387,
      "seek": 180832,
      "start": 1808.32,
      "end": 1815.0,
      "text": " So from the audience, or to one, if you're following the chat, are there any questions",
      "tokens": [
        50365,
        407,
        490,
        264,
        4034,
        11,
        420,
        281,
        472,
        11,
        498,
        291,
        434,
        3480,
        264,
        5081,
        11,
        366,
        456,
        604,
        1651,
        50699
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23343063262571773,
      "compression_ratio": 1.5181347150259068,
      "no_speech_prob": 0.0038982448168098927
    },
    {
      "id": 388,
      "seek": 180832,
      "start": 1815.0,
      "end": 1816.76,
      "text": " you'd like to address?",
      "tokens": [
        50699,
        291,
        1116,
        411,
        281,
        2985,
        30,
        50787
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23343063262571773,
      "compression_ratio": 1.5181347150259068,
      "no_speech_prob": 0.0038982448168098927
    },
    {
      "id": 389,
      "seek": 180832,
      "start": 1816.76,
      "end": 1825.32,
      "text": " I'm not seeing the questions that come through YouTube specifically, so let's see.",
      "tokens": [
        50787,
        286,
        478,
        406,
        2577,
        264,
        1651,
        300,
        808,
        807,
        3088,
        4682,
        11,
        370,
        718,
        311,
        536,
        13,
        51215
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23343063262571773,
      "compression_ratio": 1.5181347150259068,
      "no_speech_prob": 0.0038982448168098927
    },
    {
      "id": 390,
      "seek": 180832,
      "start": 1825.32,
      "end": 1830.8999999999999,
      "text": " Okay.",
      "tokens": [
        51215,
        1033,
        13,
        51494
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23343063262571773,
      "compression_ratio": 1.5181347150259068,
      "no_speech_prob": 0.0038982448168098927
    },
    {
      "id": 391,
      "seek": 180832,
      "start": 1830.8999999999999,
      "end": 1836.32,
      "text": " So if that's the case, maybe let's just end the stream with this final question.",
      "tokens": [
        51494,
        407,
        498,
        300,
        311,
        264,
        1389,
        11,
        1310,
        718,
        311,
        445,
        917,
        264,
        4309,
        365,
        341,
        2572,
        1168,
        13,
        51765
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23343063262571773,
      "compression_ratio": 1.5181347150259068,
      "no_speech_prob": 0.0038982448168098927
    },
    {
      "id": 392,
      "seek": 180832,
      "start": 1836.32,
      "end": 1837.32,
      "text": " Okay.",
      "tokens": [
        51765,
        1033,
        13,
        51815
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23343063262571773,
      "compression_ratio": 1.5181347150259068,
      "no_speech_prob": 0.0038982448168098927
    },
    {
      "id": 393,
      "seek": 180832,
      "start": 1837.32,
      "end": 1838.32,
      "text": " Thanks.",
      "tokens": [
        51815,
        2561,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.23343063262571773,
      "compression_ratio": 1.5181347150259068,
      "no_speech_prob": 0.0038982448168098927
    },
    {
      "id": 394,
      "seek": 183832,
      "start": 1838.32,
      "end": 1840.6399999999999,
      "text": " So, do you think there are any lessons learned from applications where memory did improve",
      "tokens": [
        50365,
        407,
        11,
        360,
        291,
        519,
        456,
        366,
        604,
        8820,
        3264,
        490,
        5821,
        689,
        4675,
        630,
        3470,
        50481
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 395,
      "seek": 183832,
      "start": 1840.6399999999999,
      "end": 1842.6399999999999,
      "text": " outcomes?",
      "tokens": [
        50481,
        10070,
        30,
        50581
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 396,
      "seek": 183832,
      "start": 1842.6399999999999,
      "end": 1846.3999999999999,
      "text": " Yes.",
      "tokens": [
        50581,
        1079,
        13,
        50769
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 397,
      "seek": 183832,
      "start": 1846.3999999999999,
      "end": 1850.98,
      "text": " I would say, again, it depends.",
      "tokens": [
        50769,
        286,
        576,
        584,
        11,
        797,
        11,
        309,
        5946,
        13,
        50998
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 398,
      "seek": 183832,
      "start": 1850.98,
      "end": 1856.3999999999999,
      "text": " I think a lot of the problems that I've seen when it comes to memory is the memory implementation",
      "tokens": [
        50998,
        286,
        519,
        257,
        688,
        295,
        264,
        2740,
        300,
        286,
        600,
        1612,
        562,
        309,
        1487,
        281,
        4675,
        307,
        264,
        4675,
        11420,
        51269
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 399,
      "seek": 183832,
      "start": 1856.3999999999999,
      "end": 1862.3,
      "text": " itself not being a reflection of what the application should actually do.",
      "tokens": [
        51269,
        2564,
        406,
        885,
        257,
        12914,
        295,
        437,
        264,
        3861,
        820,
        767,
        360,
        13,
        51564
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 400,
      "seek": 183832,
      "start": 1862.3,
      "end": 1866.8999999999999,
      "text": " So an example of this, and I took note of this so I can actually read it out to you,",
      "tokens": [
        51564,
        407,
        364,
        1365,
        295,
        341,
        11,
        293,
        286,
        1890,
        3637,
        295,
        341,
        370,
        286,
        393,
        767,
        1401,
        309,
        484,
        281,
        291,
        11,
        51794
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 401,
      "seek": 183832,
      "start": 1866.8999999999999,
      "end": 1867.8999999999999,
      "text": " but...",
      "tokens": [
        51794,
        457,
        485,
        51844
      ],
      "temperature": 0.0,
      "avg_logprob": -0.254028687110314,
      "compression_ratio": 1.6736401673640167,
      "no_speech_prob": 0.0008611485827714205
    },
    {
      "id": 402,
      "seek": 186790,
      "start": 1867.9,
      "end": 1873.26,
      "text": " An example of this was an implementation, without giving a lot of detail on it, that",
      "tokens": [
        50365,
        1107,
        1365,
        295,
        341,
        390,
        364,
        11420,
        11,
        1553,
        2902,
        257,
        688,
        295,
        2607,
        322,
        309,
        11,
        300,
        50633
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14252895238448163,
      "compression_ratio": 1.7012987012987013,
      "no_speech_prob": 0.0012762579135596752
    },
    {
      "id": 403,
      "seek": 186790,
      "start": 1873.26,
      "end": 1880.8000000000002,
      "text": " relied on past interactions with the user, but also the most up-to-date information,",
      "tokens": [
        50633,
        35463,
        322,
        1791,
        13280,
        365,
        264,
        4195,
        11,
        457,
        611,
        264,
        881,
        493,
        12,
        1353,
        12,
        17393,
        1589,
        11,
        51010
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14252895238448163,
      "compression_ratio": 1.7012987012987013,
      "no_speech_prob": 0.0012762579135596752
    },
    {
      "id": 404,
      "seek": 186790,
      "start": 1880.8000000000002,
      "end": 1882.94,
      "text": " specifically in this case from manuals.",
      "tokens": [
        51010,
        4682,
        294,
        341,
        1389,
        490,
        9688,
        82,
        13,
        51117
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14252895238448163,
      "compression_ratio": 1.7012987012987013,
      "no_speech_prob": 0.0012762579135596752
    },
    {
      "id": 405,
      "seek": 186790,
      "start": 1882.94,
      "end": 1889.4,
      "text": " So you can imagine manuals, they have a lot of information, and it's about a software",
      "tokens": [
        51117,
        407,
        291,
        393,
        3811,
        9688,
        82,
        11,
        436,
        362,
        257,
        688,
        295,
        1589,
        11,
        293,
        309,
        311,
        466,
        257,
        4722,
        51440
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14252895238448163,
      "compression_ratio": 1.7012987012987013,
      "no_speech_prob": 0.0012762579135596752
    },
    {
      "id": 406,
      "seek": 186790,
      "start": 1889.4,
      "end": 1892.92,
      "text": " or hardware tool.",
      "tokens": [
        51440,
        420,
        8837,
        2290,
        13,
        51616
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14252895238448163,
      "compression_ratio": 1.7012987012987013,
      "no_speech_prob": 0.0012762579135596752
    },
    {
      "id": 407,
      "seek": 186790,
      "start": 1892.92,
      "end": 1897.26,
      "text": " But let's say that that software or hardware tool goes through regular updates.",
      "tokens": [
        51616,
        583,
        718,
        311,
        584,
        300,
        300,
        4722,
        420,
        8837,
        2290,
        1709,
        807,
        3890,
        9205,
        13,
        51833
      ],
      "temperature": 0.0,
      "avg_logprob": -0.14252895238448163,
      "compression_ratio": 1.7012987012987013,
      "no_speech_prob": 0.0012762579135596752
    },
    {
      "id": 408,
      "seek": 189726,
      "start": 1897.26,
      "end": 1899.82,
      "text": " So the manual is changing regularly.",
      "tokens": [
        50365,
        407,
        264,
        9688,
        307,
        4473,
        11672,
        13,
        50493
      ],
      "temperature": 0.0,
      "avg_logprob": -0.132564058303833,
      "compression_ratio": 1.726530612244898,
      "no_speech_prob": 0.0006369742914102972
    },
    {
      "id": 409,
      "seek": 189726,
      "start": 1899.82,
      "end": 1906.46,
      "text": " Now, the memory implementation in this case actually skipped a beat, because there was",
      "tokens": [
        50493,
        823,
        11,
        264,
        4675,
        11420,
        294,
        341,
        1389,
        767,
        30193,
        257,
        4224,
        11,
        570,
        456,
        390,
        50825
      ],
      "temperature": 0.0,
      "avg_logprob": -0.132564058303833,
      "compression_ratio": 1.726530612244898,
      "no_speech_prob": 0.0006369742914102972
    },
    {
      "id": 410,
      "seek": 189726,
      "start": 1906.46,
      "end": 1911.66,
      "text": " no implementation about which memory block to give higher priority to.",
      "tokens": [
        50825,
        572,
        11420,
        466,
        597,
        4675,
        3461,
        281,
        976,
        2946,
        9365,
        281,
        13,
        51085
      ],
      "temperature": 0.0,
      "avg_logprob": -0.132564058303833,
      "compression_ratio": 1.726530612244898,
      "no_speech_prob": 0.0006369742914102972
    },
    {
      "id": 411,
      "seek": 189726,
      "start": 1911.66,
      "end": 1916.64,
      "text": " So in this case, imagine a user interacting with an agent about the information in this",
      "tokens": [
        51085,
        407,
        294,
        341,
        1389,
        11,
        3811,
        257,
        4195,
        18017,
        365,
        364,
        9461,
        466,
        264,
        1589,
        294,
        341,
        51334
      ],
      "temperature": 0.0,
      "avg_logprob": -0.132564058303833,
      "compression_ratio": 1.726530612244898,
      "no_speech_prob": 0.0006369742914102972
    },
    {
      "id": 412,
      "seek": 189726,
      "start": 1916.64,
      "end": 1924.76,
      "text": " manual, and they've had questions answered in the past by the LLM based on the manual.",
      "tokens": [
        51334,
        9688,
        11,
        293,
        436,
        600,
        632,
        1651,
        10103,
        294,
        264,
        1791,
        538,
        264,
        441,
        43,
        44,
        2361,
        322,
        264,
        9688,
        13,
        51740
      ],
      "temperature": 0.0,
      "avg_logprob": -0.132564058303833,
      "compression_ratio": 1.726530612244898,
      "no_speech_prob": 0.0006369742914102972
    },
    {
      "id": 413,
      "seek": 189726,
      "start": 1924.76,
      "end": 1926.26,
      "text": " Now that manual has changed.",
      "tokens": [
        51740,
        823,
        300,
        9688,
        575,
        3105,
        13,
        51815
      ],
      "temperature": 0.0,
      "avg_logprob": -0.132564058303833,
      "compression_ratio": 1.726530612244898,
      "no_speech_prob": 0.0006369742914102972
    },
    {
      "id": 414,
      "seek": 189726,
      "start": 1926.26,
      "end": 1927.26,
      "text": " 10 months down the road.",
      "tokens": [
        51815,
        1266,
        2493,
        760,
        264,
        3060,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.132564058303833,
      "compression_ratio": 1.726530612244898,
      "no_speech_prob": 0.0006369742914102972
    },
    {
      "id": 415,
      "seek": 192726,
      "start": 1927.26,
      "end": 1932.46,
      "text": " So the user has a very similar question.",
      "tokens": [
        50365,
        407,
        264,
        4195,
        575,
        257,
        588,
        2531,
        1168,
        13,
        50625
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 416,
      "seek": 192726,
      "start": 1932.46,
      "end": 1938.04,
      "text": " In this case, the agent kept replying based on its past interaction, which is doing the",
      "tokens": [
        50625,
        682,
        341,
        1389,
        11,
        264,
        9461,
        4305,
        1085,
        7310,
        2361,
        322,
        1080,
        1791,
        9285,
        11,
        597,
        307,
        884,
        264,
        50904
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 417,
      "seek": 192726,
      "start": 1938.04,
      "end": 1939.44,
      "text": " right job in terms of memory.",
      "tokens": [
        50904,
        558,
        1691,
        294,
        2115,
        295,
        4675,
        13,
        50974
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 418,
      "seek": 192726,
      "start": 1939.44,
      "end": 1943.96,
      "text": " Like the LLM, the agent is not at fault here, because it's actually checking past messages",
      "tokens": [
        50974,
        1743,
        264,
        441,
        43,
        44,
        11,
        264,
        9461,
        307,
        406,
        412,
        7441,
        510,
        11,
        570,
        309,
        311,
        767,
        8568,
        1791,
        7897,
        51200
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 419,
      "seek": 192726,
      "start": 1943.96,
      "end": 1946.08,
      "text": " between itself and the user.",
      "tokens": [
        51200,
        1296,
        2564,
        293,
        264,
        4195,
        13,
        51306
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 420,
      "seek": 192726,
      "start": 1946.08,
      "end": 1951.9,
      "text": " However, in the meantime, the manual has changed, and the agent is not giving priority to checking",
      "tokens": [
        51306,
        2908,
        11,
        294,
        264,
        14991,
        11,
        264,
        9688,
        575,
        3105,
        11,
        293,
        264,
        9461,
        307,
        406,
        2902,
        9365,
        281,
        8568,
        51597
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 421,
      "seek": 192726,
      "start": 1951.9,
      "end": 1956.82,
      "text": " the manual first before referring to the memory implementation that stores the chat.",
      "tokens": [
        51597,
        264,
        9688,
        700,
        949,
        13761,
        281,
        264,
        4675,
        11420,
        300,
        9512,
        264,
        5081,
        13,
        51843
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 422,
      "seek": 192726,
      "start": 1956.82,
      "end": 1957.26,
      "text": " 10 months down the road.",
      "tokens": [
        51843,
        1266,
        2493,
        760,
        264,
        3060,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2357816295463498,
      "compression_ratio": 1.7147887323943662,
      "no_speech_prob": 0.0012016480322927237
    },
    {
      "id": 423,
      "seek": 195726,
      "start": 1957.26,
      "end": 1958.26,
      "text": " So the user is now in the chat history.",
      "tokens": [
        50365,
        407,
        264,
        4195,
        307,
        586,
        294,
        264,
        5081,
        2503,
        13,
        50415
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 424,
      "seek": 195726,
      "start": 1958.26,
      "end": 1959.26,
      "text": " Does that make sense?",
      "tokens": [
        50415,
        4402,
        300,
        652,
        2020,
        30,
        50465
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 425,
      "seek": 195726,
      "start": 1959.26,
      "end": 1960.26,
      "text": " Yes, that makes sense.",
      "tokens": [
        50465,
        1079,
        11,
        300,
        1669,
        2020,
        13,
        50515
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 426,
      "seek": 195726,
      "start": 1960.26,
      "end": 1965.26,
      "text": " So let's make it a very simple example.",
      "tokens": [
        50515,
        407,
        718,
        311,
        652,
        309,
        257,
        588,
        2199,
        1365,
        13,
        50765
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 427,
      "seek": 195726,
      "start": 1965.26,
      "end": 1969.42,
      "text": " Does the vehicle have four wheels or six?",
      "tokens": [
        50765,
        4402,
        264,
        5864,
        362,
        1451,
        10046,
        420,
        2309,
        30,
        50973
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 428,
      "seek": 195726,
      "start": 1969.42,
      "end": 1970.82,
      "text": " It used to have four wheels.",
      "tokens": [
        50973,
        467,
        1143,
        281,
        362,
        1451,
        10046,
        13,
        51043
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 429,
      "seek": 195726,
      "start": 1970.82,
      "end": 1972.28,
      "text": " So the answer was four wheels.",
      "tokens": [
        51043,
        407,
        264,
        1867,
        390,
        1451,
        10046,
        13,
        51116
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 430,
      "seek": 195726,
      "start": 1972.28,
      "end": 1975.76,
      "text": " But the manual has been updated to say it has six wheels.",
      "tokens": [
        51116,
        583,
        264,
        9688,
        575,
        668,
        10588,
        281,
        584,
        309,
        575,
        2309,
        10046,
        13,
        51290
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 431,
      "seek": 195726,
      "start": 1975.76,
      "end": 1980.94,
      "text": " However, the agent is referring to the chat history initially before referring to the",
      "tokens": [
        51290,
        2908,
        11,
        264,
        9461,
        307,
        13761,
        281,
        264,
        5081,
        2503,
        9105,
        949,
        13761,
        281,
        264,
        51549
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 432,
      "seek": 195726,
      "start": 1980.94,
      "end": 1981.94,
      "text": " manual itself.",
      "tokens": [
        51549,
        9688,
        2564,
        13,
        51599
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 433,
      "seek": 195726,
      "start": 1981.94,
      "end": 1987.26,
      "text": " So in this case, the implementation was only a very short miss in that we had to provide",
      "tokens": [
        51599,
        407,
        294,
        341,
        1389,
        11,
        264,
        11420,
        390,
        787,
        257,
        588,
        2099,
        1713,
        294,
        300,
        321,
        632,
        281,
        2893,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.28576290701317975,
      "compression_ratio": 1.8735177865612649,
      "no_speech_prob": 0.01840128004550934
    },
    {
      "id": 434,
      "seek": 198726,
      "start": 1987.26,
      "end": 1994.26,
      "text": " basically a priority queue as to what to refer to, which memory to refer to first.",
      "tokens": [
        50365,
        1936,
        257,
        9365,
        18639,
        382,
        281,
        437,
        281,
        2864,
        281,
        11,
        597,
        4675,
        281,
        2864,
        281,
        700,
        13,
        50715
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13784189427152593,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.008042282424867153
    },
    {
      "id": 435,
      "seek": 198726,
      "start": 1994.26,
      "end": 2000.02,
      "text": " In this case, actually, the memory is one of them is technically actual memory, and",
      "tokens": [
        50715,
        682,
        341,
        1389,
        11,
        767,
        11,
        264,
        4675,
        307,
        472,
        295,
        552,
        307,
        12120,
        3539,
        4675,
        11,
        293,
        51003
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13784189427152593,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.008042282424867153
    },
    {
      "id": 436,
      "seek": 198726,
      "start": 2000.02,
      "end": 2002.26,
      "text": " the other is more rag.",
      "tokens": [
        51003,
        264,
        661,
        307,
        544,
        17539,
        13,
        51115
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13784189427152593,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.008042282424867153
    },
    {
      "id": 437,
      "seek": 198726,
      "start": 2002.26,
      "end": 2008.44,
      "text": " We had to refer to a vector store that retains some information for us.",
      "tokens": [
        51115,
        492,
        632,
        281,
        2864,
        281,
        257,
        8062,
        3531,
        300,
        1533,
        2315,
        512,
        1589,
        337,
        505,
        13,
        51424
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13784189427152593,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.008042282424867153
    },
    {
      "id": 438,
      "seek": 198726,
      "start": 2008.44,
      "end": 2016.32,
      "text": " So the users had to go back and rethink the workflow architecture of what we search through",
      "tokens": [
        51424,
        407,
        264,
        5022,
        632,
        281,
        352,
        646,
        293,
        34595,
        264,
        20993,
        9482,
        295,
        437,
        321,
        3164,
        807,
        51818
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13784189427152593,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.008042282424867153
    },
    {
      "id": 439,
      "seek": 198726,
      "start": 2016.32,
      "end": 2017.26,
      "text": " before we answer the user.",
      "tokens": [
        51818,
        949,
        321,
        1867,
        264,
        4195,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.13784189427152593,
      "compression_ratio": 1.7272727272727273,
      "no_speech_prob": 0.008042282424867153
    },
    {
      "id": 440,
      "seek": 201726,
      "start": 2017.26,
      "end": 2022.14,
      "text": " Okay.",
      "tokens": [
        50365,
        1033,
        13,
        50609
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 441,
      "seek": 201726,
      "start": 2022.14,
      "end": 2028.22,
      "text": " So it sounds like this is a situation where if you have constant updates to the database",
      "tokens": [
        50609,
        407,
        309,
        3263,
        411,
        341,
        307,
        257,
        2590,
        689,
        498,
        291,
        362,
        5754,
        9205,
        281,
        264,
        8149,
        50913
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 442,
      "seek": 201726,
      "start": 2028.22,
      "end": 2032.58,
      "text": " or to the knowledge base, if the memory is outdated, then it doesn't make a lot of sense",
      "tokens": [
        50913,
        420,
        281,
        264,
        3601,
        3096,
        11,
        498,
        264,
        4675,
        307,
        36313,
        11,
        550,
        309,
        1177,
        380,
        652,
        257,
        688,
        295,
        2020,
        51131
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 443,
      "seek": 201726,
      "start": 2032.58,
      "end": 2033.58,
      "text": " to use.",
      "tokens": [
        51131,
        281,
        764,
        13,
        51181
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 444,
      "seek": 201726,
      "start": 2033.58,
      "end": 2037.98,
      "text": " So you have to find essentially a way to keep updating either the vector store with the",
      "tokens": [
        51181,
        407,
        291,
        362,
        281,
        915,
        4476,
        257,
        636,
        281,
        1066,
        25113,
        2139,
        264,
        8062,
        3531,
        365,
        264,
        51401
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 445,
      "seek": 201726,
      "start": 2037.98,
      "end": 2040.76,
      "text": " memory components in addition to any other tools of reference set.",
      "tokens": [
        51401,
        4675,
        6677,
        294,
        4500,
        281,
        604,
        661,
        3873,
        295,
        6408,
        992,
        13,
        51540
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 446,
      "seek": 201726,
      "start": 2040.76,
      "end": 2041.76,
      "text": " Is that correct?",
      "tokens": [
        51540,
        1119,
        300,
        3006,
        30,
        51590
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 447,
      "seek": 201726,
      "start": 2041.76,
      "end": 2043.26,
      "text": " Yes, I would say so.",
      "tokens": [
        51590,
        1079,
        11,
        286,
        576,
        584,
        370,
        13,
        51665
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 448,
      "seek": 201726,
      "start": 2043.26,
      "end": 2046.26,
      "text": " And again, this depends on the use case because there's not a lot of application.",
      "tokens": [
        51665,
        400,
        797,
        11,
        341,
        5946,
        322,
        264,
        764,
        1389,
        570,
        456,
        311,
        406,
        257,
        688,
        295,
        3861,
        13,
        51815
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 449,
      "seek": 201726,
      "start": 2046.26,
      "end": 2047.26,
      "text": " Okay.",
      "tokens": [
        51815,
        1033,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17495426054923766,
      "compression_ratio": 1.6503496503496504,
      "no_speech_prob": 0.004741647280752659
    },
    {
      "id": 450,
      "seek": 204726,
      "start": 2047.26,
      "end": 2053.48,
      "text": " So maybe that's where the main information base changes so rapidly.",
      "tokens": [
        50365,
        407,
        1310,
        300,
        311,
        689,
        264,
        2135,
        1589,
        3096,
        2962,
        370,
        12910,
        13,
        50676
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 451,
      "seek": 204726,
      "start": 2053.48,
      "end": 2058.54,
      "text": " In this case, it was changing rapidly, so they had to rethink about how the memory access",
      "tokens": [
        50676,
        682,
        341,
        1389,
        11,
        309,
        390,
        4473,
        12910,
        11,
        370,
        436,
        632,
        281,
        34595,
        466,
        577,
        264,
        4675,
        2105,
        50929
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 452,
      "seek": 204726,
      "start": 2058.54,
      "end": 2061.26,
      "text": " implementation and workflow worked.",
      "tokens": [
        50929,
        11420,
        293,
        20993,
        2732,
        13,
        51065
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 453,
      "seek": 204726,
      "start": 2061.26,
      "end": 2062.6,
      "text": " All right.",
      "tokens": [
        51065,
        1057,
        558,
        13,
        51132
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 454,
      "seek": 204726,
      "start": 2062.6,
      "end": 2064.4,
      "text": " So these are all my questions.",
      "tokens": [
        51132,
        407,
        613,
        366,
        439,
        452,
        1651,
        13,
        51222
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 455,
      "seek": 204726,
      "start": 2064.4,
      "end": 2069.7,
      "text": " So I just wanted to do a few final talking points with you, Tawana, before we finish",
      "tokens": [
        51222,
        407,
        286,
        445,
        1415,
        281,
        360,
        257,
        1326,
        2572,
        1417,
        2793,
        365,
        291,
        11,
        314,
        1607,
        2095,
        11,
        949,
        321,
        2413,
        51487
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 456,
      "seek": 204726,
      "start": 2069.7,
      "end": 2070.7,
      "text": " up the session.",
      "tokens": [
        51487,
        493,
        264,
        5481,
        13,
        51537
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 457,
      "seek": 204726,
      "start": 2070.7,
      "end": 2075.26,
      "text": " So it sounds like this is a very powerful new addition to agentic systems, and this sort",
      "tokens": [
        51537,
        407,
        309,
        3263,
        411,
        341,
        307,
        257,
        588,
        4005,
        777,
        4500,
        281,
        9461,
        299,
        3652,
        11,
        293,
        341,
        1333,
        51765
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 458,
      "seek": 204726,
      "start": 2075.26,
      "end": 2077.06,
      "text": " of reflects on this idea of how we're constantly evolving.",
      "tokens": [
        51765,
        295,
        18926,
        322,
        341,
        1558,
        295,
        577,
        321,
        434,
        6460,
        21085,
        13,
        51855
      ],
      "temperature": 0.0,
      "avg_logprob": -0.17884490160438105,
      "compression_ratio": 1.6241610738255035,
      "no_speech_prob": 0.0029052146710455418
    },
    {
      "id": 459,
      "seek": 207706,
      "start": 2077.06,
      "end": 2080.2999999999997,
      "text": " And constantly pushing the boundaries of what we can do with LLMs.",
      "tokens": [
        50365,
        400,
        6460,
        7380,
        264,
        13180,
        295,
        437,
        321,
        393,
        360,
        365,
        441,
        43,
        26386,
        13,
        50527
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 460,
      "seek": 207706,
      "start": 2080.2999999999997,
      "end": 2084.14,
      "text": " You know, as I mentioned at the beginning, the first implementation of working with LLMs",
      "tokens": [
        50527,
        509,
        458,
        11,
        382,
        286,
        2835,
        412,
        264,
        2863,
        11,
        264,
        700,
        11420,
        295,
        1364,
        365,
        441,
        43,
        26386,
        50719
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 461,
      "seek": 207706,
      "start": 2084.14,
      "end": 2085.84,
      "text": " was all about prompting them.",
      "tokens": [
        50719,
        390,
        439,
        466,
        12391,
        278,
        552,
        13,
        50804
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 462,
      "seek": 207706,
      "start": 2085.84,
      "end": 2087.6,
      "text": " Then we realized the LLMs hallucinates.",
      "tokens": [
        50804,
        1396,
        321,
        5334,
        264,
        441,
        43,
        26386,
        35212,
        259,
        1024,
        13,
        50892
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 463,
      "seek": 207706,
      "start": 2087.6,
      "end": 2088.6,
      "text": " We have to ground them.",
      "tokens": [
        50892,
        492,
        362,
        281,
        2727,
        552,
        13,
        50942
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 464,
      "seek": 207706,
      "start": 2088.6,
      "end": 2091.48,
      "text": " We started adding vector DBs and we developed RAG.",
      "tokens": [
        50942,
        492,
        1409,
        5127,
        8062,
        26754,
        82,
        293,
        321,
        4743,
        14626,
        38,
        13,
        51086
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 465,
      "seek": 207706,
      "start": 2091.48,
      "end": 2094.54,
      "text": " Then we realized, oh, but what if we give access to tools to the LLMs?",
      "tokens": [
        51086,
        1396,
        321,
        5334,
        11,
        1954,
        11,
        457,
        437,
        498,
        321,
        976,
        2105,
        281,
        3873,
        281,
        264,
        441,
        43,
        26386,
        30,
        51239
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 466,
      "seek": 207706,
      "start": 2094.54,
      "end": 2097.22,
      "text": " We then made the next jump to agent systems.",
      "tokens": [
        51239,
        492,
        550,
        1027,
        264,
        958,
        3012,
        281,
        9461,
        3652,
        13,
        51373
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 467,
      "seek": 207706,
      "start": 2097.22,
      "end": 2103.5,
      "text": " And now we have this new step where in addition to having an LLM have access to RAG or an",
      "tokens": [
        51373,
        400,
        586,
        321,
        362,
        341,
        777,
        1823,
        689,
        294,
        4500,
        281,
        1419,
        364,
        441,
        43,
        44,
        362,
        2105,
        281,
        14626,
        38,
        420,
        364,
        51687
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 468,
      "seek": 207706,
      "start": 2103.5,
      "end": 2106.18,
      "text": " agent access to a RAG database as a tool.",
      "tokens": [
        51687,
        9461,
        2105,
        281,
        257,
        14626,
        38,
        8149,
        382,
        257,
        2290,
        13,
        51821
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 469,
      "seek": 207706,
      "start": 2106.18,
      "end": 2106.38,
      "text": " We can now improve.",
      "tokens": [
        51821,
        492,
        393,
        586,
        3470,
        13,
        51831
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 470,
      "seek": 207706,
      "start": 2106.38,
      "end": 2106.44,
      "text": " Okay.",
      "tokens": [
        51831,
        1033,
        13,
        51834
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1982018704899771,
      "compression_ratio": 1.7289156626506024,
      "no_speech_prob": 0.0033769879955798388
    },
    {
      "id": 471,
      "seek": 210644,
      "start": 2106.44,
      "end": 2111.02,
      "text": " So we can now improve the LLMs ability or the agent system, the agentic systems ability",
      "tokens": [
        50365,
        407,
        321,
        393,
        586,
        3470,
        264,
        441,
        43,
        26386,
        3485,
        420,
        264,
        9461,
        1185,
        11,
        264,
        9461,
        299,
        3652,
        3485,
        50594
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1757540745017803,
      "compression_ratio": 1.7408759124087592,
      "no_speech_prob": 0.006073372438549995
    },
    {
      "id": 472,
      "seek": 210644,
      "start": 2111.02,
      "end": 2113.82,
      "text": " to answer questions by incorporating memory.",
      "tokens": [
        50594,
        281,
        1867,
        1651,
        538,
        33613,
        4675,
        13,
        50734
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1757540745017803,
      "compression_ratio": 1.7408759124087592,
      "no_speech_prob": 0.006073372438549995
    },
    {
      "id": 473,
      "seek": 210644,
      "start": 2113.82,
      "end": 2118.52,
      "text": " And in this session, we talked about incorporating short-term memory where the LLM or the agent",
      "tokens": [
        50734,
        400,
        294,
        341,
        5481,
        11,
        321,
        2825,
        466,
        33613,
        2099,
        12,
        7039,
        4675,
        689,
        264,
        441,
        43,
        44,
        420,
        264,
        9461,
        50969
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1757540745017803,
      "compression_ratio": 1.7408759124087592,
      "no_speech_prob": 0.006073372438549995
    },
    {
      "id": 474,
      "seek": 210644,
      "start": 2118.52,
      "end": 2122.44,
      "text": " will have access to chat history up to a certain token limit.",
      "tokens": [
        50969,
        486,
        362,
        2105,
        281,
        5081,
        2503,
        493,
        281,
        257,
        1629,
        14862,
        4948,
        13,
        51165
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1757540745017803,
      "compression_ratio": 1.7408759124087592,
      "no_speech_prob": 0.006073372438549995
    },
    {
      "id": 475,
      "seek": 210644,
      "start": 2122.44,
      "end": 2128.0,
      "text": " But we can also add more sophisticated forms of memory that include long-term memory through",
      "tokens": [
        51165,
        583,
        321,
        393,
        611,
        909,
        544,
        16950,
        6422,
        295,
        4675,
        300,
        4090,
        938,
        12,
        7039,
        4675,
        807,
        51443
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1757540745017803,
      "compression_ratio": 1.7408759124087592,
      "no_speech_prob": 0.006073372438549995
    },
    {
      "id": 476,
      "seek": 210644,
      "start": 2128.0,
      "end": 2134.66,
      "text": " something like vector stores and even custom made memory blocks.",
      "tokens": [
        51443,
        746,
        411,
        8062,
        9512,
        293,
        754,
        2375,
        1027,
        4675,
        8474,
        13,
        51776
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1757540745017803,
      "compression_ratio": 1.7408759124087592,
      "no_speech_prob": 0.006073372438549995
    },
    {
      "id": 477,
      "seek": 210644,
      "start": 2134.66,
      "end": 2135.76,
      "text": " So I just want to finish up.",
      "tokens": [
        51776,
        407,
        286,
        445,
        528,
        281,
        2413,
        493,
        13,
        51831
      ],
      "temperature": 0.0,
      "avg_logprob": -0.1757540745017803,
      "compression_ratio": 1.7408759124087592,
      "no_speech_prob": 0.006073372438549995
    },
    {
      "id": 478,
      "seek": 213576,
      "start": 2135.76,
      "end": 2136.76,
      "text": " The event.",
      "tokens": [
        50365,
        440,
        2280,
        13,
        50415
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 479,
      "seek": 213576,
      "start": 2136.76,
      "end": 2137.76,
      "text": " Thank you so much to Tawana for joining us today.",
      "tokens": [
        50415,
        1044,
        291,
        370,
        709,
        281,
        314,
        1607,
        2095,
        337,
        5549,
        505,
        965,
        13,
        50465
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 480,
      "seek": 213576,
      "start": 2137.76,
      "end": 2138.76,
      "text": " So this was the very first Laura the legend series.",
      "tokens": [
        50465,
        407,
        341,
        390,
        264,
        588,
        700,
        13220,
        264,
        9451,
        2638,
        13,
        50515
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 481,
      "seek": 213576,
      "start": 2138.76,
      "end": 2139.76,
      "text": " I'm always on the lookout to invite new speakers into Tawana.",
      "tokens": [
        50515,
        286,
        478,
        1009,
        322,
        264,
        41025,
        281,
        7980,
        777,
        9518,
        666,
        314,
        1607,
        2095,
        13,
        50565
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 482,
      "seek": 213576,
      "start": 2139.76,
      "end": 2140.76,
      "text": " I was so honored to have you as the very first speaker to kick off our series.",
      "tokens": [
        50565,
        286,
        390,
        370,
        14556,
        281,
        362,
        291,
        382,
        264,
        588,
        700,
        8145,
        281,
        4437,
        766,
        527,
        2638,
        13,
        50615
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 483,
      "seek": 213576,
      "start": 2140.76,
      "end": 2141.76,
      "text": " So for those of you who are tuning in for the first time, who've been following Tawana",
      "tokens": [
        50615,
        407,
        337,
        729,
        295,
        291,
        567,
        366,
        15164,
        294,
        337,
        264,
        700,
        565,
        11,
        567,
        600,
        668,
        3480,
        314,
        1607,
        2095,
        50665
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 484,
      "seek": 213576,
      "start": 2141.76,
      "end": 2142.76,
      "text": " through LLMA index.",
      "tokens": [
        50665,
        807,
        441,
        43,
        9998,
        8186,
        13,
        50715
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 485,
      "seek": 213576,
      "start": 2142.76,
      "end": 2143.76,
      "text": " So we are the AI makerspace team.",
      "tokens": [
        50715,
        407,
        321,
        366,
        264,
        7318,
        19323,
        17940,
        1469,
        13,
        50765
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 486,
      "seek": 213576,
      "start": 2143.76,
      "end": 2148.76,
      "text": " We are on a mission to build the world's leading community for people who want to build, ship",
      "tokens": [
        50765,
        492,
        366,
        322,
        257,
        4447,
        281,
        1322,
        264,
        1002,
        311,
        5775,
        1768,
        337,
        561,
        567,
        528,
        281,
        1322,
        11,
        5374,
        51015
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 487,
      "seek": 213576,
      "start": 2148.76,
      "end": 2149.76,
      "text": " and share like legends.",
      "tokens": [
        51015,
        293,
        2073,
        411,
        27695,
        13,
        51065
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 488,
      "seek": 213576,
      "start": 2149.76,
      "end": 2150.76,
      "text": " We are on YouTube and Discord.",
      "tokens": [
        51065,
        492,
        366,
        322,
        3088,
        293,
        32623,
        13,
        51115
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 489,
      "seek": 213576,
      "start": 2150.76,
      "end": 2151.76,
      "text": " So if you want to join us, you can do that.",
      "tokens": [
        51115,
        407,
        498,
        291,
        528,
        281,
        3917,
        505,
        11,
        291,
        393,
        360,
        300,
        13,
        51165
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 490,
      "seek": 213576,
      "start": 2151.76,
      "end": 2152.76,
      "text": " We are on Twitter and Instagram.",
      "tokens": [
        51165,
        492,
        366,
        322,
        5794,
        293,
        5281,
        13,
        51215
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 491,
      "seek": 213576,
      "start": 2152.76,
      "end": 2153.76,
      "text": " We are on Facebook.",
      "tokens": [
        51215,
        492,
        366,
        322,
        4384,
        13,
        51265
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 492,
      "seek": 213576,
      "start": 2153.76,
      "end": 2154.76,
      "text": " We are on Instagram.",
      "tokens": [
        51265,
        492,
        366,
        322,
        5281,
        13,
        51315
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 493,
      "seek": 213576,
      "start": 2154.76,
      "end": 2155.76,
      "text": " We are on Twitter.",
      "tokens": [
        51315,
        492,
        366,
        322,
        5794,
        13,
        51365
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 494,
      "seek": 213576,
      "start": 2155.76,
      "end": 2156.76,
      "text": " We are on Twitter.",
      "tokens": [
        51365,
        492,
        366,
        322,
        5794,
        13,
        51415
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 495,
      "seek": 213576,
      "start": 2156.76,
      "end": 2157.76,
      "text": " We are on Twitter.",
      "tokens": [
        51415,
        492,
        366,
        322,
        5794,
        13,
        51465
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 496,
      "seek": 213576,
      "start": 2157.76,
      "end": 2158.76,
      "text": " We are on Twitter.",
      "tokens": [
        51465,
        492,
        366,
        322,
        5794,
        13,
        51515
      ],
      "temperature": 0.0,
      "avg_logprob": -0.31772935123187007,
      "compression_ratio": 2.0826446280991737,
      "no_speech_prob": 0.01915024220943451
    },
    {
      "id": 497,
      "seek": 215876,
      "start": 2158.76,
      "end": 2170.0,
      "text": " We are on LinkedIn as well.",
      "tokens": [
        50365,
        492,
        366,
        322,
        20657,
        382,
        731,
        13,
        50927
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2045763471852178,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 0.011249175295233727
    },
    {
      "id": 498,
      "seek": 215876,
      "start": 2170.0,
      "end": 2175.6000000000004,
      "text": " Every week Dr. Greg and the Wiz explore cutting edge topics on AI.",
      "tokens": [
        50927,
        2048,
        1243,
        2491,
        13,
        11490,
        293,
        264,
        43490,
        6839,
        6492,
        4691,
        8378,
        322,
        7318,
        13,
        51207
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2045763471852178,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 0.011249175295233727
    },
    {
      "id": 499,
      "seek": 215876,
      "start": 2175.6000000000004,
      "end": 2178.94,
      "text": " And I am now on the lookout for developer advocates who would like to come onto our",
      "tokens": [
        51207,
        400,
        286,
        669,
        586,
        322,
        264,
        41025,
        337,
        10754,
        25160,
        567,
        576,
        411,
        281,
        808,
        3911,
        527,
        51374
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2045763471852178,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 0.011249175295233727
    },
    {
      "id": 500,
      "seek": 215876,
      "start": 2178.94,
      "end": 2181.48,
      "text": " show, tell us about their application.",
      "tokens": [
        51374,
        855,
        11,
        980,
        505,
        466,
        641,
        3861,
        13,
        51501
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2045763471852178,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 0.011249175295233727
    },
    {
      "id": 501,
      "seek": 215876,
      "start": 2181.48,
      "end": 2186.32,
      "text": " We'll have a chance to share with our audience what's new and we're always on the lookout",
      "tokens": [
        51501,
        492,
        603,
        362,
        257,
        2931,
        281,
        2073,
        365,
        527,
        4034,
        437,
        311,
        777,
        293,
        321,
        434,
        1009,
        322,
        264,
        41025,
        51743
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2045763471852178,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 0.011249175295233727
    },
    {
      "id": 502,
      "seek": 215876,
      "start": 2186.32,
      "end": 2188.76,
      "text": " to continue finding ways to build, ship and share.",
      "tokens": [
        51743,
        281,
        2354,
        5006,
        2098,
        281,
        1322,
        11,
        5374,
        293,
        2073,
        13,
        51865
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2045763471852178,
      "compression_ratio": 1.543103448275862,
      "no_speech_prob": 0.011249175295233727
    },
    {
      "id": 503,
      "seek": 218876,
      "start": 2188.76,
      "end": 2189.96,
      "text": " Like legends together.",
      "tokens": [
        50365,
        1743,
        27695,
        1214,
        13,
        50425
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 504,
      "seek": 218876,
      "start": 2189.96,
      "end": 2193.76,
      "text": " So thank you so much Tawana for joining us and thank you everyone for tuning in.",
      "tokens": [
        50425,
        407,
        1309,
        291,
        370,
        709,
        314,
        1607,
        2095,
        337,
        5549,
        505,
        293,
        1309,
        291,
        1518,
        337,
        15164,
        294,
        13,
        50615
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 505,
      "seek": 218876,
      "start": 2193.76,
      "end": 2196.2000000000003,
      "text": " We'll see you next week.",
      "tokens": [
        50615,
        492,
        603,
        536,
        291,
        958,
        1243,
        13,
        50737
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 506,
      "seek": 218876,
      "start": 2196.2000000000003,
      "end": 2198.1000000000004,
      "text": " Thank you.",
      "tokens": [
        50737,
        1044,
        291,
        13,
        50832
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 507,
      "seek": 218876,
      "start": 2198.1000000000004,
      "end": 2202.6400000000003,
      "text": " For everyone by the way who joined through YouTube, I wasn't logged in, but I will refer",
      "tokens": [
        50832,
        1171,
        1518,
        538,
        264,
        636,
        567,
        6869,
        807,
        3088,
        11,
        286,
        2067,
        380,
        27231,
        294,
        11,
        457,
        286,
        486,
        2864,
        51059
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 508,
      "seek": 218876,
      "start": 2202.6400000000003,
      "end": 2206.1200000000003,
      "text": " back to the questions you had in there because I can't see them where we're streaming it",
      "tokens": [
        51059,
        646,
        281,
        264,
        1651,
        291,
        632,
        294,
        456,
        570,
        286,
        393,
        380,
        536,
        552,
        689,
        321,
        434,
        11791,
        309,
        51233
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 509,
      "seek": 218876,
      "start": 2206.1200000000003,
      "end": 2208.36,
      "text": " right now, but I will try to answer.",
      "tokens": [
        51233,
        558,
        586,
        11,
        457,
        286,
        486,
        853,
        281,
        1867,
        13,
        51345
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 510,
      "seek": 218876,
      "start": 2208.36,
      "end": 2209.36,
      "text": " That sounds great.",
      "tokens": [
        51345,
        663,
        3263,
        869,
        13,
        51395
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 511,
      "seek": 218876,
      "start": 2209.36,
      "end": 2210.36,
      "text": " Thank you so much.",
      "tokens": [
        51395,
        1044,
        291,
        370,
        709,
        13,
        51445
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 512,
      "seek": 218876,
      "start": 2210.36,
      "end": 2211.36,
      "text": " All right.",
      "tokens": [
        51445,
        1057,
        558,
        13,
        51495
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 513,
      "seek": 218876,
      "start": 2211.36,
      "end": 2212.36,
      "text": " Till next week, everyone.",
      "tokens": [
        51495,
        20227,
        958,
        1243,
        11,
        1518,
        13,
        51545
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 514,
      "seek": 218876,
      "start": 2212.36,
      "end": 2213.36,
      "text": " Bye bye.",
      "tokens": [
        51545,
        4621,
        6543,
        13,
        51595
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 515,
      "seek": 218876,
      "start": 2213.36,
      "end": 2214.36,
      "text": " Bye.",
      "tokens": [
        51595,
        4621,
        13,
        51645
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 516,
      "seek": 218876,
      "start": 2214.36,
      "end": 2215.36,
      "text": " Bye.",
      "tokens": [
        51645,
        4621,
        13,
        51695
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 517,
      "seek": 218876,
      "start": 2215.36,
      "end": 2216.36,
      "text": " Bye.",
      "tokens": [
        51695,
        4621,
        13,
        51745
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 518,
      "seek": 218876,
      "start": 2216.36,
      "end": 2217.36,
      "text": " Bye.",
      "tokens": [
        51745,
        4621,
        13,
        51795
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 519,
      "seek": 218876,
      "start": 2217.36,
      "end": 2218.36,
      "text": " Bye.",
      "tokens": [
        51795,
        4621,
        13,
        51845
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 520,
      "seek": 218876,
      "start": 2218.36,
      "end": 2218.4,
      "text": " Bye.",
      "tokens": [
        51845,
        4621,
        13,
        51847
      ],
      "temperature": 0.0,
      "avg_logprob": -0.15937294458088122,
      "compression_ratio": 1.793103448275862,
      "no_speech_prob": 0.001110101817175746
    },
    {
      "id": 521,
      "seek": 221876,
      "start": 2218.76,
      "end": 2219.4,
      "text": " Bye.",
      "tokens": [
        50365,
        4621,
        13,
        50397
      ],
      "temperature": 0.0,
      "avg_logprob": -0.2077991008758545,
      "compression_ratio": 0.3333333333333333,
      "no_speech_prob": 0.7243092060089111
    }
  ],
  "language": "en"
}